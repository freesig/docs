{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"coreconcepts/","text":"Coreconcepts \u00b6","title":"Coreconcepts"},{"location":"coreconcepts/#coreconcepts","text":"","title":"Coreconcepts"},{"location":"coreconcepts/hello_gui/","text":"Hello GUI \u00b6 Welcome to the first GUI tutorial. So far we have interacted with our zome using curl or hc test , but that's not as nice as having a GUI. Today you will learn how to interact with a Holochain app using a super simple web page. HTML page \u00b6 You will need somewhere for all your GUI code to live. This will be a different piece of software to your Holochain zome code. So choose somewhere outside your Holochain application. Create a folder for our GUI to live in: cd holochain/coreconcepts mkdir gui cd gui Create a new file called index.html in your favourite editor. It should live at gui/index.html . Start by adding a simple HTML template to index.html . Add this modern template: <!doctype html> < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello GUI </ title > < meta name = \"description\" content = \"GUI for a Holochain app\" > </ head > < body > </ body > </ html > Inside the <body> tag add a button: < button type = \"button\" > Say Hello </ button > To make things a bit nicer on the eyes you can add the water.css stylesheet. Add this water.css link inside the <head> tag: < link rel = \"stylesheet\" href = \"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css\" > Run a simple server \u00b6 Your index.html should now look like: <!doctype html> < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello Gui </ title > < meta name = \"description\" content = \"Gui for a Holochain app\" > < link rel = \"stylesheet\" href = \"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css\" > </ head > < body > < button type = \"button\" > Say Hello </ button > </ body > </ html > Enter the nix-shell to make sure you have all the dependencies available: nix-shell https://holochain.love Once that is all up and running, you can fire up a simple server: python -m SimpleHTTPServer And go have a look in your browser at http://0.0.0.0:8000/ . You will see something like this: hc-web-client \u00b6 Time to communicate with the app that you built in the previous tutorials. To make this easy you can use the hc-web-client . It's Holochain's JavaScript library that helps you easily setup a WebSocket connection to your app. Why WebSocket instead of HTTP? \u00b6 Having a WebSocket connection open allows your app to send messages to your GUI. While we are not doing that today, it's good to get familiar with this process. To make this process easy we have precompiled a version of the hc-web-client for you. Download it here , then unzip it and stick it in your GUI directory so that the files lives here: gui/hc-web-client/hc-web-client-0.5.1.browser.min.js gui/hc-web-client/hc-web-client-0.5.1.browser.min.js.map Once that's done you can easily link to the compiled js file by adding this script tag inside your body tag: < script type = \"text/javascript\" src = \"hc-web-client/hc-web-client-0.5.1.browser.min.js\" ></ script > Call the zome function \u00b6 Now that you have linked the hc-web-client.js library you can make a simple zome call with some vanilla JavaScript. Add this function inside your <body> tag: < script type = \"text/javascript\" > Make a WebSocket connection to Holochain on port 3401: var holochain_connection = holochainclient . connect ({ url : \"ws://localhost:3401\" }); Add a hello() JavaScript function so you can call it from your HTML: function hello () { Wait for Holochain to connect and then make a zome call: holochain_connection . then (({ callZome , close }) => { Call the hello_holo zome function in the hello zome running on the test-instance instance: callZome ( 'test-instance' , 'hello' , 'hello_holo' )({ \"args\" : {} }) Log the result in the browser's console: . then (( result ) => console . log ( result )) }) } Close the script tag: < /script> This hello function will connect to your app through WebSocket on port 3401 , call the hello zome function, and print the result to your browser's console. Let's make your button call this function by adding an onclick event handler. Add this button inside the <body> tag: < button onclick = \"hello()\" type = \"button\" > Say Hello </ button > Run your app \u00b6 To make a call from the GUI, your Holochain app must be running. So open up a new terminal window, navigate to the app you built in the previous tutorials, and enter the nix-shell: cd holochain/core_concepts/hello nix-shell https://holochain.love Now run your app: hc package hc run -p 3401 Make a zome call \u00b6 Your index.html should look like this: <! doctype html > < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello Gui < /title> < meta name = \"description\" content = \"Gui for a Holochain app\" > < meta name = \"author\" content = \"Holochain\" > < link rel = \"stylesheet\" href = \"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css\" > < /head> < body > < button onclick = \"hello()\" type = \"button\" > Say Hello < /button> < div >< span > Response :< /span><span id=\"output\"></span></div> < script type = \"text/javascript\" src = \"hc-web-client/hc-web-client-0.5.1.browser.min.js\" >< /script> < script type = \"text/javascript\" > // Connection state var holochain_connection = holochainclient . connect ({ url : \"ws://localhost:3401\" }); function hello () { holochain_connection . then (({ callZome , close }) => { callZome ( 'test-instance' , 'hello' , 'hello_holo' )({ \"args\" : {} }). then (( result ) => update_span ( result )) }) } < /script> < /body> < /html> In your other terminal window (the one with the GUI code) run the SimpleHTTPServer again: python -m SimpleHTTPServer Open up your browser and head to 0.0.0.0:8000 . The page will look the same but open you your developer console and click the \"Say Hello\" button. You should see something like this: I'm using Firefox so this might look a little different depending on your browser Woohoo! You have made a call to your Holochain app using a GUI. Render the output \u00b6 It would be nicer if we didn't need to use the developer console to see the result of the hello_holo call. So let's add a place on the page to show it. Add the following HTML inside your <body> tag: < div > Response: < span id = \"output\" ></ span ></ div > The id=\"output\" is what we will use to update this element from a JavaScript function. Add the following lines below you hello function. Add an update_span function that takes the result: function update_span ( result ) { Get the element that you'll be inserting the output into: var span = document . getElementById ( 'output' ); Parse the zome function result as JSON: var output = JSON . parse ( result ); Set the contents of the element to the zome function result: span . textContent = \" \" + output . Ok ; } Finally, update the hello function to call your new update_span function instead of console.log() . Test the output works \u00b6 Head over to 0.0.0.0:8000 in your web browser (you might need to refresh) and you should see this: Now press the Say Hello button and you get your response: Well done! You have a working GUI that can talk to your Holochain app.","title":"Hello GUI"},{"location":"coreconcepts/hello_gui/#hello-gui","text":"Welcome to the first GUI tutorial. So far we have interacted with our zome using curl or hc test , but that's not as nice as having a GUI. Today you will learn how to interact with a Holochain app using a super simple web page.","title":"Hello GUI"},{"location":"coreconcepts/hello_gui/#html-page","text":"You will need somewhere for all your GUI code to live. This will be a different piece of software to your Holochain zome code. So choose somewhere outside your Holochain application. Create a folder for our GUI to live in: cd holochain/coreconcepts mkdir gui cd gui Create a new file called index.html in your favourite editor. It should live at gui/index.html . Start by adding a simple HTML template to index.html . Add this modern template: <!doctype html> < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello GUI </ title > < meta name = \"description\" content = \"GUI for a Holochain app\" > </ head > < body > </ body > </ html > Inside the <body> tag add a button: < button type = \"button\" > Say Hello </ button > To make things a bit nicer on the eyes you can add the water.css stylesheet. Add this water.css link inside the <head> tag: < link rel = \"stylesheet\" href = \"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css\" >","title":"HTML page"},{"location":"coreconcepts/hello_gui/#run-a-simple-server","text":"Your index.html should now look like: <!doctype html> < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello Gui </ title > < meta name = \"description\" content = \"Gui for a Holochain app\" > < link rel = \"stylesheet\" href = \"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css\" > </ head > < body > < button type = \"button\" > Say Hello </ button > </ body > </ html > Enter the nix-shell to make sure you have all the dependencies available: nix-shell https://holochain.love Once that is all up and running, you can fire up a simple server: python -m SimpleHTTPServer And go have a look in your browser at http://0.0.0.0:8000/ . You will see something like this:","title":"Run a simple server"},{"location":"coreconcepts/hello_gui/#hc-web-client","text":"Time to communicate with the app that you built in the previous tutorials. To make this easy you can use the hc-web-client . It's Holochain's JavaScript library that helps you easily setup a WebSocket connection to your app.","title":"hc-web-client"},{"location":"coreconcepts/hello_gui/#why-websocket-instead-of-http","text":"Having a WebSocket connection open allows your app to send messages to your GUI. While we are not doing that today, it's good to get familiar with this process. To make this process easy we have precompiled a version of the hc-web-client for you. Download it here , then unzip it and stick it in your GUI directory so that the files lives here: gui/hc-web-client/hc-web-client-0.5.1.browser.min.js gui/hc-web-client/hc-web-client-0.5.1.browser.min.js.map Once that's done you can easily link to the compiled js file by adding this script tag inside your body tag: < script type = \"text/javascript\" src = \"hc-web-client/hc-web-client-0.5.1.browser.min.js\" ></ script >","title":"Why WebSocket instead of HTTP?"},{"location":"coreconcepts/hello_gui/#call-the-zome-function","text":"Now that you have linked the hc-web-client.js library you can make a simple zome call with some vanilla JavaScript. Add this function inside your <body> tag: < script type = \"text/javascript\" > Make a WebSocket connection to Holochain on port 3401: var holochain_connection = holochainclient . connect ({ url : \"ws://localhost:3401\" }); Add a hello() JavaScript function so you can call it from your HTML: function hello () { Wait for Holochain to connect and then make a zome call: holochain_connection . then (({ callZome , close }) => { Call the hello_holo zome function in the hello zome running on the test-instance instance: callZome ( 'test-instance' , 'hello' , 'hello_holo' )({ \"args\" : {} }) Log the result in the browser's console: . then (( result ) => console . log ( result )) }) } Close the script tag: < /script> This hello function will connect to your app through WebSocket on port 3401 , call the hello zome function, and print the result to your browser's console. Let's make your button call this function by adding an onclick event handler. Add this button inside the <body> tag: < button onclick = \"hello()\" type = \"button\" > Say Hello </ button >","title":"Call the zome function"},{"location":"coreconcepts/hello_gui/#run-your-app","text":"To make a call from the GUI, your Holochain app must be running. So open up a new terminal window, navigate to the app you built in the previous tutorials, and enter the nix-shell: cd holochain/core_concepts/hello nix-shell https://holochain.love Now run your app: hc package hc run -p 3401","title":"Run your app"},{"location":"coreconcepts/hello_gui/#make-a-zome-call","text":"Your index.html should look like this: <! doctype html > < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello Gui < /title> < meta name = \"description\" content = \"Gui for a Holochain app\" > < meta name = \"author\" content = \"Holochain\" > < link rel = \"stylesheet\" href = \"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css\" > < /head> < body > < button onclick = \"hello()\" type = \"button\" > Say Hello < /button> < div >< span > Response :< /span><span id=\"output\"></span></div> < script type = \"text/javascript\" src = \"hc-web-client/hc-web-client-0.5.1.browser.min.js\" >< /script> < script type = \"text/javascript\" > // Connection state var holochain_connection = holochainclient . connect ({ url : \"ws://localhost:3401\" }); function hello () { holochain_connection . then (({ callZome , close }) => { callZome ( 'test-instance' , 'hello' , 'hello_holo' )({ \"args\" : {} }). then (( result ) => update_span ( result )) }) } < /script> < /body> < /html> In your other terminal window (the one with the GUI code) run the SimpleHTTPServer again: python -m SimpleHTTPServer Open up your browser and head to 0.0.0.0:8000 . The page will look the same but open you your developer console and click the \"Say Hello\" button. You should see something like this: I'm using Firefox so this might look a little different depending on your browser Woohoo! You have made a call to your Holochain app using a GUI.","title":"Make a zome call"},{"location":"coreconcepts/hello_gui/#render-the-output","text":"It would be nicer if we didn't need to use the developer console to see the result of the hello_holo call. So let's add a place on the page to show it. Add the following HTML inside your <body> tag: < div > Response: < span id = \"output\" ></ span ></ div > The id=\"output\" is what we will use to update this element from a JavaScript function. Add the following lines below you hello function. Add an update_span function that takes the result: function update_span ( result ) { Get the element that you'll be inserting the output into: var span = document . getElementById ( 'output' ); Parse the zome function result as JSON: var output = JSON . parse ( result ); Set the contents of the element to the zome function result: span . textContent = \" \" + output . Ok ; } Finally, update the hello function to call your new update_span function instead of console.log() .","title":"Render the output"},{"location":"coreconcepts/hello_gui/#test-the-output-works","text":"Head over to 0.0.0.0:8000 in your web browser (you might need to refresh) and you should see this: Now press the Say Hello button and you get your response: Well done! You have a working GUI that can talk to your Holochain app.","title":"Test the output works"},{"location":"coreconcepts/hello_holo/","text":"Hello Holo Tutorial \u00b6 Let's begin with the classic Hello World Holo tutorial! You will see it's super easy to create a distributed application with Holochain. Setup \u00b6 Complete the installation guide . This will give you an app development environment including the Holochain developer tool hc . Open up a terminal (command prompt in Windows). Enter the development environment. macOS/Linux (you'll remember this command from the installation tutorial): nix-shell https://holochain.love Windows (do this in the place where you installed Holochain): vagrant up vagrant ssh nix-shell https://holochain.love Initializing your new app \u00b6 Pick a new home for all your future Holochain applications to live. Something like home_directory/holochain/ . Then create a coreconcepts folder for this tutorial series: cd ~ mkdir holochain cd holochain mkdir coreconcepts cd coreconcepts Time to put the holochain command line tool ( hc ) to work and make your app. Initialize a new app and enter the app directory: Run in nix-shell hc init cc_tuts cd cc_tuts Compile \u00b6 It's an always good to frequently compile your app. That way you catch any mistakes early on. Give it a go by asking hc to package your app: Run in nix-shell hc package Packaging your app means you are compiling the code into a DNA file and getting it ready to be run. You should see a successful compilation like this: Created DNA package file at \"/Users/username/holochain/testing_tuts/hello_holo/dist/hello_holo.dna.json\" DNA hash: QmY 7 rhg 4 sf 6 xqQMRL 1 u 1 CnXVgmamTfxC 59 c 9 RaoFqM 2 eRs Generate a zome \u00b6 Your app doesn't really do too much right now because it needs a zome . A zome is Holochain's way of organizing code into nice units that perform a certain task (like saying hello). Generate a zome called hello inside the zome's folder: Run in nix-shell hc generate zomes/hello rust-proc Compile \u00b6 Run in nix-shell hc package Zomes can take a little while to compile the first time. Compiling will be much faster the next time you do it. Feel free to move on with the tutorial while your app compiles. If all went well you should see: > cargo build --release --target = wasm32-unknown-unknown --target-dir = target Compiling hello v0.1.0 ( /Users/username/holochain/core_concepts/hello_hollo/zomes/hello/code ) Finished release [ optimized ] target ( s ) in 11 .95s > cargo build --release --target = wasm32-unknown-unknown --target-dir = target Finished release [ optimized ] target ( s ) in 0 .50s Created DNA package file at \"/Users/username/holochain/core_concepts/hello_hollo/dist/hello_hollo.dna.json\" DNA hash: QmdNyxke1Z9Kunws4WUXHnt4cdKQnPogC7YPpfQx67fo1z Folder layout \u00b6 Look at the folder layout \u00b6 Open the lib.rs file \u00b6 The zome is a Rust project and makes use of macros so you can avoid writing a lot of boilerplate code. The main file you will be editing is hello_hollo/zomes/code/src/lib.rs . Open up the lib.rs file in an editor and let's have a look at the generated code. The following are all the imports. You are telling Rust, \"hey, I need things from all these crates in order to do my job.\" #![feature(proc_macro_hygiene)] extern crate hdk ; extern crate hdk_proc_macros ; extern crate serde ; #[macro_use] extern crate serde_derive ; extern crate serde_json ; extern crate holochain_json_derive ; Next are the use statements. They are saying, \"I want to use these specific things from the above crates.\" You only need a few items for this tutorial so go ahead and remove the others: #![feature(proc_macro_hygiene)] - #[macro_use] extern crate hdk; extern crate hdk_proc_macros; extern crate serde; #[macro_use] extern crate serde_derive; extern crate serde_json; - #[macro_use] extern crate holochain_json_derive; use hdk::{ - entry_definition::ValidatingEntryType, error::ZomeApiResult, }; - use hdk::holochain_core_types::{ - entry::Entry, - dna::entry_types::Sharing, - }; - use hdk::holochain_json_api::{ - json::JsonString, - error::JsonError - }; - use hdk::holochain_persistence_api::{ - cas::content::Address - }; use hdk_proc_macros::zome; You should be left with this: use hdk :: { error :: ZomeApiResult , }; use hdk_proc_macros :: zome ; There are a few sections of generated code that are not useful for this tutorial. Remove the following piece of code: - #[derive(Serialize, Deserialize, Debug, DefaultJson,Clone)] - pub struct MyEntry { - content: String, - } The my_zome module is where all your zome code live. #[zome] is a procedural macro that says that the following module defines all the things that Holochain should know about this zome. It saves you writing lots of code. Change it to hello_zome for this tutorial series: #[zome] - mod my_zome { + mod hello_zome { The init function is run when a user starts the app for the first time. Every zome defines this function so it can do some initial setup tasks. In this zome it doesn't do anything. #[init] fn init () { Return success with the empty value () . In Rust () is called the unit type and is similar (though not identical) to a void type in other languages. Ok (()) } This required function is run at application start too, once by the new user and once by the existing peers. It checks that the user is allowed to join the network. In this case it gives everyone a free pass. #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } Remove the following template code: - #[entry_def] - fn my_entry_def() -> ValidatingEntryType { - entry!( - name: \"my_entry\", - description: \"this is a same entry defintion\", - sharing: Sharing::Public, - validation_package: || { - hdk::ValidationPackageDefinition::Entry - }, - validation: | _validation_data: hdk::EntryValidationData<MyEntry>| { - Ok(()) - } - ) - } - - #[zome_fn(\"hc_public\")] - fn create_my_entry(entry: MyEntry) -> ZomeApiResult<Address> { - let entry = Entry::App(\"my_entry\".into(), entry.into()); - let address = hdk::commit_entry(&entry)?; - Ok(address) - } - - #[zome_fn(\"hc_public\")] - fn get_my_entry(address: Address) -> ZomeApiResult<Option<Entry>> { - hdk::get_entry(&address) - } A note about return values \u00b6 You'll often see Rust functions returning some sort of Result value. This is a special Rust type that can either be Ok(some_value) to show that the function succeeded or Err(some_error) to report an error. All required Holochain functions, such as init and validators, are expected return a special result type called ZomeApiResult , which shuttles data back and forth between your app and the conductor. It also automatically converts data to JSON and back, so it makes sense to use it in your public functions too. Add a function to say hello :) \u00b6 Now tell the zome to return Hello Holo from a public function. Locate the validate_agent function: pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } You're going to put your public zome function after it. The hc_public procedural macro will turn the function directly below it into a public function that GUIs, other zomes, and DNAs can call. It takes note of the function's name, the parameters it accepts, and the type of value it returns, so Holochain can call it properly. Add the hc_public macro: #[zome_fn( \"hc_public\" )] The function hello_holo takes no arguments and returns a Holochain result type. We're also telling Holochain that if the result is Ok then it will contain a string. Start the function: fn hello_holo () -> ZomeApiResult < String > { Return an Ok result that contains our greeting. into() is a bit of Rust oddness that just means \"turn this slice into a String \": Ok ( \"Hello Holo\" . into ()) } Compile \u00b6 Check your code #![feature(proc_macro_hygiene)] extern crate hdk ; extern crate hdk_proc_macros ; extern crate serde ; #[macro_use] extern crate serde_derive ; extern crate serde_json ; extern crate holochain_json_derive ; use hdk :: { error :: ZomeApiResult , }; use hdk_proc_macros :: zome ; #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } } If you do find any errors, remember to fix them before moving on. You can always get help on the forum . Run in nix-shell hc package Talk to your app through HTTP \u00b6 To interact with your application you can run it in HTTP mode. Run in nix-shell hc run -i http You can send a POST message to your app using curl , a little command for making HTTP requests. (It's included in the Holochain dev environment.) You will need to open a new terminal window and enter the nix-shell again: nix-shell https://holochain.love Enter the following request, which will call the hello_holo function and return the result: Run in nix-shell curl -X POST -H \"Content-Type: application/json\" -d '{\"id\": \"0\", \"jsonrpc\": \"2.0\", \"method\": \"call\", \"params\": {\"instance_id\": \"test-instance\", \"zome\": \"hello\", \"function\": \"hello_holo\", \"args\": {} }}' http://127.0.0.1:8888 And you should get back your string from the hello_holo function: { \"jsonrpc\" : \"2.0\" , \"result\" : \"{\\\"Ok\\\":\\\"Hello Holo\\\"}\" , \"id\" : \"0\" } Congratulations --- you have created your first distributed Holochain application!","title":"Hello Holo Tutorial"},{"location":"coreconcepts/hello_holo/#hello-holo-tutorial","text":"Let's begin with the classic Hello World Holo tutorial! You will see it's super easy to create a distributed application with Holochain.","title":"Hello Holo Tutorial"},{"location":"coreconcepts/hello_holo/#setup","text":"Complete the installation guide . This will give you an app development environment including the Holochain developer tool hc . Open up a terminal (command prompt in Windows). Enter the development environment. macOS/Linux (you'll remember this command from the installation tutorial): nix-shell https://holochain.love Windows (do this in the place where you installed Holochain): vagrant up vagrant ssh nix-shell https://holochain.love","title":"Setup"},{"location":"coreconcepts/hello_holo/#initializing-your-new-app","text":"Pick a new home for all your future Holochain applications to live. Something like home_directory/holochain/ . Then create a coreconcepts folder for this tutorial series: cd ~ mkdir holochain cd holochain mkdir coreconcepts cd coreconcepts Time to put the holochain command line tool ( hc ) to work and make your app. Initialize a new app and enter the app directory: Run in nix-shell hc init cc_tuts cd cc_tuts","title":"Initializing your new app"},{"location":"coreconcepts/hello_holo/#compile","text":"It's an always good to frequently compile your app. That way you catch any mistakes early on. Give it a go by asking hc to package your app: Run in nix-shell hc package Packaging your app means you are compiling the code into a DNA file and getting it ready to be run. You should see a successful compilation like this: Created DNA package file at \"/Users/username/holochain/testing_tuts/hello_holo/dist/hello_holo.dna.json\" DNA hash: QmY 7 rhg 4 sf 6 xqQMRL 1 u 1 CnXVgmamTfxC 59 c 9 RaoFqM 2 eRs","title":"Compile"},{"location":"coreconcepts/hello_holo/#generate-a-zome","text":"Your app doesn't really do too much right now because it needs a zome . A zome is Holochain's way of organizing code into nice units that perform a certain task (like saying hello). Generate a zome called hello inside the zome's folder: Run in nix-shell hc generate zomes/hello rust-proc","title":"Generate a zome"},{"location":"coreconcepts/hello_holo/#compile_1","text":"Run in nix-shell hc package Zomes can take a little while to compile the first time. Compiling will be much faster the next time you do it. Feel free to move on with the tutorial while your app compiles. If all went well you should see: > cargo build --release --target = wasm32-unknown-unknown --target-dir = target Compiling hello v0.1.0 ( /Users/username/holochain/core_concepts/hello_hollo/zomes/hello/code ) Finished release [ optimized ] target ( s ) in 11 .95s > cargo build --release --target = wasm32-unknown-unknown --target-dir = target Finished release [ optimized ] target ( s ) in 0 .50s Created DNA package file at \"/Users/username/holochain/core_concepts/hello_hollo/dist/hello_hollo.dna.json\" DNA hash: QmdNyxke1Z9Kunws4WUXHnt4cdKQnPogC7YPpfQx67fo1z","title":"Compile"},{"location":"coreconcepts/hello_holo/#folder-layout","text":"","title":"Folder layout"},{"location":"coreconcepts/hello_holo/#look-at-the-folder-layout","text":"","title":"Look at the folder layout"},{"location":"coreconcepts/hello_holo/#open-the-librs-file","text":"The zome is a Rust project and makes use of macros so you can avoid writing a lot of boilerplate code. The main file you will be editing is hello_hollo/zomes/code/src/lib.rs . Open up the lib.rs file in an editor and let's have a look at the generated code. The following are all the imports. You are telling Rust, \"hey, I need things from all these crates in order to do my job.\" #![feature(proc_macro_hygiene)] extern crate hdk ; extern crate hdk_proc_macros ; extern crate serde ; #[macro_use] extern crate serde_derive ; extern crate serde_json ; extern crate holochain_json_derive ; Next are the use statements. They are saying, \"I want to use these specific things from the above crates.\" You only need a few items for this tutorial so go ahead and remove the others: #![feature(proc_macro_hygiene)] - #[macro_use] extern crate hdk; extern crate hdk_proc_macros; extern crate serde; #[macro_use] extern crate serde_derive; extern crate serde_json; - #[macro_use] extern crate holochain_json_derive; use hdk::{ - entry_definition::ValidatingEntryType, error::ZomeApiResult, }; - use hdk::holochain_core_types::{ - entry::Entry, - dna::entry_types::Sharing, - }; - use hdk::holochain_json_api::{ - json::JsonString, - error::JsonError - }; - use hdk::holochain_persistence_api::{ - cas::content::Address - }; use hdk_proc_macros::zome; You should be left with this: use hdk :: { error :: ZomeApiResult , }; use hdk_proc_macros :: zome ; There are a few sections of generated code that are not useful for this tutorial. Remove the following piece of code: - #[derive(Serialize, Deserialize, Debug, DefaultJson,Clone)] - pub struct MyEntry { - content: String, - } The my_zome module is where all your zome code live. #[zome] is a procedural macro that says that the following module defines all the things that Holochain should know about this zome. It saves you writing lots of code. Change it to hello_zome for this tutorial series: #[zome] - mod my_zome { + mod hello_zome { The init function is run when a user starts the app for the first time. Every zome defines this function so it can do some initial setup tasks. In this zome it doesn't do anything. #[init] fn init () { Return success with the empty value () . In Rust () is called the unit type and is similar (though not identical) to a void type in other languages. Ok (()) } This required function is run at application start too, once by the new user and once by the existing peers. It checks that the user is allowed to join the network. In this case it gives everyone a free pass. #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } Remove the following template code: - #[entry_def] - fn my_entry_def() -> ValidatingEntryType { - entry!( - name: \"my_entry\", - description: \"this is a same entry defintion\", - sharing: Sharing::Public, - validation_package: || { - hdk::ValidationPackageDefinition::Entry - }, - validation: | _validation_data: hdk::EntryValidationData<MyEntry>| { - Ok(()) - } - ) - } - - #[zome_fn(\"hc_public\")] - fn create_my_entry(entry: MyEntry) -> ZomeApiResult<Address> { - let entry = Entry::App(\"my_entry\".into(), entry.into()); - let address = hdk::commit_entry(&entry)?; - Ok(address) - } - - #[zome_fn(\"hc_public\")] - fn get_my_entry(address: Address) -> ZomeApiResult<Option<Entry>> { - hdk::get_entry(&address) - }","title":"Open the lib.rs file"},{"location":"coreconcepts/hello_holo/#a-note-about-return-values","text":"You'll often see Rust functions returning some sort of Result value. This is a special Rust type that can either be Ok(some_value) to show that the function succeeded or Err(some_error) to report an error. All required Holochain functions, such as init and validators, are expected return a special result type called ZomeApiResult , which shuttles data back and forth between your app and the conductor. It also automatically converts data to JSON and back, so it makes sense to use it in your public functions too.","title":"A note about return values"},{"location":"coreconcepts/hello_holo/#add-a-function-to-say-hello","text":"Now tell the zome to return Hello Holo from a public function. Locate the validate_agent function: pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } You're going to put your public zome function after it. The hc_public procedural macro will turn the function directly below it into a public function that GUIs, other zomes, and DNAs can call. It takes note of the function's name, the parameters it accepts, and the type of value it returns, so Holochain can call it properly. Add the hc_public macro: #[zome_fn( \"hc_public\" )] The function hello_holo takes no arguments and returns a Holochain result type. We're also telling Holochain that if the result is Ok then it will contain a string. Start the function: fn hello_holo () -> ZomeApiResult < String > { Return an Ok result that contains our greeting. into() is a bit of Rust oddness that just means \"turn this slice into a String \": Ok ( \"Hello Holo\" . into ()) }","title":"Add a function to say hello :)"},{"location":"coreconcepts/hello_holo/#compile_2","text":"Check your code #![feature(proc_macro_hygiene)] extern crate hdk ; extern crate hdk_proc_macros ; extern crate serde ; #[macro_use] extern crate serde_derive ; extern crate serde_json ; extern crate holochain_json_derive ; use hdk :: { error :: ZomeApiResult , }; use hdk_proc_macros :: zome ; #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } } If you do find any errors, remember to fix them before moving on. You can always get help on the forum . Run in nix-shell hc package","title":"Compile"},{"location":"coreconcepts/hello_holo/#talk-to-your-app-through-http","text":"To interact with your application you can run it in HTTP mode. Run in nix-shell hc run -i http You can send a POST message to your app using curl , a little command for making HTTP requests. (It's included in the Holochain dev environment.) You will need to open a new terminal window and enter the nix-shell again: nix-shell https://holochain.love Enter the following request, which will call the hello_holo function and return the result: Run in nix-shell curl -X POST -H \"Content-Type: application/json\" -d '{\"id\": \"0\", \"jsonrpc\": \"2.0\", \"method\": \"call\", \"params\": {\"instance_id\": \"test-instance\", \"zome\": \"hello\", \"function\": \"hello_holo\", \"args\": {} }}' http://127.0.0.1:8888 And you should get back your string from the hello_holo function: { \"jsonrpc\" : \"2.0\" , \"result\" : \"{\\\"Ok\\\":\\\"Hello Holo\\\"}\" , \"id\" : \"0\" } Congratulations --- you have created your first distributed Holochain application!","title":"Talk to your app through HTTP"},{"location":"coreconcepts/hello_me/","text":"Hello Me \u00b6 Welcome back to another tutorial in the Core Concepts series. Today you will learn how to add an entry type to your zome and start writing entries to your source chain. Remember an entry is a piece of data in your source chain that has been validated. The design of today's app will be: Add a person entry type that stores information about a person. Expose the public function create_person for your UI to create and store a person entry. Expose a public function retrieve_person for your UI to retrieve a person entry. Add the UI components to interact with these functions. This tutorial builds on the previous tutorial so go back and complete that if you haven't already. Test first \u00b6 Let's start by writing a test so it's easy to see we have a working app before writing the UI. Open up your tutorial/test/index.js . This is how we left the testing scenario in the Hello Test tutorial: diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { // Make a call to the `hello_holo` Zome function // passing no arguments. const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); // Make sure the result is ok. t . ok ( result . Ok ); // Check that the result matches what you expected. t . deepEqual ( result , { Ok : 'Hello Holo' }) // <---- Put your new tests here }) Add the new tests below t.deepEqual(result, { Ok: 'Hello Holo' }) . Add a call to the create_person function with a person whose name is Alice: const create_result = await alice . call ( \"hello\" , \"create_person\" , { \"person\" : { \"name\" : \"Alice\" }}); Check that the result of the call is Ok: t . ok ( create_result . Ok ); Add a call to the retrieve_person function with the address from the last call: const retrieve_result = await alice . call ( \"hello\" , \"retrieve_person\" , { \"address\" : create_result . Ok }); Check that this call is Ok as well: t . ok ( retrieve_result . Ok ); This is the actual result we want at the end of the test. Check that the entry at the address is indeed Alice : t . deepEqual ( retrieve_result , { Ok : { App : [ 'person' , '{\"name\":\"Alice\"}' ] }}) Running the test \u00b6 Your test should now look like this: Check your code diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { // Make a call to the `hello_holo` Zome function // passing no arguments. const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); // Make sure the result is ok. t . ok ( result . Ok ); // Check that the result matches what you expected. t . deepEqual ( result , { Ok : 'Hello Holo' }) // <---- Put your new tests here const create_result = await alice . call ( \"hello\" , \"create_person\" , { \"person\" : { \"name\" : \"Alice\" }}); t . ok ( create_result . Ok ); const retrieve_result = await alice . call ( \"hello\" , \"retrieve_person\" , { \"address\" : create_result . Ok }); t . ok ( retrieve_result . Ok ); t . deepEqual ( retrieve_result , { Ok : { App : [ 'person' , '{\"name\":\"Alice\"}' ] }}) }) Obviously these tests will fail right now. Can you guess what the first failure will be? Let's have a look. Enter the nix-shell if you don't have it open already: nix-shell https://holochain.love Run the test: nix-shell ] hc test Note that this test might actually get stuck because we haven't put in the required functions yet. Press ctrl-c to exit a stuck test. Entry \u00b6 Open up your zomes/hello/code/src/lib.rs file. To add an entry into your source chain you start by telling Holochain what kinds of entry exist. First we'll create a struct to define the shape of the data. In a moment we will add a Person struct, but this is where to put it: // <---- Add the person struct here. #[zome] mod hello_zome { Add the following lines. Allow this struct to be easily converted to and from JSON: #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] Represent a person as a struct: pub struct Person { Represent their name as a String: name : String , } Look for the following lines inside the hello_zome mod. #[zome] mod hello_zome { /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. Add the person_entry_def function, which tells Holochain about the person entry type: #[entry_def] fn person_entry_def () -> ValidatingEntryType { Add the entry! macro that lets you easily create a ValidatingEntryType : entry ! ( Give it the same name as the Person struct, just to be consistent. Entry types are usually in lowercase. Add the name and description of the entry: name : \"person\" , description : \"Person to say hello to\" , Entries of this type are just for this agent's eyes only, so set the entry sharing to private: sharing : Sharing :: Private , Add the validation_package function that says what is needed to validate this entry: validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, Add the validation function that validates this entry. It returns that this entry is always Ok as long as it fits the shape of the Person struct: validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } Now you can create actual person entries and store them on your source chain. A note on validation: Validation is very important. It is the \"rules of the game\" for your Holochain app. It is meaningful to emphasize that although we are returning Ok(()) that we are still validating that the data type checks as a Person with a name property containing a String . Essentially this rule says the person entry must be in this format. Add some use statements \u00b6 In the above code we have used a few types and macros that are not mentioned anywhere else. So the Rust compiler doesn't know where to find them yet. Add the following use statements: Compile \u00b6 Check your code #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Person { name : String , } #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. #[entry_def] fn person_entry_def () -> ValidatingEntryType { entry ! ( name : \"person\" , description : \"Person to say hello to\" , sharing : Sharing :: Private , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } Package the app (in the nix-shell) and check that there's no compile errors: nix-shell ] hc package Create a person \u00b6 Now you need a way for you UI to actually create a person entry. Holochain has a concept called hc_public which is a way of telling the runtime make this function available to call from outside this zome. Add the following lines below the previous person_entry_def function. Add a public function that takes a Person and returns a result with an Address : #[zome_fn( \"hc_public\" )] pub fn create_person ( person : Person ) -> ZomeApiResult < Address > { Create an entry from the passed argument: let entry = Entry :: App ( \"person\" . into (), person . into ()); Commit the entry to your local source chain: let address = hdk :: commit_entry ( & entry ) ? ; Return the Ok result with the new person entry's address: Ok ( address ) } Compile \u00b6 Check your code #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Person { name : String , } #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. #[entry_def] fn person_entry_def () -> ValidatingEntryType { entry ! ( name : \"person\" , description : \"Person to say hello to\" , sharing : Sharing :: Private , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } #[zome_fn( \"hc_public\" )] pub fn create_person ( person : Person ) -> ZomeApiResult < Address > { let entry = Entry :: App ( \"person\" . into (), person . into ()); let address = hdk :: commit_entry ( & entry ) ? ; Ok ( address ) } Check for compile errors again: nix-shell ] hc package Retrieve person \u00b6 Lastly you need a way for your UI to get a person entry back from the source chain. Add the following lines below the create_person function. Add a public retrieve_person function that takes an Address and maybe returns an Entry (because it might not exist): #[zome_fn( \"hc_public\" )] fn retrieve_person ( address : Address ) -> ZomeApiResult < Option < Entry >> { Get the entry from your local storage, asking for it by address: hdk :: get_entry ( & address ) } In Rust the last line is always returned. You do not need to explicitly say return . Test \u00b6 Check your code #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Person { name : String , } #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. #[entry_def] fn person_entry_def () -> ValidatingEntryType { entry ! ( name : \"person\" , description : \"Person to say hello to\" , sharing : Sharing :: Private , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } #[zome_fn( \"hc_public\" )] pub fn create_person ( person : Person ) -> ZomeApiResult < Address > { let entry = Entry :: App ( \"person\" . into (), person . into ()); let address = hdk :: commit_entry ( & entry ) ? ; Ok ( address ) } #[zome_fn( \"hc_public\" )] fn retrieve_person ( address : Address ) -> ZomeApiResult < Option < Entry >> { hdk :: get_entry ( & address ) } Instead of directly compiling, you can run the test you wrote at the start (the test always compiles before it runs): nix-shell ] hc test If everything went smoothly you will see: 1 ..5 # tests 5 # pass 5 # ok UI \u00b6 Now you can move onto the modifying the UI to interact with zome functions you just created. First let's do some housekeeping and move the JavaScript from the previous tutorial into its own file. Go to the GUI project folder that you created in the Hello GUI tutorial: cd holochain/coreconcepts/gui Create a new hello.js file, open it in your favorite editor, and open the index.html alongside it. Move the everything inside the <script> tag into the hello.js : < script type = \"text/javascript\" > <!-- Everything from HERE to --> // Connection state var holochain_connection = holochainclient . connect ({ url : \"ws://localhost:3401\" }); function hello () { holochain_connection . then (({ callZome , close }) => { callZome ( 'test-instance' , 'hello' , 'hello_holo' )({ \"args\" : {} }). then (( result ) => update_span ( result )) }) } function update_span ( result ) { var span = document . getElementById ( 'output' ); var output = JSON . parse ( result ); span . textContent = \" \" + output . Ok ; } <!-- HERE --> < /script> Add the src attribute to the <script> tag: < script type = \"text/javascript\" src = \"hello.js\" ></ script > Create person UI widget \u00b6 Let's start with the HTML elements to create a person. Look for the previous 'say hello' elements. < button onclick = \"hello()\" type = \"button\" > Say Hello </ button > < span > Response: </ span >< span id = \"output\" ></ span > <!-- Put the following lines here --> Below them, add a text box so the user can enter their name: < input type = \"text\" id = \"name\" placeholder = \"Enter your name :)\" >< br > Add a button that calls a (yet to be written) JavaScript function called create_person : < button onclick = \"create_person()\" type = \"button\" > Submit Name </ button > Add a span with the id address_output so you can render the result of this call: < div > Address: < span id = \"address_output\" ></ span ></ div > TODO: Add collapsable html code panel. Switch to your hello.js file \u00b6 Let's write the create_person function that will call your zome. Add the create_person function: function create_person () { Get the text box by its ID name and save the current text value into the name variable: var name = document . getElementById ( 'name' ). value ; Wait for the connection and then make a zome call: holochain_connection . then (({ callZome , close }) => { Call create_person in your hello zome and pass in the name variable as part of a person structure, then write the result to the console: callZome ( 'test-instance' , 'hello' , 'create_person' )({ person : { name : name } }). then (( result ) => console . log ( result , 'address_output' )) }) } Run the server and open a browser \u00b6 TODO: Add collapsable hello.js code panel. Go ahead and test your first call. Open a new terminal window and enter the nix-shell: nix-shell https://holochain.love Run the server: nix-shell ] python -m SimpleHTTPServer In your other terminal window package and run your zome: nix-shell ] hc package nix-shell ] hc run -p 8080 Now that both your UI server and your Holochain conductor server are running, open up a browser and go to 0.0.0.0:8000 . You should see the HTML elements you created: Open the developer console, enter your name, and press the \"Submit Name\" button. You should something similar to this: The address you see will probably be different, because you probably typed in your own name. Show the new entry's address \u00b6 Now we're going to show the address on the page rather than the developer console. But first, a bit of refactoring. If you make the update_span function more generic, then you can reuse it for each element that shows the output for a zome function. Pass in the element's ID so that the function can be reused: Enter the browser \u00b6 Go back to your browser and refresh the page. This time when you enter your name and press Submit Name , you will see the address show up: Retrieve a person entry and show it in the UI \u00b6 Back in the index.html file now. Add a text box so the user can enter the address that is returned from the create_person function: < input type = \"text\" id = \"address_in\" placeholder = \"Enter the entry address\" >< br > Add a button that calls the (yet to be written) retrieve_person JavaScript function: < button onclick = \"retrieve_person()\" type = \"button\" > Get Entry </ button > Add a span with the ID entry_output to display the person that is returned from the retrieve_person function: < div > Person: < span id = \"entry_output\" ></ span ></ div > Go to your hello.js file \u00b6 Add the retrieve_person function to call the zome function of the same name and show its response: function retrieve_person () { Get the value from the address_in text box: var address = document . getElementById ( 'address_in' ). value ; Wait for the connection and then make a zome call: holochain_connection . then (({ callZome , close }) => { Call the retrieve_person public zome function, passing in the address. Then pass the result to update_person : callZome ( 'test-instance' , 'hello' , 'retrieve_person' )({ address : address }). then (( result ) => update_person ( result )) }) } Add the update_person function. It is very similar to update_element except that you need to parse the result and then parse the inner array. function update_person ( result ) { var person = document . getElementById ( 'entry_output' ); var output = JSON . parse ( result ); var output = JSON . parse ( output . Ok . App [ 1 ]); person . textContent = \" \" + output . name ; }","title":"Hello Me"},{"location":"coreconcepts/hello_me/#hello-me","text":"Welcome back to another tutorial in the Core Concepts series. Today you will learn how to add an entry type to your zome and start writing entries to your source chain. Remember an entry is a piece of data in your source chain that has been validated. The design of today's app will be: Add a person entry type that stores information about a person. Expose the public function create_person for your UI to create and store a person entry. Expose a public function retrieve_person for your UI to retrieve a person entry. Add the UI components to interact with these functions. This tutorial builds on the previous tutorial so go back and complete that if you haven't already.","title":"Hello Me"},{"location":"coreconcepts/hello_me/#test-first","text":"Let's start by writing a test so it's easy to see we have a working app before writing the UI. Open up your tutorial/test/index.js . This is how we left the testing scenario in the Hello Test tutorial: diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { // Make a call to the `hello_holo` Zome function // passing no arguments. const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); // Make sure the result is ok. t . ok ( result . Ok ); // Check that the result matches what you expected. t . deepEqual ( result , { Ok : 'Hello Holo' }) // <---- Put your new tests here }) Add the new tests below t.deepEqual(result, { Ok: 'Hello Holo' }) . Add a call to the create_person function with a person whose name is Alice: const create_result = await alice . call ( \"hello\" , \"create_person\" , { \"person\" : { \"name\" : \"Alice\" }}); Check that the result of the call is Ok: t . ok ( create_result . Ok ); Add a call to the retrieve_person function with the address from the last call: const retrieve_result = await alice . call ( \"hello\" , \"retrieve_person\" , { \"address\" : create_result . Ok }); Check that this call is Ok as well: t . ok ( retrieve_result . Ok ); This is the actual result we want at the end of the test. Check that the entry at the address is indeed Alice : t . deepEqual ( retrieve_result , { Ok : { App : [ 'person' , '{\"name\":\"Alice\"}' ] }})","title":"Test first"},{"location":"coreconcepts/hello_me/#running-the-test","text":"Your test should now look like this: Check your code diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { // Make a call to the `hello_holo` Zome function // passing no arguments. const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); // Make sure the result is ok. t . ok ( result . Ok ); // Check that the result matches what you expected. t . deepEqual ( result , { Ok : 'Hello Holo' }) // <---- Put your new tests here const create_result = await alice . call ( \"hello\" , \"create_person\" , { \"person\" : { \"name\" : \"Alice\" }}); t . ok ( create_result . Ok ); const retrieve_result = await alice . call ( \"hello\" , \"retrieve_person\" , { \"address\" : create_result . Ok }); t . ok ( retrieve_result . Ok ); t . deepEqual ( retrieve_result , { Ok : { App : [ 'person' , '{\"name\":\"Alice\"}' ] }}) }) Obviously these tests will fail right now. Can you guess what the first failure will be? Let's have a look. Enter the nix-shell if you don't have it open already: nix-shell https://holochain.love Run the test: nix-shell ] hc test Note that this test might actually get stuck because we haven't put in the required functions yet. Press ctrl-c to exit a stuck test.","title":"Running the test"},{"location":"coreconcepts/hello_me/#entry","text":"Open up your zomes/hello/code/src/lib.rs file. To add an entry into your source chain you start by telling Holochain what kinds of entry exist. First we'll create a struct to define the shape of the data. In a moment we will add a Person struct, but this is where to put it: // <---- Add the person struct here. #[zome] mod hello_zome { Add the following lines. Allow this struct to be easily converted to and from JSON: #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] Represent a person as a struct: pub struct Person { Represent their name as a String: name : String , } Look for the following lines inside the hello_zome mod. #[zome] mod hello_zome { /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. Add the person_entry_def function, which tells Holochain about the person entry type: #[entry_def] fn person_entry_def () -> ValidatingEntryType { Add the entry! macro that lets you easily create a ValidatingEntryType : entry ! ( Give it the same name as the Person struct, just to be consistent. Entry types are usually in lowercase. Add the name and description of the entry: name : \"person\" , description : \"Person to say hello to\" , Entries of this type are just for this agent's eyes only, so set the entry sharing to private: sharing : Sharing :: Private , Add the validation_package function that says what is needed to validate this entry: validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, Add the validation function that validates this entry. It returns that this entry is always Ok as long as it fits the shape of the Person struct: validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } Now you can create actual person entries and store them on your source chain. A note on validation: Validation is very important. It is the \"rules of the game\" for your Holochain app. It is meaningful to emphasize that although we are returning Ok(()) that we are still validating that the data type checks as a Person with a name property containing a String . Essentially this rule says the person entry must be in this format.","title":"Entry"},{"location":"coreconcepts/hello_me/#add-some-use-statements","text":"In the above code we have used a few types and macros that are not mentioned anywhere else. So the Rust compiler doesn't know where to find them yet. Add the following use statements:","title":"Add some use statements"},{"location":"coreconcepts/hello_me/#compile","text":"Check your code #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Person { name : String , } #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. #[entry_def] fn person_entry_def () -> ValidatingEntryType { entry ! ( name : \"person\" , description : \"Person to say hello to\" , sharing : Sharing :: Private , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } Package the app (in the nix-shell) and check that there's no compile errors: nix-shell ] hc package","title":"Compile"},{"location":"coreconcepts/hello_me/#create-a-person","text":"Now you need a way for you UI to actually create a person entry. Holochain has a concept called hc_public which is a way of telling the runtime make this function available to call from outside this zome. Add the following lines below the previous person_entry_def function. Add a public function that takes a Person and returns a result with an Address : #[zome_fn( \"hc_public\" )] pub fn create_person ( person : Person ) -> ZomeApiResult < Address > { Create an entry from the passed argument: let entry = Entry :: App ( \"person\" . into (), person . into ()); Commit the entry to your local source chain: let address = hdk :: commit_entry ( & entry ) ? ; Return the Ok result with the new person entry's address: Ok ( address ) }","title":"Create a person"},{"location":"coreconcepts/hello_me/#compile_1","text":"Check your code #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Person { name : String , } #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. #[entry_def] fn person_entry_def () -> ValidatingEntryType { entry ! ( name : \"person\" , description : \"Person to say hello to\" , sharing : Sharing :: Private , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } #[zome_fn( \"hc_public\" )] pub fn create_person ( person : Person ) -> ZomeApiResult < Address > { let entry = Entry :: App ( \"person\" . into (), person . into ()); let address = hdk :: commit_entry ( & entry ) ? ; Ok ( address ) } Check for compile errors again: nix-shell ] hc package","title":"Compile"},{"location":"coreconcepts/hello_me/#retrieve-person","text":"Lastly you need a way for your UI to get a person entry back from the source chain. Add the following lines below the create_person function. Add a public retrieve_person function that takes an Address and maybe returns an Entry (because it might not exist): #[zome_fn( \"hc_public\" )] fn retrieve_person ( address : Address ) -> ZomeApiResult < Option < Entry >> { Get the entry from your local storage, asking for it by address: hdk :: get_entry ( & address ) } In Rust the last line is always returned. You do not need to explicitly say return .","title":"Retrieve person"},{"location":"coreconcepts/hello_me/#test","text":"Check your code #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Person { name : String , } #[zome] mod hello_zome { #[init] fn init () { Ok (()) } #[validate_agent] pub fn validate_agent ( validation_data : EntryValidationData < AgentId > ) { Ok (()) } /* --- Lines omitted -- */ #[zome_fn( \"hc_public\" )] fn hello_holo () -> ZomeApiResult < String > { Ok ( \"Hello Holo\" . into ()) } // <---- Add the following lines here. #[entry_def] fn person_entry_def () -> ValidatingEntryType { entry ! ( name : \"person\" , description : \"Person to say hello to\" , sharing : Sharing :: Private , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Person >| { Ok (()) } ) } #[zome_fn( \"hc_public\" )] pub fn create_person ( person : Person ) -> ZomeApiResult < Address > { let entry = Entry :: App ( \"person\" . into (), person . into ()); let address = hdk :: commit_entry ( & entry ) ? ; Ok ( address ) } #[zome_fn( \"hc_public\" )] fn retrieve_person ( address : Address ) -> ZomeApiResult < Option < Entry >> { hdk :: get_entry ( & address ) } Instead of directly compiling, you can run the test you wrote at the start (the test always compiles before it runs): nix-shell ] hc test If everything went smoothly you will see: 1 ..5 # tests 5 # pass 5 # ok","title":"Test"},{"location":"coreconcepts/hello_me/#ui","text":"Now you can move onto the modifying the UI to interact with zome functions you just created. First let's do some housekeeping and move the JavaScript from the previous tutorial into its own file. Go to the GUI project folder that you created in the Hello GUI tutorial: cd holochain/coreconcepts/gui Create a new hello.js file, open it in your favorite editor, and open the index.html alongside it. Move the everything inside the <script> tag into the hello.js : < script type = \"text/javascript\" > <!-- Everything from HERE to --> // Connection state var holochain_connection = holochainclient . connect ({ url : \"ws://localhost:3401\" }); function hello () { holochain_connection . then (({ callZome , close }) => { callZome ( 'test-instance' , 'hello' , 'hello_holo' )({ \"args\" : {} }). then (( result ) => update_span ( result )) }) } function update_span ( result ) { var span = document . getElementById ( 'output' ); var output = JSON . parse ( result ); span . textContent = \" \" + output . Ok ; } <!-- HERE --> < /script> Add the src attribute to the <script> tag: < script type = \"text/javascript\" src = \"hello.js\" ></ script >","title":"UI"},{"location":"coreconcepts/hello_me/#create-person-ui-widget","text":"Let's start with the HTML elements to create a person. Look for the previous 'say hello' elements. < button onclick = \"hello()\" type = \"button\" > Say Hello </ button > < span > Response: </ span >< span id = \"output\" ></ span > <!-- Put the following lines here --> Below them, add a text box so the user can enter their name: < input type = \"text\" id = \"name\" placeholder = \"Enter your name :)\" >< br > Add a button that calls a (yet to be written) JavaScript function called create_person : < button onclick = \"create_person()\" type = \"button\" > Submit Name </ button > Add a span with the id address_output so you can render the result of this call: < div > Address: < span id = \"address_output\" ></ span ></ div > TODO: Add collapsable html code panel.","title":"Create person UI widget"},{"location":"coreconcepts/hello_me/#switch-to-your-hellojs-file","text":"Let's write the create_person function that will call your zome. Add the create_person function: function create_person () { Get the text box by its ID name and save the current text value into the name variable: var name = document . getElementById ( 'name' ). value ; Wait for the connection and then make a zome call: holochain_connection . then (({ callZome , close }) => { Call create_person in your hello zome and pass in the name variable as part of a person structure, then write the result to the console: callZome ( 'test-instance' , 'hello' , 'create_person' )({ person : { name : name } }). then (( result ) => console . log ( result , 'address_output' )) }) }","title":"Switch to your hello.js file"},{"location":"coreconcepts/hello_me/#run-the-server-and-open-a-browser","text":"TODO: Add collapsable hello.js code panel. Go ahead and test your first call. Open a new terminal window and enter the nix-shell: nix-shell https://holochain.love Run the server: nix-shell ] python -m SimpleHTTPServer In your other terminal window package and run your zome: nix-shell ] hc package nix-shell ] hc run -p 8080 Now that both your UI server and your Holochain conductor server are running, open up a browser and go to 0.0.0.0:8000 . You should see the HTML elements you created: Open the developer console, enter your name, and press the \"Submit Name\" button. You should something similar to this: The address you see will probably be different, because you probably typed in your own name.","title":"Run the server and open a browser"},{"location":"coreconcepts/hello_me/#show-the-new-entrys-address","text":"Now we're going to show the address on the page rather than the developer console. But first, a bit of refactoring. If you make the update_span function more generic, then you can reuse it for each element that shows the output for a zome function. Pass in the element's ID so that the function can be reused:","title":"Show the new entry's address"},{"location":"coreconcepts/hello_me/#enter-the-browser","text":"Go back to your browser and refresh the page. This time when you enter your name and press Submit Name , you will see the address show up:","title":"Enter the browser"},{"location":"coreconcepts/hello_me/#retrieve-a-person-entry-and-show-it-in-the-ui","text":"Back in the index.html file now. Add a text box so the user can enter the address that is returned from the create_person function: < input type = \"text\" id = \"address_in\" placeholder = \"Enter the entry address\" >< br > Add a button that calls the (yet to be written) retrieve_person JavaScript function: < button onclick = \"retrieve_person()\" type = \"button\" > Get Entry </ button > Add a span with the ID entry_output to display the person that is returned from the retrieve_person function: < div > Person: < span id = \"entry_output\" ></ span ></ div >","title":"Retrieve a person entry and show it in the UI"},{"location":"coreconcepts/hello_me/#go-to-your-hellojs-file","text":"Add the retrieve_person function to call the zome function of the same name and show its response: function retrieve_person () { Get the value from the address_in text box: var address = document . getElementById ( 'address_in' ). value ; Wait for the connection and then make a zome call: holochain_connection . then (({ callZome , close }) => { Call the retrieve_person public zome function, passing in the address. Then pass the result to update_person : callZome ( 'test-instance' , 'hello' , 'retrieve_person' )({ address : address }). then (( result ) => update_person ( result )) }) } Add the update_person function. It is very similar to update_element except that you need to parse the result and then parse the inner array. function update_person ( result ) { var person = document . getElementById ( 'entry_output' ); var output = JSON . parse ( result ); var output = JSON . parse ( output . Ok . App [ 1 ]); person . textContent = \" \" + output . name ; }","title":"Go to your hello.js file"},{"location":"coreconcepts/hello_test/","text":"Hello Test Tutorial \u00b6 Welcome to the Hello Test tutorial. Today you will be learning how to test your Holochain apps. This tutorial will add to the previous Hello Holo tutorial, so make sure you do that one first. Testing is a really important part of building higher quality apps but it's also a an excellent way to think through how your app will be used. Understand the tests \u00b6 When you ran hc init in the previous tutorial Holochain already generated some tests for you. The tests are written in JavaScript and use the Holochain testing framework Diorama , along with a popular test harness called Tape . You can run them with Node.JS , a runtime that lets you execute JavaScript in the terminal. Open up the cc_tuts/test/index.js in your favourite text editor. Have a look through the code. Imports required to do testing: const path = require ( 'path' ) const tape = require ( 'tape' ) const { Diorama , tapeExecutor , backwardCompatibilityMiddleware } = require ( '@holochain/diorama' ) This is a catch-all error logger that will let you know if a Promise fails and there's no error handler to hear it. Promise s are a way of simplifying complex asynchronous code, and Diorama uses a lot of them. process . on ( 'unhandledRejection' , error => { console . error ( 'got unhandledRejection:' , error ); }); The path to your compiled DNA. const dnaPath = path . join ( __dirname , \"../dist/cc_tuts.dna.json\" ) const dna = Diorama . dna ( dnaPath , 'cc_tuts' ) Setup a testing scenario. This creates two agents: Alice and Bob. const diorama = new Diorama ({ instances : { alice : dna , bob : dna , }, bridges : [], debugLog : false , executor : tapeExecutor ( require ( 'tape' )), middleware : backwardCompatibilityMiddleware , }) This is the test that Holochain generated based on the my_entry struct and the zome functions that work with it. We removed them in our Hello Holo tutorial, so let's remove the test. Remove the following section: diorama . registerScenario ( \"description of example test\" , async ( s , t , { alice }) => { // Make a call to a Zome function // indicating the function, and passing it an input const addr = await alice . call ( \"my_zome\" , \"create_my_entry\" , { \"entry\" : { \"content\" : \"sample content\" }}) const result = await alice . call ( \"my_zome\" , \"get_my_entry\" , { \"address\" : addr . Ok }) // check for equality of the actual and expected results t . deepEqual ( result , { Ok : { App : [ 'my_entry' , '{\"content\":\"sample content\"}' ] } }) }) This line will run the tests that you have set up. diorama . run () Create a test scenario \u00b6 Tests are organized by creating scenarios. Think of them as a series of actions that the user or group of users take when interacting with your app. For this test you simply want to get the Alice user to call the hello_holo zome function. Then check that you get the result Hello Holo . Place the following just above diorama.run() . Register a test scenario that checks hello_holo() returns the correct value: diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { Make a call to the hello_holo Zome function, passing no arguments: const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); Make sure the result is okay: t . ok ( result . Ok ); Check that the result matches what you expected: Check your code const path = require ( 'path' ) const tape = require ( 'tape' ) const { Diorama , tapeExecutor , backwardCompatibilityMiddleware } = require ( '@holochain/diorama' ) process . on ( 'unhandledRejection' , error => { console . error ( 'got unhandledRejection:' , error ); }); const dnaPath = path . join ( __dirname , \"../dist/cc_tuts.dna.json\" ) const dna = Diorama . dna ( dnaPath , 'cc_tuts' ) const diorama = new Diorama ({ instances : { alice : dna , bob : dna , }, bridges : [], debugLog : false , executor : tapeExecutor ( require ( 'tape' )), middleware : backwardCompatibilityMiddleware , }) diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); t . ok ( result . Ok ); t . deepEqual ( result , { Ok : 'Hello Holo' }) }) diorama . run () Now in the hello_helo directory, run the test like this: $ hc test This will compile and run the test scenario you just wrote. You will see a lot of output. But if everything went okay, then right at the end you will see: # tests 2 # pass 2 # ok Congratulations; you have tested your first Holochain app. Look at you go!","title":"Hello Test Tutorial"},{"location":"coreconcepts/hello_test/#hello-test-tutorial","text":"Welcome to the Hello Test tutorial. Today you will be learning how to test your Holochain apps. This tutorial will add to the previous Hello Holo tutorial, so make sure you do that one first. Testing is a really important part of building higher quality apps but it's also a an excellent way to think through how your app will be used.","title":"Hello Test Tutorial"},{"location":"coreconcepts/hello_test/#understand-the-tests","text":"When you ran hc init in the previous tutorial Holochain already generated some tests for you. The tests are written in JavaScript and use the Holochain testing framework Diorama , along with a popular test harness called Tape . You can run them with Node.JS , a runtime that lets you execute JavaScript in the terminal. Open up the cc_tuts/test/index.js in your favourite text editor. Have a look through the code. Imports required to do testing: const path = require ( 'path' ) const tape = require ( 'tape' ) const { Diorama , tapeExecutor , backwardCompatibilityMiddleware } = require ( '@holochain/diorama' ) This is a catch-all error logger that will let you know if a Promise fails and there's no error handler to hear it. Promise s are a way of simplifying complex asynchronous code, and Diorama uses a lot of them. process . on ( 'unhandledRejection' , error => { console . error ( 'got unhandledRejection:' , error ); }); The path to your compiled DNA. const dnaPath = path . join ( __dirname , \"../dist/cc_tuts.dna.json\" ) const dna = Diorama . dna ( dnaPath , 'cc_tuts' ) Setup a testing scenario. This creates two agents: Alice and Bob. const diorama = new Diorama ({ instances : { alice : dna , bob : dna , }, bridges : [], debugLog : false , executor : tapeExecutor ( require ( 'tape' )), middleware : backwardCompatibilityMiddleware , }) This is the test that Holochain generated based on the my_entry struct and the zome functions that work with it. We removed them in our Hello Holo tutorial, so let's remove the test. Remove the following section: diorama . registerScenario ( \"description of example test\" , async ( s , t , { alice }) => { // Make a call to a Zome function // indicating the function, and passing it an input const addr = await alice . call ( \"my_zome\" , \"create_my_entry\" , { \"entry\" : { \"content\" : \"sample content\" }}) const result = await alice . call ( \"my_zome\" , \"get_my_entry\" , { \"address\" : addr . Ok }) // check for equality of the actual and expected results t . deepEqual ( result , { Ok : { App : [ 'my_entry' , '{\"content\":\"sample content\"}' ] } }) }) This line will run the tests that you have set up. diorama . run ()","title":"Understand the tests"},{"location":"coreconcepts/hello_test/#create-a-test-scenario","text":"Tests are organized by creating scenarios. Think of them as a series of actions that the user or group of users take when interacting with your app. For this test you simply want to get the Alice user to call the hello_holo zome function. Then check that you get the result Hello Holo . Place the following just above diorama.run() . Register a test scenario that checks hello_holo() returns the correct value: diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { Make a call to the hello_holo Zome function, passing no arguments: const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); Make sure the result is okay: t . ok ( result . Ok ); Check that the result matches what you expected: Check your code const path = require ( 'path' ) const tape = require ( 'tape' ) const { Diorama , tapeExecutor , backwardCompatibilityMiddleware } = require ( '@holochain/diorama' ) process . on ( 'unhandledRejection' , error => { console . error ( 'got unhandledRejection:' , error ); }); const dnaPath = path . join ( __dirname , \"../dist/cc_tuts.dna.json\" ) const dna = Diorama . dna ( dnaPath , 'cc_tuts' ) const diorama = new Diorama ({ instances : { alice : dna , bob : dna , }, bridges : [], debugLog : false , executor : tapeExecutor ( require ( 'tape' )), middleware : backwardCompatibilityMiddleware , }) diorama . registerScenario ( \"Test hello holo\" , async ( s , t , { alice }) => { const result = await alice . call ( \"hello\" , \"hello_holo\" , {}); t . ok ( result . Ok ); t . deepEqual ( result , { Ok : 'Hello Holo' }) }) diorama . run () Now in the hello_helo directory, run the test like this: $ hc test This will compile and run the test scenario you just wrote. You will see a lot of output. But if everything went okay, then right at the end you will see: # tests 2 # pass 2 # ok Congratulations; you have tested your first Holochain app. Look at you go!","title":"Create a test scenario"},{"location":"coreconcepts/hello_world/","text":"Hello World \u00b6 The goal of this tutorial is to add an entry in Alice's instance and then retrieve that same entry in Bob's instance. Make your entry public \u00b6 So far the only entry you have had has been private. But this isn't that useful if you want your users to be able to share entries on the same network. Open up your zomes/hello/code/src/lib.rs file. Change the entry sharing to Sharing::Public : Add Bob to the test \u00b6 Previously you made a test where Alice made a few zome calls and verified the results. Now, to test that the entries can be shared between agents on the same DNA, you can use Bob in your tests to interact with Alice. Open up your test/index.js file and add/update the following lines: Add bob to the scenario: - diorama.registerScenario(\"Test Hello Holo\", async (s, t, { alice }) => { + diorama.registerScenario(\"Test Hello Holo\", async (s, t, { alice, bob }) => { Make the retrieve_person call with the result from create_person : const bob_retrieve_result = await bob . call ( \"hello\" , \"retrieve_person\" , { \"address\" : create_result . Ok }); Check that the result was Ok: t . ok ( bob_retrieve_result . Ok ); Check that the result does indeed match the person entry that Alice created: t . deepEqual ( bob_retrieve_result , { Ok : { App : [ 'person' , '{\"name\":\"Alice\"}' ] }}) Your test should look like this: Check your code Run the test \u00b6 Enter the nix-shell if you don't have it open already: nix-shell https://holochain.love Now run the test and make sure it passes: nix-shell ] hc test 1..7 # tests 7 # pass 7 # ok Conductor \u00b6 Now it would be cool to see this happen for real outside of a test. Up till now you have only used hc run to run a single instance of a node. However, in order to have two separate instances communicate on one machine, we need to run holochain directly and pass it a config file. Note \u00b6 hc and holochain are both conductors that host your apps on your users' machines. hc run is for testing and development, and holochain is for end-users. It can host multiple instances of multiple DNAs for multiple users. Normally Alice and Bob would be running instances of your app in their own conductors on their own machines. But for the purposes of this tutorial, it'll be a lot more convenient to try this on one machine, so you don't have to worry about network setup. Before you can create the config file, you will need to generate some keys for your agents. Use hc keygen in your nix-shell to generate a key for each agent: nix-shell] hc keygen -n This will output something similar to the following: Generating keystore (this will take a few moments)... Succesfully created new agent keystore. Public address: HcSCJhRioEqzvx9sooOfw6ANditrqdcxwfV7p7KP6extmnmzJIs83uKmfO9b8kz Keystore written to: /Users/user/Library/Preferences/org.holochain.holochain/keys/HcSCJhRioEqzvx9sooOfw6ANditrqdcxwfV7p7KP6extmnmzJIs83uKmfO9b8kz You can set this file in a conductor config as keystore_file for an agent. Take note of the Public address ; you will need it later. Copy the newly generated keystore to your working folder (replace the path with the one in the Keystore written to: line from the output of the previous command): cp <path_of_generated_keystore> agent1.key Now run hc keygen again but copy the key store to agent2.key: cp <path_of_generated_keystore> agent2.key Config file \u00b6 Create a new file in the root directory of your project called conductor-config.toml . Add an agent with ID test_agent1 and name it Agent 1 : # ----------- Agents ----------- [[agents]] id = \"test_agent1\" name = \"Agent 1\" Use the public address and keystore from hc keygen that you made for agent 1 before here: public_address = \"<public_address_of_agent_1>\" keystore_file = \"./agent1.key\" Add an agent with ID test_agent2 and name it Agent 2 : [[agents]] id = \"test_agent2\" name = \"Agent 2\" Use the public address and keystore from hc keygen that you made for agent 2 before here: public_address = \"<public_address_of_agent_2>\" keystore_file = \"./agent2.key\" Package your DNA and take note of its hash: nix-shell] hc package You will see something similar to this: DNA hash: QmS7wUJj6XZR1SBVk1idGh6bK8gN6RNSFXP2GoC8yCJUzn Add the DNA to your config file with ID hello and the hash you just saw above: # ----------- DNAs ----------- [[dnas]] id = \"hello\" file = \"dist/hello_holo.dna.json\" hash = \"<dna_hash>\" Connect agent 1 to the hello DNA to create an instance for Alice: [[instances]] id = \"Alice\" dna = \"hello\" agent = \"test_agent1\" [instances.storage] type = \"memory\" Add the Bob instance with the same hello dna: [[instances]] id = \"Bob\" dna = \"hello\" agent = \"test_agent2\" [instances.storage] type = \"memory\" Setup the WebSocket interface on socket 3041 : [[interfaces]] id = \"websocket_interface\" [interfaces.driver] type = \"websocket\" port = 3401 Add your instances to this interface so you can call their zome functions: [[interfaces.instances]] id = \"Alice\" [[interfaces.instances]] id = \"Bob\" Note \u00b6 Again, in real life Alice and Bob would each have their own conductor, so they wouldn't be listening on the same WebSocket interface. Allow the users to choose their instance \u00b6 Before you can use two agents, you need a way for the UI to specify which instance the user wants to use. You can do this by setting the instance ID in the zome call. You can think of an instance as a running version of a DNA, in the same way that a variable is an instance of a struct. Open the gui/index.html file. Add a text box for your users to set the agent ID: < input type = \"text\" id = \"instance\" placeholder = \"Enter your instance ID\" >< br > Open the gui/index.js and do the following for every callZome call: Run the app and two UIs \u00b6 Now the fun part, where you get to play with what you just wrote. Open up three terminal windows and enter the nix-shell in each one: nix-shell https://holochain.love Terminal one \u00b6 Go to the root folder of your app: nix-shell] cd /path/to/my/app Start by running the conductor. It's a bit different this time; instead of hc run you will use holochain directly: nix-shell] holochain -c conductor-config.toml Terminal two \u00b6 Go to the root folder of your GUI: nix-shell] cd /path/to/my/gui Run a GUI on port 8000 : nix-shell] python -m SimpleHTTPServer 8000 Terminal three \u00b6 Go to the root folder of your GUI: nix-shell] cd /path/to/my/gui Run a GUI on port 8001 : nix-shell] python -m SimpleHTTPServer 8001 Open up the browser \u00b6 Open two tabs. Tab one \u00b6 Go to 0.0.0.0:8000 . Enter Alice into the Enter your instance ID text box. Tab two \u00b6 Go to 0.0.0.0:8001 . Enter Bob into the Enter your instance ID text box. Tab one --- Alice \u00b6 Create a person entry with your name: Tab two --- Bob \u00b6 Copy the address from the Alice tab and retrieve the person entry: Hooray! Alice and Bob are now able to find each other's information on the DHT","title":"Hello World"},{"location":"coreconcepts/hello_world/#hello-world","text":"The goal of this tutorial is to add an entry in Alice's instance and then retrieve that same entry in Bob's instance.","title":"Hello World"},{"location":"coreconcepts/hello_world/#make-your-entry-public","text":"So far the only entry you have had has been private. But this isn't that useful if you want your users to be able to share entries on the same network. Open up your zomes/hello/code/src/lib.rs file. Change the entry sharing to Sharing::Public :","title":"Make your entry public"},{"location":"coreconcepts/hello_world/#add-bob-to-the-test","text":"Previously you made a test where Alice made a few zome calls and verified the results. Now, to test that the entries can be shared between agents on the same DNA, you can use Bob in your tests to interact with Alice. Open up your test/index.js file and add/update the following lines: Add bob to the scenario: - diorama.registerScenario(\"Test Hello Holo\", async (s, t, { alice }) => { + diorama.registerScenario(\"Test Hello Holo\", async (s, t, { alice, bob }) => { Make the retrieve_person call with the result from create_person : const bob_retrieve_result = await bob . call ( \"hello\" , \"retrieve_person\" , { \"address\" : create_result . Ok }); Check that the result was Ok: t . ok ( bob_retrieve_result . Ok ); Check that the result does indeed match the person entry that Alice created: t . deepEqual ( bob_retrieve_result , { Ok : { App : [ 'person' , '{\"name\":\"Alice\"}' ] }}) Your test should look like this: Check your code","title":"Add Bob to the test"},{"location":"coreconcepts/hello_world/#run-the-test","text":"Enter the nix-shell if you don't have it open already: nix-shell https://holochain.love Now run the test and make sure it passes: nix-shell ] hc test 1..7 # tests 7 # pass 7 # ok","title":"Run the test"},{"location":"coreconcepts/hello_world/#conductor","text":"Now it would be cool to see this happen for real outside of a test. Up till now you have only used hc run to run a single instance of a node. However, in order to have two separate instances communicate on one machine, we need to run holochain directly and pass it a config file.","title":"Conductor"},{"location":"coreconcepts/hello_world/#note","text":"hc and holochain are both conductors that host your apps on your users' machines. hc run is for testing and development, and holochain is for end-users. It can host multiple instances of multiple DNAs for multiple users. Normally Alice and Bob would be running instances of your app in their own conductors on their own machines. But for the purposes of this tutorial, it'll be a lot more convenient to try this on one machine, so you don't have to worry about network setup. Before you can create the config file, you will need to generate some keys for your agents. Use hc keygen in your nix-shell to generate a key for each agent: nix-shell] hc keygen -n This will output something similar to the following: Generating keystore (this will take a few moments)... Succesfully created new agent keystore. Public address: HcSCJhRioEqzvx9sooOfw6ANditrqdcxwfV7p7KP6extmnmzJIs83uKmfO9b8kz Keystore written to: /Users/user/Library/Preferences/org.holochain.holochain/keys/HcSCJhRioEqzvx9sooOfw6ANditrqdcxwfV7p7KP6extmnmzJIs83uKmfO9b8kz You can set this file in a conductor config as keystore_file for an agent. Take note of the Public address ; you will need it later. Copy the newly generated keystore to your working folder (replace the path with the one in the Keystore written to: line from the output of the previous command): cp <path_of_generated_keystore> agent1.key Now run hc keygen again but copy the key store to agent2.key: cp <path_of_generated_keystore> agent2.key","title":"Note"},{"location":"coreconcepts/hello_world/#config-file","text":"Create a new file in the root directory of your project called conductor-config.toml . Add an agent with ID test_agent1 and name it Agent 1 : # ----------- Agents ----------- [[agents]] id = \"test_agent1\" name = \"Agent 1\" Use the public address and keystore from hc keygen that you made for agent 1 before here: public_address = \"<public_address_of_agent_1>\" keystore_file = \"./agent1.key\" Add an agent with ID test_agent2 and name it Agent 2 : [[agents]] id = \"test_agent2\" name = \"Agent 2\" Use the public address and keystore from hc keygen that you made for agent 2 before here: public_address = \"<public_address_of_agent_2>\" keystore_file = \"./agent2.key\" Package your DNA and take note of its hash: nix-shell] hc package You will see something similar to this: DNA hash: QmS7wUJj6XZR1SBVk1idGh6bK8gN6RNSFXP2GoC8yCJUzn Add the DNA to your config file with ID hello and the hash you just saw above: # ----------- DNAs ----------- [[dnas]] id = \"hello\" file = \"dist/hello_holo.dna.json\" hash = \"<dna_hash>\" Connect agent 1 to the hello DNA to create an instance for Alice: [[instances]] id = \"Alice\" dna = \"hello\" agent = \"test_agent1\" [instances.storage] type = \"memory\" Add the Bob instance with the same hello dna: [[instances]] id = \"Bob\" dna = \"hello\" agent = \"test_agent2\" [instances.storage] type = \"memory\" Setup the WebSocket interface on socket 3041 : [[interfaces]] id = \"websocket_interface\" [interfaces.driver] type = \"websocket\" port = 3401 Add your instances to this interface so you can call their zome functions: [[interfaces.instances]] id = \"Alice\" [[interfaces.instances]] id = \"Bob\"","title":"Config file"},{"location":"coreconcepts/hello_world/#note_1","text":"Again, in real life Alice and Bob would each have their own conductor, so they wouldn't be listening on the same WebSocket interface.","title":"Note"},{"location":"coreconcepts/hello_world/#allow-the-users-to-choose-their-instance","text":"Before you can use two agents, you need a way for the UI to specify which instance the user wants to use. You can do this by setting the instance ID in the zome call. You can think of an instance as a running version of a DNA, in the same way that a variable is an instance of a struct. Open the gui/index.html file. Add a text box for your users to set the agent ID: < input type = \"text\" id = \"instance\" placeholder = \"Enter your instance ID\" >< br > Open the gui/index.js and do the following for every callZome call:","title":"Allow the users to choose their instance"},{"location":"coreconcepts/hello_world/#run-the-app-and-two-uis","text":"Now the fun part, where you get to play with what you just wrote. Open up three terminal windows and enter the nix-shell in each one: nix-shell https://holochain.love","title":"Run the app and two UIs"},{"location":"coreconcepts/hello_world/#terminal-one","text":"Go to the root folder of your app: nix-shell] cd /path/to/my/app Start by running the conductor. It's a bit different this time; instead of hc run you will use holochain directly: nix-shell] holochain -c conductor-config.toml","title":"Terminal one"},{"location":"coreconcepts/hello_world/#terminal-two","text":"Go to the root folder of your GUI: nix-shell] cd /path/to/my/gui Run a GUI on port 8000 : nix-shell] python -m SimpleHTTPServer 8000","title":"Terminal two"},{"location":"coreconcepts/hello_world/#terminal-three","text":"Go to the root folder of your GUI: nix-shell] cd /path/to/my/gui Run a GUI on port 8001 : nix-shell] python -m SimpleHTTPServer 8001","title":"Terminal three"},{"location":"coreconcepts/hello_world/#open-up-the-browser","text":"Open two tabs.","title":"Open up the browser"},{"location":"coreconcepts/hello_world/#tab-one","text":"Go to 0.0.0.0:8000 . Enter Alice into the Enter your instance ID text box.","title":"Tab one"},{"location":"coreconcepts/hello_world/#tab-two","text":"Go to 0.0.0.0:8001 . Enter Bob into the Enter your instance ID text box.","title":"Tab two"},{"location":"coreconcepts/hello_world/#tab-one-alice","text":"Create a person entry with your name:","title":"Tab one --- Alice"},{"location":"coreconcepts/hello_world/#tab-two-bob","text":"Copy the address from the Alice tab and retrieve the person entry: Hooray! Alice and Bob are now able to find each other's information on the DHT","title":"Tab two --- Bob"},{"location":"coreconcepts/simple_micro_blog/","text":"Simple Micro Blog tutorial \u00b6 Welcome to the Simple Micro blog tutorial in the Core Concepts tutorial series. The aim of this tutorial is to show how entries can be linked to each other in a Holochain app. A link is simply a relationship between two entries. It's a useful way to find some data from something you already know. For example, you could link from your user's agent ID entry to their blog posts. You will be building on the previous Hello World tutorial and making a super simple blog app. The app's users will be able to post a blog post and then retrieve other users' posts. DNA hash \u00b6 The way you run your conductor has changed from hc run to calling holochain directly. As a consequence, the hash of your app's DNA now lives in the conductor-config.toml file. However, anytime you change your code and run hc package the hash will be different. So you will need to update the conductor-config.toml file. Enter the nix-shell: nix-shell https://holochain.love Package your app: hc package Copy the DNA hash (example shown): DNA hash: QmfKyAk2jXgESca2zju6QbkLqUM1xEjqDsmHRgRxoFp39q Update the conductor-config.toml dna hash: [[dnas]] id = \"hello\" file = \"dist/hello_holo.dna.json\" hash = \"<new_dna_hash>\" Post \u00b6 We will store our posts as a Post struct that holds a message of type String , a timestamp of type u64 , and an author ID of type Address . We're done with the Hello World tutorial, so remove the Person struct and add the Post struct: Entry \u00b6 Update the person entry type definition to post : Agent ID \u00b6 #[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Agent { id : String , } Now you have a post entry but you also need some way to find the posts an agent makes. To do this you can create an agent 'anchor' entry which you will use to link to the posts that the user makes. An anchor is a simple string whose only purpose is to be an easy-to-find entry to attach links to. Define an agent anchor entry type by adding the following lines below the post_entry_def . Add an agent_entry_def function which creates an entry type for the agent: #[entry_def] fn agent_entry_def () -> ValidatingEntryType { Start the entry! macro for the agent entry: entry ! ( name : \"agent\" , description : \"Hash of agent\" , Set sharing to public so other agents can find this agent's anchor (and hence their posts): sharing : Sharing :: Public , Add basic validation to make sure this is the Agent type that is passed in: validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Agent >| { Ok (()) }, Now you want to be able to link this agent entry to the post entry. Start out with the to! link macro, which lets you create link definitions that link from this entry type to another entry type: links : [ to ! ( Define a link type from this entry to the post entry called author_post : \"post\" , link_type : \"author_post\" , Add empty validation for this link: validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: LinkValidationData | { Ok (()) } ) ] ) } Create a post \u00b6 Remove the create_person function. You need a function for creating a new post. Think about the ingredients that might go into the Post structure: a message, a timestamp, and and the author's ID. The message will come from the UI. For simplicity the timestamp will come from the UI as well. Time is a pretty tricky concept in the distributed world and requires careful planning. The author's ID will come from the special constant hdk::AGENT_ADDRESS , which you can access from your zome functions. Why do I have to specify a timestamp and author? Aren't they already in the entry's header? \u00b6 If two agents publish entries with identical type and content, they'll have the same address on the DHT. That means that, for all purposes, there's only one entry with two authors. This is fine for some cases. But it causes problems in a microblog. When one author wants to delete an existing message, does the other author's copy get deleted too? Adding a timestamp and author ID makes the two posts distinct and gives them their own addresses. Add a public create_post function that takes a message as a String and a timestamp as a u64 : #[zome_fn( \"hc_public\" )] pub fn create_post ( message : String , timestamp : u64 ) -> ZomeApiResult < Address > { Create the Post using the message, timestamp, and author's address: let post = Post { message , timestamp , author_id : hdk :: AGENT_ADDRESS . clone (), }; Create the Agent struct from the AGENT_ADDRESS , turn it into an Entry and commit it: let agent_id = Agent { id : hdk :: AGENT_ADDRESS . clone (). into () }; let entry = Entry :: App ( \"agent\" . into (), agent_id . into ()); let agent_address = hdk :: commit_entry ( & entry ) ? ; Commit the post entry: let entry = Entry :: App ( \"post\" . into (), post . into ()); let address = hdk :: commit_entry ( & entry ) ? ; Create an author_post link from the agent to the post: hdk :: link_entries ( & agent_address , & address , \"author_post\" , \"\" ) ? ; Return everything is Ok with the new post's address: Ok ( address ) } Retrieve all of a user's posts \u00b6 Add the retrieve_posts public function that takes an author address and returns a vector of posts: #[zome_fn( \"hc_public\" )] fn retrieve_posts ( author_address : Address ) -> ZomeApiResult < Vec < Post >> { Create an Agent struct from the passed address, turn it into an Entry , and calculate its address: let agent_id = Agent { id : author_address . into () }; let entry = Entry :: App ( \"agent\" . into (), agent_id . into ()); let agent_address = hdk :: entry_address ( & entry ) ? ; Get all the author_post links from the agent's address and load them as the Post type: hdk :: utils :: get_links_and_load_type ( & agent_address , LinkMatch :: Exactly ( \"author_post\" ), LinkMatch :: Any , ) } (Note that because you've already told Rust that this function is going to return a vector of posts, the compiler will tell get_links_and_load_type what type to use in the conversion.) We're using a new directive, link::LinkMatch . You'll need to add it to your use statements at the top of the file: use hdk :: holochain_core_types :: { entry :: Entry , dna :: entry_types :: Sharing , link :: LinkMatch , }; Get the agent's ID \u00b6 As a user, you will need some way of getting your own agent's ID in the UI later so that you can pass it to others. Then they can try getting your posts. Add a public get_agent_id function that returns an Address : #[zome_fn( \"hc_public\" )] fn get_agent_id () -> ZomeApiResult < Address > { For this app you can use the agent's address as their ID, because that's what we're storing in the agent anchor entries: Ok ( hdk :: AGENT_ADDRESS . clone ()) } Show the agent's ID in the UI \u00b6 Let's start on the UI. Go to your GUI folder and open up the index.html file. To make it easy to pass around agent ID, you can display the ID for the instance that each GUI is currently targeting. This should happen when the page loads and when the instance ID changes. Add an onload event to the body that will call the get_agent_id function when the page loads: < body onload = \"get_agent_id()\" > Add an onfocusout event to the instance text box that will call the same function when unfocused: < input type = \"text\" id = \"instance\" onfocusout = \"get_agent_id()\" placeholder = \"Enter your instance ID\" > Now open up the hello.js file and add the get_agent_id function: function get_agent_id () { Get the instance value and set up a zome call connection: var instance = document . getElementById ( 'instance' ). value ; holochainclient . connect ({ url : \"ws://localhost:3401\" }). then (({ callZome , close }) => { Call the get_agent_id zome function and then update the agent_id element with the result: callZome ( instance , 'hello' , 'get_agent_id' )({}). then (( result ) => update_element ( result , 'agent_id' )) }) } Update the UI to allow posts to be created \u00b6 Back in index.html turn the \"create person\" HTML into a post entry widget. Use a textarea , call the create_post function, and update all the labels and IDs: Update the UI to retrieve an agent's posts \u00b6 Update the \"retrieve person\" HTML to retrieve posts: Call create_post from JavaScript \u00b6 In the hello.js file add the create_post function that your HTML calls: function create_post () { Get the post message and instance ID: var message = document . getElementById ( 'post' ). value ; var instance = document . getElementById ( 'instance' ). value ; Get the current timestamp: var timestamp = Date . now (); Make a zome call to create_post with the message and timestamp: holochainclient . connect ({ url : \"ws://localhost:3401\" }). then (({ callZome , close }) => { callZome ( instance , 'hello' , 'create_post' )({ message : message , timestamp : timestamp }). then (( result ) => update_element ( result , 'post_address' )) }) } Update the posts list dynamically \u00b6 Add an empty list below the post_agent_id text box: < ul id = \"posts_output\" ></ ul > In the hello.js file add the following lines to update the posts_output dynamically. Add the display_posts function: function display_posts ( result ) { Get the posts_output HTML element: var list = document . getElementById ( 'posts_output' ); Wipe the current contents of the list, if any: list . innerHTML = \"\" ; Parse the zome function's result as JSON: var output = JSON . parse ( result ); Sort the posts by their timestamps: var posts = output . Ok . sort (( a , b ) => a . timestamp - b . timestamp ); For each post add a <li> element that contains the post's message: for ( post of posts ) { var node = document . createElement ( \"LI\" ); var textnode = document . createTextNode ( post . message ); node . appendChild ( textnode ); list . appendChild ( node ); } } Get this agent's ID \u00b6 Add the get_agent_id function: function get_agent_id () { var instance = document . getElementById ( 'instance' ). value ; Call the get_agent_id zome function and update the agent_id element: holochainclient . connect ({ url : \"ws://localhost:3401\" }). then (({ callZome , close }) => { callZome ( instance , 'hello' , 'get_agent_id' )({}). then (( result ) => update_element ( result , 'agent_id' )) }) } Retrieve an agent's posts \u00b6 This is very similar to retrieve_person , so just update that function:","title":"Simple Micro Blog tutorial"},{"location":"coreconcepts/simple_micro_blog/#simple-micro-blog-tutorial","text":"Welcome to the Simple Micro blog tutorial in the Core Concepts tutorial series. The aim of this tutorial is to show how entries can be linked to each other in a Holochain app. A link is simply a relationship between two entries. It's a useful way to find some data from something you already know. For example, you could link from your user's agent ID entry to their blog posts. You will be building on the previous Hello World tutorial and making a super simple blog app. The app's users will be able to post a blog post and then retrieve other users' posts.","title":"Simple Micro Blog tutorial"},{"location":"coreconcepts/simple_micro_blog/#dna-hash","text":"The way you run your conductor has changed from hc run to calling holochain directly. As a consequence, the hash of your app's DNA now lives in the conductor-config.toml file. However, anytime you change your code and run hc package the hash will be different. So you will need to update the conductor-config.toml file. Enter the nix-shell: nix-shell https://holochain.love Package your app: hc package Copy the DNA hash (example shown): DNA hash: QmfKyAk2jXgESca2zju6QbkLqUM1xEjqDsmHRgRxoFp39q Update the conductor-config.toml dna hash: [[dnas]] id = \"hello\" file = \"dist/hello_holo.dna.json\" hash = \"<new_dna_hash>\"","title":"DNA hash"},{"location":"coreconcepts/simple_micro_blog/#post","text":"We will store our posts as a Post struct that holds a message of type String , a timestamp of type u64 , and an author ID of type Address . We're done with the Hello World tutorial, so remove the Person struct and add the Post struct:","title":"Post"},{"location":"coreconcepts/simple_micro_blog/#entry","text":"Update the person entry type definition to post :","title":"Entry"},{"location":"coreconcepts/simple_micro_blog/#agent-id","text":"#[derive(Serialize, Deserialize, Debug, DefaultJson, Clone)] pub struct Agent { id : String , } Now you have a post entry but you also need some way to find the posts an agent makes. To do this you can create an agent 'anchor' entry which you will use to link to the posts that the user makes. An anchor is a simple string whose only purpose is to be an easy-to-find entry to attach links to. Define an agent anchor entry type by adding the following lines below the post_entry_def . Add an agent_entry_def function which creates an entry type for the agent: #[entry_def] fn agent_entry_def () -> ValidatingEntryType { Start the entry! macro for the agent entry: entry ! ( name : \"agent\" , description : \"Hash of agent\" , Set sharing to public so other agents can find this agent's anchor (and hence their posts): sharing : Sharing :: Public , Add basic validation to make sure this is the Agent type that is passed in: validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: EntryValidationData < Agent >| { Ok (()) }, Now you want to be able to link this agent entry to the post entry. Start out with the to! link macro, which lets you create link definitions that link from this entry type to another entry type: links : [ to ! ( Define a link type from this entry to the post entry called author_post : \"post\" , link_type : \"author_post\" , Add empty validation for this link: validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : hdk :: LinkValidationData | { Ok (()) } ) ] ) }","title":"Agent ID"},{"location":"coreconcepts/simple_micro_blog/#create-a-post","text":"Remove the create_person function. You need a function for creating a new post. Think about the ingredients that might go into the Post structure: a message, a timestamp, and and the author's ID. The message will come from the UI. For simplicity the timestamp will come from the UI as well. Time is a pretty tricky concept in the distributed world and requires careful planning. The author's ID will come from the special constant hdk::AGENT_ADDRESS , which you can access from your zome functions.","title":"Create a post"},{"location":"coreconcepts/simple_micro_blog/#why-do-i-have-to-specify-a-timestamp-and-author-arent-they-already-in-the-entrys-header","text":"If two agents publish entries with identical type and content, they'll have the same address on the DHT. That means that, for all purposes, there's only one entry with two authors. This is fine for some cases. But it causes problems in a microblog. When one author wants to delete an existing message, does the other author's copy get deleted too? Adding a timestamp and author ID makes the two posts distinct and gives them their own addresses. Add a public create_post function that takes a message as a String and a timestamp as a u64 : #[zome_fn( \"hc_public\" )] pub fn create_post ( message : String , timestamp : u64 ) -> ZomeApiResult < Address > { Create the Post using the message, timestamp, and author's address: let post = Post { message , timestamp , author_id : hdk :: AGENT_ADDRESS . clone (), }; Create the Agent struct from the AGENT_ADDRESS , turn it into an Entry and commit it: let agent_id = Agent { id : hdk :: AGENT_ADDRESS . clone (). into () }; let entry = Entry :: App ( \"agent\" . into (), agent_id . into ()); let agent_address = hdk :: commit_entry ( & entry ) ? ; Commit the post entry: let entry = Entry :: App ( \"post\" . into (), post . into ()); let address = hdk :: commit_entry ( & entry ) ? ; Create an author_post link from the agent to the post: hdk :: link_entries ( & agent_address , & address , \"author_post\" , \"\" ) ? ; Return everything is Ok with the new post's address: Ok ( address ) }","title":"Why do I have to specify a timestamp and author? Aren't they already in the entry's header?"},{"location":"coreconcepts/simple_micro_blog/#retrieve-all-of-a-users-posts","text":"Add the retrieve_posts public function that takes an author address and returns a vector of posts: #[zome_fn( \"hc_public\" )] fn retrieve_posts ( author_address : Address ) -> ZomeApiResult < Vec < Post >> { Create an Agent struct from the passed address, turn it into an Entry , and calculate its address: let agent_id = Agent { id : author_address . into () }; let entry = Entry :: App ( \"agent\" . into (), agent_id . into ()); let agent_address = hdk :: entry_address ( & entry ) ? ; Get all the author_post links from the agent's address and load them as the Post type: hdk :: utils :: get_links_and_load_type ( & agent_address , LinkMatch :: Exactly ( \"author_post\" ), LinkMatch :: Any , ) } (Note that because you've already told Rust that this function is going to return a vector of posts, the compiler will tell get_links_and_load_type what type to use in the conversion.) We're using a new directive, link::LinkMatch . You'll need to add it to your use statements at the top of the file: use hdk :: holochain_core_types :: { entry :: Entry , dna :: entry_types :: Sharing , link :: LinkMatch , };","title":"Retrieve all of a user's posts"},{"location":"coreconcepts/simple_micro_blog/#get-the-agents-id","text":"As a user, you will need some way of getting your own agent's ID in the UI later so that you can pass it to others. Then they can try getting your posts. Add a public get_agent_id function that returns an Address : #[zome_fn( \"hc_public\" )] fn get_agent_id () -> ZomeApiResult < Address > { For this app you can use the agent's address as their ID, because that's what we're storing in the agent anchor entries: Ok ( hdk :: AGENT_ADDRESS . clone ()) }","title":"Get the agent's ID"},{"location":"coreconcepts/simple_micro_blog/#show-the-agents-id-in-the-ui","text":"Let's start on the UI. Go to your GUI folder and open up the index.html file. To make it easy to pass around agent ID, you can display the ID for the instance that each GUI is currently targeting. This should happen when the page loads and when the instance ID changes. Add an onload event to the body that will call the get_agent_id function when the page loads: < body onload = \"get_agent_id()\" > Add an onfocusout event to the instance text box that will call the same function when unfocused: < input type = \"text\" id = \"instance\" onfocusout = \"get_agent_id()\" placeholder = \"Enter your instance ID\" > Now open up the hello.js file and add the get_agent_id function: function get_agent_id () { Get the instance value and set up a zome call connection: var instance = document . getElementById ( 'instance' ). value ; holochainclient . connect ({ url : \"ws://localhost:3401\" }). then (({ callZome , close }) => { Call the get_agent_id zome function and then update the agent_id element with the result: callZome ( instance , 'hello' , 'get_agent_id' )({}). then (( result ) => update_element ( result , 'agent_id' )) }) }","title":"Show the agent's ID in the UI"},{"location":"coreconcepts/simple_micro_blog/#update-the-ui-to-allow-posts-to-be-created","text":"Back in index.html turn the \"create person\" HTML into a post entry widget. Use a textarea , call the create_post function, and update all the labels and IDs:","title":"Update the UI to allow posts to be created"},{"location":"coreconcepts/simple_micro_blog/#update-the-ui-to-retrieve-an-agents-posts","text":"Update the \"retrieve person\" HTML to retrieve posts:","title":"Update the UI to retrieve an agent's posts"},{"location":"coreconcepts/simple_micro_blog/#call-create_post-from-javascript","text":"In the hello.js file add the create_post function that your HTML calls: function create_post () { Get the post message and instance ID: var message = document . getElementById ( 'post' ). value ; var instance = document . getElementById ( 'instance' ). value ; Get the current timestamp: var timestamp = Date . now (); Make a zome call to create_post with the message and timestamp: holochainclient . connect ({ url : \"ws://localhost:3401\" }). then (({ callZome , close }) => { callZome ( instance , 'hello' , 'create_post' )({ message : message , timestamp : timestamp }). then (( result ) => update_element ( result , 'post_address' )) }) }","title":"Call create_post from JavaScript"},{"location":"coreconcepts/simple_micro_blog/#update-the-posts-list-dynamically","text":"Add an empty list below the post_agent_id text box: < ul id = \"posts_output\" ></ ul > In the hello.js file add the following lines to update the posts_output dynamically. Add the display_posts function: function display_posts ( result ) { Get the posts_output HTML element: var list = document . getElementById ( 'posts_output' ); Wipe the current contents of the list, if any: list . innerHTML = \"\" ; Parse the zome function's result as JSON: var output = JSON . parse ( result ); Sort the posts by their timestamps: var posts = output . Ok . sort (( a , b ) => a . timestamp - b . timestamp ); For each post add a <li> element that contains the post's message: for ( post of posts ) { var node = document . createElement ( \"LI\" ); var textnode = document . createTextNode ( post . message ); node . appendChild ( textnode ); list . appendChild ( node ); } }","title":"Update the posts list dynamically"},{"location":"coreconcepts/simple_micro_blog/#get-this-agents-id","text":"Add the get_agent_id function: function get_agent_id () { var instance = document . getElementById ( 'instance' ). value ; Call the get_agent_id zome function and update the agent_id element: holochainclient . connect ({ url : \"ws://localhost:3401\" }). then (({ callZome , close }) => { callZome ( instance , 'hello' , 'get_agent_id' )({}). then (( result ) => update_element ( result , 'agent_id' )) }) }","title":"Get this agent's ID"},{"location":"coreconcepts/simple_micro_blog/#retrieve-an-agents-posts","text":"This is very similar to retrieve_person , so just update that function:","title":"Retrieve an agent's posts"},{"location":"guide/access_instance_info/","text":"Access Instance Info \u00b6 Other info about running Instances in the Conductor can be retrieved via functions on a Conductor. conductor.agent_id(instanceId) => string \u00b6 Get the agent_id for an instance, by passing an instance id. Name instanceId Type string Description Specifies an instance by its instanceId. This instanceId should be the equivalent to an instanceConfig.name which was passed to Config.instance . This in turn would be equivalent to the original name given to Config.agent , unless you overrode it when calling Config.instance . See more here . Example \u00b6 const aliceAgentId = conductor . agent_id ( 'alice' ) console . log ( aliceAgentId ) // alice-----------------------------------------------------------------------------AAAIuDJb4M conductor.dna_address(instanceId) => string \u00b6 Get the address of the DNA for an instance, by passing an instance id. Name instanceId Type string Description Specifies an instance by its instanceId. This instanceId should be the equivalent to an instanceConfig.name which was passed to Config.instance . This in turn would be equivalent to the original name given to Config.agent , unless you overrode it when calling Config.instance . See more here . Example \u00b6 const dnaAddress = conductor . dna_address ( 'alice' ) console . log ( dnaAddress ) // QmYiUmMEq1WQmSSjbM7pcLCy1GkdkfbwH5cxugGmeNZPE3","title":"Access Instance Info"},{"location":"guide/access_instance_info/#access-instance-info","text":"Other info about running Instances in the Conductor can be retrieved via functions on a Conductor.","title":"Access Instance Info"},{"location":"guide/access_instance_info/#conductoragent_idinstanceid-string","text":"Get the agent_id for an instance, by passing an instance id. Name instanceId Type string Description Specifies an instance by its instanceId. This instanceId should be the equivalent to an instanceConfig.name which was passed to Config.instance . This in turn would be equivalent to the original name given to Config.agent , unless you overrode it when calling Config.instance . See more here .","title":"conductor.agent_id(instanceId) =&gt; string"},{"location":"guide/access_instance_info/#example","text":"const aliceAgentId = conductor . agent_id ( 'alice' ) console . log ( aliceAgentId ) // alice-----------------------------------------------------------------------------AAAIuDJb4M","title":"Example"},{"location":"guide/access_instance_info/#conductordna_addressinstanceid-string","text":"Get the address of the DNA for an instance, by passing an instance id. Name instanceId Type string Description Specifies an instance by its instanceId. This instanceId should be the equivalent to an instanceConfig.name which was passed to Config.instance . This in turn would be equivalent to the original name given to Config.agent , unless you overrode it when calling Config.instance . See more here .","title":"conductor.dna_address(instanceId) =&gt; string"},{"location":"guide/access_instance_info/#example_1","text":"const dnaAddress = conductor . dna_address ( 'alice' ) console . log ( dnaAddress ) // QmYiUmMEq1WQmSSjbM7pcLCy1GkdkfbwH5cxugGmeNZPE3","title":"Example"},{"location":"guide/agent/","text":"Agent \u00b6","title":"(E) Agent"},{"location":"guide/agent/#agent","text":"","title":"Agent"},{"location":"guide/alpha_migrate/","text":"Updating from holochain-proto to holochain-rust \u00b6 If you wrote an application for holochain-proto , you are likely wondering what it may take to port your app to the new holochain-rust version of Holochain. The following should provide multiple levels of insight into what this could involve. At a very general level: - In terms of code, you have at least 2 options - rewriting the code in Rust - waiting for Assemblyscript support, and migrating at that point to Assemblyscript (the caveat to this approach is that it is not yet known at which point this support will arrive) - The API between a user interface and Holochain has switched from HTTP to Websockets (for now), and so any user interface must be updated to use this approach. - The DNA file has been simplified. Less is defined as JSON in the dna.json file and more is defined in the code. - Testing of DNA utilizes Nodejs to run tests, using the testing library of your choice. This replaces the custom (and limited) JSON test configuration employed by holochain-proto. - Schemas for entry types are no longer defined using json-schema, but using native Rust structs. At the level of the code, in more detail, the changes are as follows (note that this is in reference to Javascript Zomes being ported to Rust Zomes): - all camel case function names are now snake case - makeHash is now named entry_address - commit is now named commit_entry - get is now named get_entry - update is now named update_entry - remove is now named remove_entry - Links are no longer created using commit , but instead have their own method, named link_entries - Instead of being implicitly imported, the Zome API functions are explicitly imported into Zomes, e.g. extern crate hdk; - The code of each Zome must now utilize a Rust \"macro\" called \"define_zome!\", and its various subproperties, which did not previously exist. - Many aspects of validation have changed, see the section below on validation Updating Validation \u00b6 There is a conceptual change to the approach to validation of entries, and even whereabouts that logic lives in the code. In holochain-proto , there were a number of hooks which Holochain would call back into, to perform validation, such as - validateCommit - validatePut - validateMod - validateDel - validateLink Regardless of how many entry types there were, there would still be only 5 callbacks defined maximum. These validation callbacks were performed at a certain stage in the lifecycle of an entry. Now, an entry type is defined all in one place, including its validation rules, which are unique to it as an entry type. This could look as follows: #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Person { name : String , } entry ! ( name : \"person\" , description : \"\" , sharing : Sharing :: Public , native_type : Person , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | person : Person , validation_data : hdk :: ValidationData | { ( person . name . len () >= 2 ) . ok_or_else ( || String :: from ( \"Name must be at least 2 characters\" )) } ) The callback validation , replaces validateCommit and all the rest from holochain-proto. However, validation still happens at various times in the lifecycle of an entry, so if the validation is to operate differently between initial commit to the chain, update , or remove , then that logic must be written into this single validation function. To determine which context validation is being called within, you can check in a property of the second parameter of the callback, which in the example above is called validation_data . For this, you can use the Rust match operator, and check against the validation_data.action . It will be one of an enum that can be seen in detail in the API reference . Yet to cover: \u00b6 Capabilities Traits UI","title":"Updating from Proto to Rust"},{"location":"guide/alpha_migrate/#updating-from-holochain-proto-to-holochain-rust","text":"If you wrote an application for holochain-proto , you are likely wondering what it may take to port your app to the new holochain-rust version of Holochain. The following should provide multiple levels of insight into what this could involve. At a very general level: - In terms of code, you have at least 2 options - rewriting the code in Rust - waiting for Assemblyscript support, and migrating at that point to Assemblyscript (the caveat to this approach is that it is not yet known at which point this support will arrive) - The API between a user interface and Holochain has switched from HTTP to Websockets (for now), and so any user interface must be updated to use this approach. - The DNA file has been simplified. Less is defined as JSON in the dna.json file and more is defined in the code. - Testing of DNA utilizes Nodejs to run tests, using the testing library of your choice. This replaces the custom (and limited) JSON test configuration employed by holochain-proto. - Schemas for entry types are no longer defined using json-schema, but using native Rust structs. At the level of the code, in more detail, the changes are as follows (note that this is in reference to Javascript Zomes being ported to Rust Zomes): - all camel case function names are now snake case - makeHash is now named entry_address - commit is now named commit_entry - get is now named get_entry - update is now named update_entry - remove is now named remove_entry - Links are no longer created using commit , but instead have their own method, named link_entries - Instead of being implicitly imported, the Zome API functions are explicitly imported into Zomes, e.g. extern crate hdk; - The code of each Zome must now utilize a Rust \"macro\" called \"define_zome!\", and its various subproperties, which did not previously exist. - Many aspects of validation have changed, see the section below on validation","title":"Updating from holochain-proto to holochain-rust"},{"location":"guide/alpha_migrate/#updating-validation","text":"There is a conceptual change to the approach to validation of entries, and even whereabouts that logic lives in the code. In holochain-proto , there were a number of hooks which Holochain would call back into, to perform validation, such as - validateCommit - validatePut - validateMod - validateDel - validateLink Regardless of how many entry types there were, there would still be only 5 callbacks defined maximum. These validation callbacks were performed at a certain stage in the lifecycle of an entry. Now, an entry type is defined all in one place, including its validation rules, which are unique to it as an entry type. This could look as follows: #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Person { name : String , } entry ! ( name : \"person\" , description : \"\" , sharing : Sharing :: Public , native_type : Person , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | person : Person , validation_data : hdk :: ValidationData | { ( person . name . len () >= 2 ) . ok_or_else ( || String :: from ( \"Name must be at least 2 characters\" )) } ) The callback validation , replaces validateCommit and all the rest from holochain-proto. However, validation still happens at various times in the lifecycle of an entry, so if the validation is to operate differently between initial commit to the chain, update , or remove , then that logic must be written into this single validation function. To determine which context validation is being called within, you can check in a property of the second parameter of the callback, which in the example above is called validation_data . For this, you can use the Rust match operator, and check against the validation_data.action . It will be one of an enum that can be seen in detail in the API reference .","title":"Updating Validation"},{"location":"guide/alpha_migrate/#yet-to-cover","text":"Capabilities Traits UI","title":"Yet to cover:"},{"location":"guide/apps_advanced_topics/","text":"Building Holochain Apps: Advanced Topics \u00b6","title":"Building Holochain Apps: Advanced Topics"},{"location":"guide/apps_advanced_topics/#building-holochain-apps-advanced-topics","text":"","title":"Building Holochain Apps: Advanced Topics"},{"location":"guide/apps_user_interfaces/","text":"Building Holochain Apps: User Interfaces \u00b6 Holochain is designed to be flexible and accomodating when it comes to building user interfaces. There is not a single approach to developing user interfaces that is enforced by Holochain, though there are some approaches that have extra tooling, support and focus. First and foremost, Holochain supports APIs that make building UIs with the technologies of the web (HTML, CSS, JS, etc) easy. Put simply, it is entirely possible to build a user interface for Holochain completely in HTML, CSS, and JavaScript.","title":"Building Holochain Apps - User Interfaces"},{"location":"guide/apps_user_interfaces/#building-holochain-apps-user-interfaces","text":"Holochain is designed to be flexible and accomodating when it comes to building user interfaces. There is not a single approach to developing user interfaces that is enforced by Holochain, though there are some approaches that have extra tooling, support and focus. First and foremost, Holochain supports APIs that make building UIs with the technologies of the web (HTML, CSS, JS, etc) easy. Put simply, it is entirely possible to build a user interface for Holochain completely in HTML, CSS, and JavaScript.","title":"Building Holochain Apps: User Interfaces"},{"location":"guide/bridging/","text":"Building Holochain Apps: Bridging \u00b6 As you saw in Building Apps each DNA has a unique hash that spawns a brand new DHT network and creates isolated source chains for each agent. Even when you change the DNA, releasing a new version of the app, it will spawn a brand new DHT network and source chains. So if every app lives in an entirely separated world how can they talk to each other? This is where bridging comes into play. A bridge is a connector between two apps (or two versions of the same app, for that matter) that allows a synchronous bidirectional transfer of information between them. To use a bridge, right now you need to configure a production Holochain conductor , at least two instances configured, along the lines of the following example setup (in a conductor-config.toml file): [[instances]] id = \"caller-instance\" dna = \"caller-dna\" agent = \"caller-agent\" [instances.logger] type = \"simple\" [instances.storage] type = \"memory\" [[instances]] id = \"target-instance\" dna = target-dna\" agent = \"target-agent\" [instances.logger] type = \"simple\" [instances.storage] type = \"memory\" [[bridges]] caller_id = \"caller-instance\" callee_id = \"target-instance\" handle = \"sample-bridge\" Then on the caller DNA you have to initiate the bridge call using hdk::call like this: let response = match hdk :: call ( \"sample-bridge\" , \"sample_zome\" , Address :: from ( PUBLIC_TOKEN . to_string ()), // never mind this for now \"sample_function\" , json ! ({ \"some_param\" : \"some_val\" , }). into () ) { Ok ( json ) => serde_json :: from_str ( & json . to_string ()). unwrap (), // converts the return to JSON Err ( e ) => return Err ( e ) }; And the corresponding target / callee DNA on the other end should have a zome called \"sample_zome\", with a function as follows: pub fn handle_sample_function ( some_param : String ) -> ZomeApiResult < Address > { // do something here } define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [ sample_function : { inputs : | some_param : String | , outputs : | result : ZomeApiResult < Address >| , handler : handle_sample_function } ] traits : { hc_public [ sample_function ] } Remember that the call will block the execution of the caller DNA until the callee (target) finishes executing the call, so it's best to mind performance issues when working with bridges. Try to make contextual or incremental calls rather than all-encompassing ones.","title":"Building Holochain Apps - Bridging"},{"location":"guide/bridging/#building-holochain-apps-bridging","text":"As you saw in Building Apps each DNA has a unique hash that spawns a brand new DHT network and creates isolated source chains for each agent. Even when you change the DNA, releasing a new version of the app, it will spawn a brand new DHT network and source chains. So if every app lives in an entirely separated world how can they talk to each other? This is where bridging comes into play. A bridge is a connector between two apps (or two versions of the same app, for that matter) that allows a synchronous bidirectional transfer of information between them. To use a bridge, right now you need to configure a production Holochain conductor , at least two instances configured, along the lines of the following example setup (in a conductor-config.toml file): [[instances]] id = \"caller-instance\" dna = \"caller-dna\" agent = \"caller-agent\" [instances.logger] type = \"simple\" [instances.storage] type = \"memory\" [[instances]] id = \"target-instance\" dna = target-dna\" agent = \"target-agent\" [instances.logger] type = \"simple\" [instances.storage] type = \"memory\" [[bridges]] caller_id = \"caller-instance\" callee_id = \"target-instance\" handle = \"sample-bridge\" Then on the caller DNA you have to initiate the bridge call using hdk::call like this: let response = match hdk :: call ( \"sample-bridge\" , \"sample_zome\" , Address :: from ( PUBLIC_TOKEN . to_string ()), // never mind this for now \"sample_function\" , json ! ({ \"some_param\" : \"some_val\" , }). into () ) { Ok ( json ) => serde_json :: from_str ( & json . to_string ()). unwrap (), // converts the return to JSON Err ( e ) => return Err ( e ) }; And the corresponding target / callee DNA on the other end should have a zome called \"sample_zome\", with a function as follows: pub fn handle_sample_function ( some_param : String ) -> ZomeApiResult < Address > { // do something here } define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [ sample_function : { inputs : | some_param : String | , outputs : | result : ZomeApiResult < Address >| , handler : handle_sample_function } ] traits : { hc_public [ sample_function ] } Remember that the call will block the execution of the caller DNA until the callee (target) finishes executing the call, so it's best to mind performance issues when working with bridges. Try to make contextual or incremental calls rather than all-encompassing ones.","title":"Building Holochain Apps: Bridging"},{"location":"guide/build_files/","text":".hcbuild Files \u00b6 In the process of building a .dna.json file during packaging, here is what Holochain does: - It iterates Zome by Zome adding them to the JSON - For each Zome, it looks for any folders containing a .hcbuild file - For any folder with a .hcbuild file, it executes one or more commands from the .hcbuild file to create a WASM file - It takes that built WASM file and Base64 encodes it, then stores a key/value pair for the Zome with the key as the folder name and the encoded WASM as the value When using hc generate to scaffold a Zome , you will have a .hcbuild file automatically. If you create your Zome manually however, you will need to create the file yourself. Here's the structure of a .hcbuild file, using a Rust Zome which builds using Cargo as an example: { \"steps\" : [ { \"command\" : \"cargo\" , \"arguments\" : [ \"build\" , \"--release\" , \"--target=wasm32-unknown-unknown\" ] }, ], \"artifact\" : \"target/wasm32-unknown-unknown/release/code.wasm\" } The two top level properties are steps and artifact . steps is a list of commands which will be sequentially executed to build a WASM file. artifact is the expected path to the built WASM file. Under steps , each key refers to the bin(ary) of the command that will be executed, such as cargo . The value of cargo , the command, is an array of arguments: build , and the two -- flags. In order to determine what should go here, just try running the commands yourself from a terminal, while in the directory of the Zome code. That would look, for example, like running: cargo build --release --target = wasm32-unknown-unknown Building in Rust: Rust -> WASM compilation tools \u00b6 If we take Zome code in Rust as an example, you will need Rust and Cargo set up appropriately to build WASM from Rust code. To enable it, run the following: $ rustup target add wasm32-unknown-unknown This adds WASM as a compilation target for Rust, so that you can run the previously mentioned command with --target=wasm32-unknown-unknown .","title":".hcbuild Files"},{"location":"guide/build_files/#hcbuild-files","text":"In the process of building a .dna.json file during packaging, here is what Holochain does: - It iterates Zome by Zome adding them to the JSON - For each Zome, it looks for any folders containing a .hcbuild file - For any folder with a .hcbuild file, it executes one or more commands from the .hcbuild file to create a WASM file - It takes that built WASM file and Base64 encodes it, then stores a key/value pair for the Zome with the key as the folder name and the encoded WASM as the value When using hc generate to scaffold a Zome , you will have a .hcbuild file automatically. If you create your Zome manually however, you will need to create the file yourself. Here's the structure of a .hcbuild file, using a Rust Zome which builds using Cargo as an example: { \"steps\" : [ { \"command\" : \"cargo\" , \"arguments\" : [ \"build\" , \"--release\" , \"--target=wasm32-unknown-unknown\" ] }, ], \"artifact\" : \"target/wasm32-unknown-unknown/release/code.wasm\" } The two top level properties are steps and artifact . steps is a list of commands which will be sequentially executed to build a WASM file. artifact is the expected path to the built WASM file. Under steps , each key refers to the bin(ary) of the command that will be executed, such as cargo . The value of cargo , the command, is an array of arguments: build , and the two -- flags. In order to determine what should go here, just try running the commands yourself from a terminal, while in the directory of the Zome code. That would look, for example, like running: cargo build --release --target = wasm32-unknown-unknown","title":".hcbuild Files"},{"location":"guide/build_files/#building-in-rust-rust-wasm-compilation-tools","text":"If we take Zome code in Rust as an example, you will need Rust and Cargo set up appropriately to build WASM from Rust code. To enable it, run the following: $ rustup target add wasm32-unknown-unknown This adds WASM as a compilation target for Rust, so that you can run the previously mentioned command with --target=wasm32-unknown-unknown .","title":"Building in Rust: Rust -&gt; WASM compilation tools"},{"location":"guide/building_apps/","text":"Building Apps \u00b6 If you're looking to build a Holochain app, it is important to first know what a Holochain app is. First, recall that Holochain is an engine that can run your distributed apps. That engine expects and requires your application to be in a certain format that is unique to Holochain. That format is referred to as the \"DNA\" of your application. The DNA of an application exists as a single file, which is mounted and executed by Holochain. Writing your application in a single file would not be feasible or desirable, however. Instead, you are supplied the tools to store your application code across a set of files within a folder, and tools to build all that code down into one file, in the DNA format. While there are lots of details to learn about Holochain and DNA, it can be useful to first look from a general perspective. Holochain and DNA \u00b6 Recall that a goal of Holochain is to enable cryptographically secured, tamper-proof peer-to-peer applications. DNA files play a fundamental role in enabling this. Imagine that we think of an application and its users as a game. When people play any game, it's important that they play by the same rules -- otherwise, they are actually playing different games. With Holochain, a DNA file contains the complete set of rules and logic for an application. Thus, when users independently run an app with identical DNA, they are playing the same game -- running the same application with cryptographic security. What this allows in technical terms is that these independent users can begin sharing data with one another and validating one anothers data. Thus, users can interact with the data in this distributed peer-to-peer system with full confidence in the integrity of that data. The key takeaway from this is that if you change the DNA (the configuration, validation rules, and application logic) and a user runs it, they are basically running a different app. If this brings up questions for you about updating your application to different versions, good catch. This concern will be addressed later in this section. Before exploring the details of Holochain DNA, take a minute to explore the different platforms that you can target with Holochain.","title":"Building Apps"},{"location":"guide/building_apps/#building-apps","text":"If you're looking to build a Holochain app, it is important to first know what a Holochain app is. First, recall that Holochain is an engine that can run your distributed apps. That engine expects and requires your application to be in a certain format that is unique to Holochain. That format is referred to as the \"DNA\" of your application. The DNA of an application exists as a single file, which is mounted and executed by Holochain. Writing your application in a single file would not be feasible or desirable, however. Instead, you are supplied the tools to store your application code across a set of files within a folder, and tools to build all that code down into one file, in the DNA format. While there are lots of details to learn about Holochain and DNA, it can be useful to first look from a general perspective.","title":"Building Apps"},{"location":"guide/building_apps/#holochain-and-dna","text":"Recall that a goal of Holochain is to enable cryptographically secured, tamper-proof peer-to-peer applications. DNA files play a fundamental role in enabling this. Imagine that we think of an application and its users as a game. When people play any game, it's important that they play by the same rules -- otherwise, they are actually playing different games. With Holochain, a DNA file contains the complete set of rules and logic for an application. Thus, when users independently run an app with identical DNA, they are playing the same game -- running the same application with cryptographic security. What this allows in technical terms is that these independent users can begin sharing data with one another and validating one anothers data. Thus, users can interact with the data in this distributed peer-to-peer system with full confidence in the integrity of that data. The key takeaway from this is that if you change the DNA (the configuration, validation rules, and application logic) and a user runs it, they are basically running a different app. If this brings up questions for you about updating your application to different versions, good catch. This concern will be addressed later in this section. Before exploring the details of Holochain DNA, take a minute to explore the different platforms that you can target with Holochain.","title":"Holochain and DNA"},{"location":"guide/building_for_android/","text":"Building For Android \u00b6 Note: These instructions for building Holochain on Android are adapted from here . In order to get to libraries that can be linked against when building HoloSqape for Android, you basically just need to setup up according targets for cargo. Given that the Android SDK is installed, here are the steps to setting things up for building: Install the Android tools: a. Install Android Studio b. Open Android Studio and navigate to SDK Tools: - MacOS: Android Studio > Preferences > Appearance & Behaviour > Android SDK > SDK Tools - Linux: Configure (gear) > Appearance & Behavior > System Settings > Android SDK c. Check the following options for installation and click OK: * Android SDK Tools * NDK * CMake * LLDB d. Get a beverage of your choice (or a full meal for that matter) why you wait for the lengthy download Setup ANDROID_HOME env variable: On MacOS export ANDROID_HOME = /Users/ $USER /Library/Android/sdk Linux: (assuming you used defaults when installing Android Studio) export ANDROID_HOME = $HOME /Android/Sdk Create standalone NDKs (the commands below put the NDK in your home dir but you can put them where you like): export NDK_HOME = $ANDROID_HOME /ndk-bundle cd ~ mkdir NDK ${ NDK_HOME } /build/tools/make_standalone_toolchain.py --api 26 --arch arm64 --install-dir NDK/arm64 ${ NDK_HOME } /build/tools/make_standalone_toolchain.py --api 26 --arch arm --install-dir NDK/arm ${ NDK_HOME } /build/tools/make_standalone_toolchain.py --api 26 --arch x86 --install-dir NDK/x86 Add the following lines to your ~/.cargo/config : [target.aarch64-linux-android] ar = \"<your $HOME value here>/NDK/arm64/bin/aarch64-linux-android-ar\" linker = \"<your $HOME value here>/NDK/arm64/bin/aarch64-linux-android-clang\" [target.armv7-linux-androideabi] ar = \"<your $HOME value here>/NDK/arm/bin/arm-linux-androideabi-ar\" linker = \"<your $HOME value here>/NDK/arm/bin/arm-linux-androideabi-clang\" [target.i686-linux-android] ar = \"<your $HOME value here>/NDK/x86/bin/i686-linux-android-ar\" linker = \"<your $HOME value here>/NDK/x86/bin/i686-linux-android-clang\" (this toml file needs absolute paths, so you need to prefix the path with your home dir). Now you can add those targets to your rust installation with: rustup target add aarch64-linux-android armv7-linux-androideabi i686-linux-android Finally, should now be able to build Holochain for Android with your chosen target, e.g.: cd <holochain repo> cargo build --target armv7-linux-androideabi --release NOTE: there is currently a problem in that wabt (which we use in testing as a dev dependency) won't compile on android, and the cargo builder compiles dev dependencies even though they aren't being used in release builds. Thus as a work around, for the cargo build command above to work, you need to manually comment out the dev dependency section in both core/Cargo.toml and core_api/Cargo.toml","title":"Building For Android"},{"location":"guide/building_for_android/#building-for-android","text":"Note: These instructions for building Holochain on Android are adapted from here . In order to get to libraries that can be linked against when building HoloSqape for Android, you basically just need to setup up according targets for cargo. Given that the Android SDK is installed, here are the steps to setting things up for building: Install the Android tools: a. Install Android Studio b. Open Android Studio and navigate to SDK Tools: - MacOS: Android Studio > Preferences > Appearance & Behaviour > Android SDK > SDK Tools - Linux: Configure (gear) > Appearance & Behavior > System Settings > Android SDK c. Check the following options for installation and click OK: * Android SDK Tools * NDK * CMake * LLDB d. Get a beverage of your choice (or a full meal for that matter) why you wait for the lengthy download Setup ANDROID_HOME env variable: On MacOS export ANDROID_HOME = /Users/ $USER /Library/Android/sdk Linux: (assuming you used defaults when installing Android Studio) export ANDROID_HOME = $HOME /Android/Sdk Create standalone NDKs (the commands below put the NDK in your home dir but you can put them where you like): export NDK_HOME = $ANDROID_HOME /ndk-bundle cd ~ mkdir NDK ${ NDK_HOME } /build/tools/make_standalone_toolchain.py --api 26 --arch arm64 --install-dir NDK/arm64 ${ NDK_HOME } /build/tools/make_standalone_toolchain.py --api 26 --arch arm --install-dir NDK/arm ${ NDK_HOME } /build/tools/make_standalone_toolchain.py --api 26 --arch x86 --install-dir NDK/x86 Add the following lines to your ~/.cargo/config : [target.aarch64-linux-android] ar = \"<your $HOME value here>/NDK/arm64/bin/aarch64-linux-android-ar\" linker = \"<your $HOME value here>/NDK/arm64/bin/aarch64-linux-android-clang\" [target.armv7-linux-androideabi] ar = \"<your $HOME value here>/NDK/arm/bin/arm-linux-androideabi-ar\" linker = \"<your $HOME value here>/NDK/arm/bin/arm-linux-androideabi-clang\" [target.i686-linux-android] ar = \"<your $HOME value here>/NDK/x86/bin/i686-linux-android-ar\" linker = \"<your $HOME value here>/NDK/x86/bin/i686-linux-android-clang\" (this toml file needs absolute paths, so you need to prefix the path with your home dir). Now you can add those targets to your rust installation with: rustup target add aarch64-linux-android armv7-linux-androideabi i686-linux-android Finally, should now be able to build Holochain for Android with your chosen target, e.g.: cd <holochain repo> cargo build --target armv7-linux-androideabi --release NOTE: there is currently a problem in that wabt (which we use in testing as a dev dependency) won't compile on android, and the cargo builder compiles dev dependencies even though they aren't being used in release builds. Thus as a work around, for the cargo build command above to work, you need to manually comment out the dev dependency section in both core/Cargo.toml and core_api/Cargo.toml","title":"Building For Android"},{"location":"guide/building_for_different_platforms/","text":"Building for Different Platforms \u00b6 Holochain is designed to run on many different platforms. Essentially it can run on any platform that Rust and the Rust WASM interpreter targets. Thus Holochain DNA's will be able to run on platforms ranging from Raspberry Pis to Android smartphones once the tools have been fully developed. We have experimented with C bindings that allowed us to run Holochain DNAs, in a Qt and Qml based cross-platform deployment, which besides running on desktop machines also worked on Android . We expect other approaches to running Holochain apps on different platforms to proliferate, including compiling Holochain directly in your native application, whether it be an Electron app, a command-line Rust based app, or an Android app.","title":"Building for Different Platforms"},{"location":"guide/building_for_different_platforms/#building-for-different-platforms","text":"Holochain is designed to run on many different platforms. Essentially it can run on any platform that Rust and the Rust WASM interpreter targets. Thus Holochain DNA's will be able to run on platforms ranging from Raspberry Pis to Android smartphones once the tools have been fully developed. We have experimented with C bindings that allowed us to run Holochain DNAs, in a Qt and Qml based cross-platform deployment, which besides running on desktop machines also worked on Android . We expect other approaches to running Holochain apps on different platforms to proliferate, including compiling Holochain directly in your native application, whether it be an Electron app, a command-line Rust based app, or an Android app.","title":"Building for Different Platforms"},{"location":"guide/built_with_holochain/","text":"Built With Holochain \u00b6 Please click \"suggest an edit\", and add something you built with Holochain to the list! Holochain Basic Chat Todo list UI (Gatsby/Redux) TODOlist example HoloChat Simple UI HoloVault Simple App Todo List tutorial Hylo : a social network for community management, messaging and collaboration. They have a working network, although apparently with no activity. See also https://github.com/Hylozoic/hylo-holo-dnas. Coolcats2 : happ intended to replace Twitter. Learn Using Test Driven Development Holo . (People often confuse Holo and Holochain. To make it clear they are not the same thing, Holo is actually a web hosting happ, built on Holochain, to make it easier to bridge users to Holochain via web apps without having to know anything about Holochain, or do anything with it. It is not the same thing as Holochain, the agent-centric distributed app framework with unlimited scalability, or with scalability at least proportional to the number of nodes, with the limit on that number only being constrained by physics, number of computers,). Producer\u2019s Market . Provides a market or exchange for producers and consumers, with at least the initial focus on agriculture. See also this blog post . Morpheus Network : automated, global, platform-agnostic supply-chain network. Holosupply : digital supply chain integration platform. Hypergroove : a data-driven, SaaS platform for more sustainable, inclusive, community-driven agri-food systems, with a focus on urban agriculture and p2p trade. Redgrid : a SaaS platform for the Internet of Energy, connecting energy generation and storage devices and owners with power-consuming appliances and users. The first app built will be \"Market Adaptive Demand Management, which will allow users\u2019 high consumption devices\u2014such as HVACs\u2014to automatically respond to external market prices and signals in order to reduce consumption and costs. This will be applied to households, commercial buildings and multi-building sites. The application will be integrated into utility demand management programs to pay users for reducing their demand peaks, easing strain on the grid.\" It also aims to provide access to the 1.2b people in energy poverty. See also https://holo.host/project/redgrid/. Metavents : a planning platform for events and humanitarian project with event fees donated to the same humanitarian projects, using the transparency of Holochain. It uses visual planning with 2D, 3D, VR/AR and AI. Bridgit / Crowdfact . Bridgit is a Distributed Search and Discovery protocol and a protocol for the OverWeb, a new layer of trust, ideas, interactions, and experiences that display on top of the page source. Available as a Chrome extension. Crowdfact is part of Bridgit and involves crowd-sourced fact-checking. Scribe : The SCRIBE Project aims to build a distributed Conceptionary Manager which is a distributed application that allows to manage one or more Conceptionaries. There's a Github repo at https://github.com/iPlumb3r/Th3Sr1b3Pr0j3ct. Prtcl (The Underscore Protocol) : a way for people to share ideas, content creation, and conversation with each other. It is inspired by Git, which is a version control system for distributed tracking changes in software development. It allows collaborators to work together on idea contexts and to develop branches of multiple perspectives within those contexts. It also supports arranging and nesting different but related contexts into more complex idea structures. Video timestamp. See also this video here . Set Match Games : a platform for collaborative for video game design. See also this video here . Hololution : personal productivity apps that will then transform into collaboration apps. Video timestamp: https://youtu.be/Y8gXQankAu8?t=503. Omni : The Omni project establishes a global scholarly commons. Omni is a distributed scholarly network for direct scholarly communication, treated as main access point for all other Omni Project applications. humm : a beautiful p2p publishing platform. Control your content and its distribution, own your creativity and humanity. They're aiming to foster a new kind of economy around content creation (which Holochain would be suitable for with microtransfers). Video timestamp: https://youtu.be/Y8gXQankAu8?t=597. Comet . Holochain powered distributed Reddit-like application, with differences. \"Because it is built on Holochain, Comet has no censorship or moderation, and can run on the devices of the users without the help of servers. Comet also replaces the idea of \"subreddits\" with tags, and allows posts to be created and crossposted with more than 1 tag. Perhaps the most interesting feature of Comet, though, is its voting system. Firstly, votes can be fractional, they range from -1 to 1. Votes are also not counted as \"objective\". The scores of posts and comments are always calculated from the perspective of a particular agent (the user). This agent will have upvoted and downvoted other posts from other people. The score is counted depending on how the agent has voted on the other voters in the past. This helps fight spam, vote manipulation, and helps maintain community in an otherwise lawless space.\" See also this blog post and https://youtu.be/Y8gXQankAu8?t=636. Junto . A social network designed for authenticity. See also https://holo.host/project/junto/. Core.Network : a social network designed to unify all social networks into a single visual dashboard. Haven: see https://www.youtube.com/watch?v=Y8gXQankAu8&t=809s. Holohouse : Coliving, Coworking and Coop, sustainable and with a purpose Allianceblock : \"The All-In-One Investment Ecosystem\" Orion : a liquidity aggregator protocol. \"A standard for connecting to centralized and decentralized exchanges, enabling an ecosystem of dApps to solve liquidity issues and price parity. The Orion Protocol enables cross chain trading, omni-exchange accessibility, and liquidity.\" Pacio\u2019s Tender . A zero-fee transaction platform for distributed apps, built on Holochain. Jala : \"A digital platform that makes participation in any venture verifiable, intuitive and automated without the need for intermediaries.\" Bizgees : Transforming refugees into entrepreneurs using FinTech. bHive : \"The bHive Cooperative is a community owned person-to-person sharing economy platform being developed for Bendigo [bank] by a team of five local entrepreneurs. bHive is the future of work.\" Joatu : skills and goods marketplace for local communities. 2RowFlow : \"A Digital Commons for Treaty People in Canada\" Arcade City Realgame . A decentralized ride-sharing platform. See also https://github.com/ArcadeCity/unter and https://forum.arcade.city/t/pivoting-upward/37. It's not clear whether the latest closed-source implementation is built on Holochain\u2014so it's best to assume that it's not. Infinite World Game : \"an operating system for humanity. A whole systems breakthrough in service to life. The Infinite World Game (IWG) is an unprecedented innovation. A vision and whole-systems platform for a society and world that does things differently. Cocreative players in the IWG organise themselves as living cells in a planetary super-organism (or \u2018Universal Fractal Organism\u2019, UFO), where \u2018thriveability\u2019 is the new name of the game for humanity and the planet. Drawing on a new world view and the principles of living systems, the IWG moves beyond our current concepts of economics and social organisation and offers a fresh way to share and regulate resources, interrelate, develop innovations, realise ourselves and co-ordinate our intelligence. As we elevate our capacity as a species, the IWG reorganises our social processes to go beyond product and profitability alone towards a world where every action is creating more life, love and thriveability for all.\" Sacred Capital : infrastructure for building and exchanging reputational credit. See also this blog post . HoloREA : happ infrastructure that uses the Resources, events, agents (REA) accounting model , with an implementation based on the ValueFlows protocol, and is built on Holochain . See also this video . Cogov : a framework for experimentation with social system innovation. See also http://cogov.tech/. Creafree : \"Creafree registers, publishes and promotes creations to better meet the needs of the 21st century.\" Ulex : an open source legal system. \"In the case of Ulex, the \u201csoftware\u201d is not necessarily code (although you can access Tom W Bell\u2019s Ulex Gitrepository here), rather they are rules that a legal system can operate under. Because Ulex is open source, its foundation is not tied to any one country and can be used in different jurisdictions. The goal of Ulex is to foster an open source community that creates and tries different variants of Ulex. Ulex protects human rights with an efficient and fair dispute resolution process, promoting the rule of law. It is not imposed by any one government, but instead is adopted by the mutual consent of those it governs. \" See also https://physes.github.io/Ulex/ and https://ulex.app/. Our World/HoloNET/HoloUnity/OASIS API Our World is an exciting immersive next generation 3D XR/IR (Infinite Reality) educational game/platform/social network/ecosystem teaching people on how to look after themselves, each other and the planet using the latest cutting-edge technology. It is the XR/IR Gamification layer of the new interplanetary operating system & the new internet (Web 3.0), which is being built by the elite technical wizards stationed around the world. It is a 5th dimensional and ascension training platform, teaching people vital life lessons as well as acting as a real-time simulation of the real world. Our World uses HoloNET (.NET Holochain client) and HoloUnity (Unity Holochain Client) and the OASIS API/Karma/Profile/Avatar System. Read more on our GitHub Page or on the Holochain Forum . See also \u00b6 this video: 37 Projects Building on Holochain .","title":"Built With Holochain"},{"location":"guide/built_with_holochain/#built-with-holochain","text":"Please click \"suggest an edit\", and add something you built with Holochain to the list! Holochain Basic Chat Todo list UI (Gatsby/Redux) TODOlist example HoloChat Simple UI HoloVault Simple App Todo List tutorial Hylo : a social network for community management, messaging and collaboration. They have a working network, although apparently with no activity. See also https://github.com/Hylozoic/hylo-holo-dnas. Coolcats2 : happ intended to replace Twitter. Learn Using Test Driven Development Holo . (People often confuse Holo and Holochain. To make it clear they are not the same thing, Holo is actually a web hosting happ, built on Holochain, to make it easier to bridge users to Holochain via web apps without having to know anything about Holochain, or do anything with it. It is not the same thing as Holochain, the agent-centric distributed app framework with unlimited scalability, or with scalability at least proportional to the number of nodes, with the limit on that number only being constrained by physics, number of computers,). Producer\u2019s Market . Provides a market or exchange for producers and consumers, with at least the initial focus on agriculture. See also this blog post . Morpheus Network : automated, global, platform-agnostic supply-chain network. Holosupply : digital supply chain integration platform. Hypergroove : a data-driven, SaaS platform for more sustainable, inclusive, community-driven agri-food systems, with a focus on urban agriculture and p2p trade. Redgrid : a SaaS platform for the Internet of Energy, connecting energy generation and storage devices and owners with power-consuming appliances and users. The first app built will be \"Market Adaptive Demand Management, which will allow users\u2019 high consumption devices\u2014such as HVACs\u2014to automatically respond to external market prices and signals in order to reduce consumption and costs. This will be applied to households, commercial buildings and multi-building sites. The application will be integrated into utility demand management programs to pay users for reducing their demand peaks, easing strain on the grid.\" It also aims to provide access to the 1.2b people in energy poverty. See also https://holo.host/project/redgrid/. Metavents : a planning platform for events and humanitarian project with event fees donated to the same humanitarian projects, using the transparency of Holochain. It uses visual planning with 2D, 3D, VR/AR and AI. Bridgit / Crowdfact . Bridgit is a Distributed Search and Discovery protocol and a protocol for the OverWeb, a new layer of trust, ideas, interactions, and experiences that display on top of the page source. Available as a Chrome extension. Crowdfact is part of Bridgit and involves crowd-sourced fact-checking. Scribe : The SCRIBE Project aims to build a distributed Conceptionary Manager which is a distributed application that allows to manage one or more Conceptionaries. There's a Github repo at https://github.com/iPlumb3r/Th3Sr1b3Pr0j3ct. Prtcl (The Underscore Protocol) : a way for people to share ideas, content creation, and conversation with each other. It is inspired by Git, which is a version control system for distributed tracking changes in software development. It allows collaborators to work together on idea contexts and to develop branches of multiple perspectives within those contexts. It also supports arranging and nesting different but related contexts into more complex idea structures. Video timestamp. See also this video here . Set Match Games : a platform for collaborative for video game design. See also this video here . Hololution : personal productivity apps that will then transform into collaboration apps. Video timestamp: https://youtu.be/Y8gXQankAu8?t=503. Omni : The Omni project establishes a global scholarly commons. Omni is a distributed scholarly network for direct scholarly communication, treated as main access point for all other Omni Project applications. humm : a beautiful p2p publishing platform. Control your content and its distribution, own your creativity and humanity. They're aiming to foster a new kind of economy around content creation (which Holochain would be suitable for with microtransfers). Video timestamp: https://youtu.be/Y8gXQankAu8?t=597. Comet . Holochain powered distributed Reddit-like application, with differences. \"Because it is built on Holochain, Comet has no censorship or moderation, and can run on the devices of the users without the help of servers. Comet also replaces the idea of \"subreddits\" with tags, and allows posts to be created and crossposted with more than 1 tag. Perhaps the most interesting feature of Comet, though, is its voting system. Firstly, votes can be fractional, they range from -1 to 1. Votes are also not counted as \"objective\". The scores of posts and comments are always calculated from the perspective of a particular agent (the user). This agent will have upvoted and downvoted other posts from other people. The score is counted depending on how the agent has voted on the other voters in the past. This helps fight spam, vote manipulation, and helps maintain community in an otherwise lawless space.\" See also this blog post and https://youtu.be/Y8gXQankAu8?t=636. Junto . A social network designed for authenticity. See also https://holo.host/project/junto/. Core.Network : a social network designed to unify all social networks into a single visual dashboard. Haven: see https://www.youtube.com/watch?v=Y8gXQankAu8&t=809s. Holohouse : Coliving, Coworking and Coop, sustainable and with a purpose Allianceblock : \"The All-In-One Investment Ecosystem\" Orion : a liquidity aggregator protocol. \"A standard for connecting to centralized and decentralized exchanges, enabling an ecosystem of dApps to solve liquidity issues and price parity. The Orion Protocol enables cross chain trading, omni-exchange accessibility, and liquidity.\" Pacio\u2019s Tender . A zero-fee transaction platform for distributed apps, built on Holochain. Jala : \"A digital platform that makes participation in any venture verifiable, intuitive and automated without the need for intermediaries.\" Bizgees : Transforming refugees into entrepreneurs using FinTech. bHive : \"The bHive Cooperative is a community owned person-to-person sharing economy platform being developed for Bendigo [bank] by a team of five local entrepreneurs. bHive is the future of work.\" Joatu : skills and goods marketplace for local communities. 2RowFlow : \"A Digital Commons for Treaty People in Canada\" Arcade City Realgame . A decentralized ride-sharing platform. See also https://github.com/ArcadeCity/unter and https://forum.arcade.city/t/pivoting-upward/37. It's not clear whether the latest closed-source implementation is built on Holochain\u2014so it's best to assume that it's not. Infinite World Game : \"an operating system for humanity. A whole systems breakthrough in service to life. The Infinite World Game (IWG) is an unprecedented innovation. A vision and whole-systems platform for a society and world that does things differently. Cocreative players in the IWG organise themselves as living cells in a planetary super-organism (or \u2018Universal Fractal Organism\u2019, UFO), where \u2018thriveability\u2019 is the new name of the game for humanity and the planet. Drawing on a new world view and the principles of living systems, the IWG moves beyond our current concepts of economics and social organisation and offers a fresh way to share and regulate resources, interrelate, develop innovations, realise ourselves and co-ordinate our intelligence. As we elevate our capacity as a species, the IWG reorganises our social processes to go beyond product and profitability alone towards a world where every action is creating more life, love and thriveability for all.\" Sacred Capital : infrastructure for building and exchanging reputational credit. See also this blog post . HoloREA : happ infrastructure that uses the Resources, events, agents (REA) accounting model , with an implementation based on the ValueFlows protocol, and is built on Holochain . See also this video . Cogov : a framework for experimentation with social system innovation. See also http://cogov.tech/. Creafree : \"Creafree registers, publishes and promotes creations to better meet the needs of the 21st century.\" Ulex : an open source legal system. \"In the case of Ulex, the \u201csoftware\u201d is not necessarily code (although you can access Tom W Bell\u2019s Ulex Gitrepository here), rather they are rules that a legal system can operate under. Because Ulex is open source, its foundation is not tied to any one country and can be used in different jurisdictions. The goal of Ulex is to foster an open source community that creates and tries different variants of Ulex. Ulex protects human rights with an efficient and fair dispute resolution process, promoting the rule of law. It is not imposed by any one government, but instead is adopted by the mutual consent of those it governs. \" See also https://physes.github.io/Ulex/ and https://ulex.app/. Our World/HoloNET/HoloUnity/OASIS API Our World is an exciting immersive next generation 3D XR/IR (Infinite Reality) educational game/platform/social network/ecosystem teaching people on how to look after themselves, each other and the planet using the latest cutting-edge technology. It is the XR/IR Gamification layer of the new interplanetary operating system & the new internet (Web 3.0), which is being built by the elite technical wizards stationed around the world. It is a 5th dimensional and ascension training platform, teaching people vital life lessons as well as acting as a real-time simulation of the real world. Our World uses HoloNET (.NET Holochain client) and HoloUnity (Unity Holochain Client) and the OASIS API/Karma/Profile/Avatar System. Read more on our GitHub Page or on the Holochain Forum .","title":"Built With Holochain"},{"location":"guide/built_with_holochain/#see-also","text":"this video: 37 Projects Building on Holochain .","title":"See also"},{"location":"guide/conductor_admin/","text":"Administering Conductors \u00b6 It is possible to dynamically configure a Conductor via a JSON-RPC interface connection. There is a powerful API that is exposed for doing so. To do this, first, recall that the admin = true property needs to be set for the interface that should allow admin access. Second, it is helpful to review and understand the behaviours around the persistence_dir property for the Conductor. You can find details of the API for this functionality in the full API reference material . Scroll to view the with_admin_dna_functions comment block and the with_admin_ui_functions comment block. Calling these functions works exactly the same way as the other JSON-RPC API calls . As mentioned in production Conductor , there is a GUI in development that will cover all this functionality, so that it does not have to be done programmatically, but can be done by any user simply point and click.","title":"Conductor Admin"},{"location":"guide/conductor_admin/#administering-conductors","text":"It is possible to dynamically configure a Conductor via a JSON-RPC interface connection. There is a powerful API that is exposed for doing so. To do this, first, recall that the admin = true property needs to be set for the interface that should allow admin access. Second, it is helpful to review and understand the behaviours around the persistence_dir property for the Conductor. You can find details of the API for this functionality in the full API reference material . Scroll to view the with_admin_dna_functions comment block and the with_admin_ui_functions comment block. Calling these functions works exactly the same way as the other JSON-RPC API calls . As mentioned in production Conductor , there is a GUI in development that will cover all this functionality, so that it does not have to be done programmatically, but can be done by any user simply point and click.","title":"Administering Conductors"},{"location":"guide/conductor_agents/","text":"Agents \u00b6 agents is an array of configurations for \"agents\". This means that you can define, and later reference, multiple distinct agents in this single config file. An \"agent\" has a name, ID, public address and is defined by a private key that resides in a file on their device. Required : agents is a required property in the config file. It is the ONLY required property. Properties \u00b6 id : string \u00b6 Give an ID of your choice to the agent name : string \u00b6 Give a name of your choice to the agent public_address : string \u00b6 A public address for the agent. Run hc keygen and copy the public address to this value keystore_file : string \u00b6 Path to the keystore file for this agent. Copy the path from when you ran hc keygen into this value. Example \u00b6 [[agents]] id = \"test_agent2\" name = \"HoloTester2\" public_address = \"HcSCJts3fQ6Y4c4xr795Zj6inhTjecrfrsSFOrU9Jmnhnj5bdoXkoPSJivrm3wi\" keystore_file = \"/org.holochain.holochain/keys/HcSCJts3fQ6Y4c4xr795Zj6inhTjecrfrsSFOrU9Jmnhnj5bdoXkoPSJivrm3wi\"","title":"Agents"},{"location":"guide/conductor_agents/#agents","text":"agents is an array of configurations for \"agents\". This means that you can define, and later reference, multiple distinct agents in this single config file. An \"agent\" has a name, ID, public address and is defined by a private key that resides in a file on their device. Required : agents is a required property in the config file. It is the ONLY required property.","title":"Agents"},{"location":"guide/conductor_agents/#properties","text":"","title":"Properties"},{"location":"guide/conductor_agents/#id-string","text":"Give an ID of your choice to the agent","title":"id: string"},{"location":"guide/conductor_agents/#name-string","text":"Give a name of your choice to the agent","title":"name: string"},{"location":"guide/conductor_agents/#public_address-string","text":"A public address for the agent. Run hc keygen and copy the public address to this value","title":"public_address: string"},{"location":"guide/conductor_agents/#keystore_file-string","text":"Path to the keystore file for this agent. Copy the path from when you ran hc keygen into this value.","title":"keystore_file: string"},{"location":"guide/conductor_agents/#example","text":"[[agents]] id = \"test_agent2\" name = \"HoloTester2\" public_address = \"HcSCJts3fQ6Y4c4xr795Zj6inhTjecrfrsSFOrU9Jmnhnj5bdoXkoPSJivrm3wi\" keystore_file = \"/org.holochain.holochain/keys/HcSCJts3fQ6Y4c4xr795Zj6inhTjecrfrsSFOrU9Jmnhnj5bdoXkoPSJivrm3wi\"","title":"Example"},{"location":"guide/conductor_bridges/","text":"Bridges \u00b6 bridges is an array of configuration instances that are configured to be able to make calls to Zome functions of another instance. You can think of this of as configuring an internal direct interface between DNA instances. The section on bridging provides more information on how this ability is used to to compose complex applications out of many DNA instances. Optional Properties \u00b6 caller_id : string \u00b6 A reference to the given ID of a defined instance that calls the other one. This instance depends on the callee. callee_id : string \u00b6 A reference to the given ID of a defined instance that exposes capabilities through this bridge. This instance is used by the caller. handle : string \u00b6 The caller's local handle for this bridge and the callee. A caller can have many bridges to other DNAs and those DNAs could by bound dynamically. Callers reference callees by this arbitrary but unique local name. Example \u00b6 [[bridges]] caller_id = \"app1\" callee_id = \"app2\" handle = \"happ-store\"","title":"Bridges"},{"location":"guide/conductor_bridges/#bridges","text":"bridges is an array of configuration instances that are configured to be able to make calls to Zome functions of another instance. You can think of this of as configuring an internal direct interface between DNA instances. The section on bridging provides more information on how this ability is used to to compose complex applications out of many DNA instances. Optional","title":"Bridges"},{"location":"guide/conductor_bridges/#properties","text":"","title":"Properties"},{"location":"guide/conductor_bridges/#caller_id-string","text":"A reference to the given ID of a defined instance that calls the other one. This instance depends on the callee.","title":"caller_id: string"},{"location":"guide/conductor_bridges/#callee_id-string","text":"A reference to the given ID of a defined instance that exposes capabilities through this bridge. This instance is used by the caller.","title":"callee_id: string"},{"location":"guide/conductor_bridges/#handle-string","text":"The caller's local handle for this bridge and the callee. A caller can have many bridges to other DNAs and those DNAs could by bound dynamically. Callers reference callees by this arbitrary but unique local name.","title":"handle: string"},{"location":"guide/conductor_bridges/#example","text":"[[bridges]] caller_id = \"app1\" callee_id = \"app2\" handle = \"happ-store\"","title":"Example"},{"location":"guide/conductor_dnas/","text":"DNAs \u00b6 dnas is an array of configurations for \"DNAs\" that are available to be instantiated in the Conductor. A DNA is a packaged JSON file containing a valid DNA configuration including the WASM code for the Zomes. How to package DNA from source files can be read about here . Optional Properties \u00b6 id : string \u00b6 Give an ID of your choice to this DNA file : string \u00b6 Path to the packaged DNA file hash : string Optional \u00b6 A hash can optionally be provided, which could be used to validate that the DNA being installed is the DNA that was intended to be installed. Example \u00b6 [[dnas]] id = \"app spec rust\" file = \"example-config/app_spec.dna.json\"","title":"DNAs"},{"location":"guide/conductor_dnas/#dnas","text":"dnas is an array of configurations for \"DNAs\" that are available to be instantiated in the Conductor. A DNA is a packaged JSON file containing a valid DNA configuration including the WASM code for the Zomes. How to package DNA from source files can be read about here . Optional","title":"DNAs"},{"location":"guide/conductor_dnas/#properties","text":"","title":"Properties"},{"location":"guide/conductor_dnas/#id-string","text":"Give an ID of your choice to this DNA","title":"id: string"},{"location":"guide/conductor_dnas/#file-string","text":"Path to the packaged DNA file","title":"file: string"},{"location":"guide/conductor_dnas/#hash-string-optional","text":"A hash can optionally be provided, which could be used to validate that the DNA being installed is the DNA that was intended to be installed.","title":"hash: string Optional"},{"location":"guide/conductor_dnas/#example","text":"[[dnas]] id = \"app spec rust\" file = \"example-config/app_spec.dna.json\"","title":"Example"},{"location":"guide/conductor_instances/","text":"Instances \u00b6 instances is an array of configurations of DNA instances, each of which is a running copy of a DNA , by a particular agent . Based on these configurations, the Conductor will attempt to start up these instances, initializing (or resuming) a local source chain and DHT. It is possible to use the same DNA with multiple different agents , but it is not recommended to run two instances with the same DNA and same agent. An instance has a configurable storage property, which can be set to save to disk, or just store temporarily in memory, which is useful for testing purposes. Optional Properties \u00b6 id : string \u00b6 Give an ID of your choice to this instance agent : string \u00b6 A reference to the given ID of a defined agent dna : string \u00b6 A reference to the given ID of a defined DNA storage : StorageConfiguration \u00b6 A table for configuring the approach to storage of the local source chain and DHT for this instance StorageConfiguration.type : enum \u00b6 Select between different storage implementations. There are three so far: memory : Persist actions taken in this instance only to memory. Everything will disappear when the Conductor process stops. file : Persist actions taken in this instance to the disk of the device the Conductor is running on. If the Conductor process stops and then restarts, the actions taken will resume at the place in the local source chain they last were at. pickle : Persists to a fast memory call which is eventually persisted to a file storage every 5 seconds. The actions taken will also resume at the place in the local source chain they were last. If an application error does occur, it will make sure to persist the latest data prior to any shutdown occurring. StorageConfiguration.path : string \u00b6 Path to the folder in which to store the data for this instance. Example \u00b6 [[instances]] id = \"app spec instance 1\" agent = \"test agent 1\" dna = \"app spec rust\" [instances.storage] type = \"file\" path = \"example-config/tmp-storage\"","title":"Instances"},{"location":"guide/conductor_instances/#instances","text":"instances is an array of configurations of DNA instances, each of which is a running copy of a DNA , by a particular agent . Based on these configurations, the Conductor will attempt to start up these instances, initializing (or resuming) a local source chain and DHT. It is possible to use the same DNA with multiple different agents , but it is not recommended to run two instances with the same DNA and same agent. An instance has a configurable storage property, which can be set to save to disk, or just store temporarily in memory, which is useful for testing purposes. Optional","title":"Instances"},{"location":"guide/conductor_instances/#properties","text":"","title":"Properties"},{"location":"guide/conductor_instances/#id-string","text":"Give an ID of your choice to this instance","title":"id: string"},{"location":"guide/conductor_instances/#agent-string","text":"A reference to the given ID of a defined agent","title":"agent: string"},{"location":"guide/conductor_instances/#dna-string","text":"A reference to the given ID of a defined DNA","title":"dna: string"},{"location":"guide/conductor_instances/#storage-storageconfiguration","text":"A table for configuring the approach to storage of the local source chain and DHT for this instance","title":"storage: StorageConfiguration"},{"location":"guide/conductor_instances/#storageconfigurationtype-enum","text":"Select between different storage implementations. There are three so far: memory : Persist actions taken in this instance only to memory. Everything will disappear when the Conductor process stops. file : Persist actions taken in this instance to the disk of the device the Conductor is running on. If the Conductor process stops and then restarts, the actions taken will resume at the place in the local source chain they last were at. pickle : Persists to a fast memory call which is eventually persisted to a file storage every 5 seconds. The actions taken will also resume at the place in the local source chain they were last. If an application error does occur, it will make sure to persist the latest data prior to any shutdown occurring.","title":"StorageConfiguration.type: enum"},{"location":"guide/conductor_instances/#storageconfigurationpath-string","text":"Path to the folder in which to store the data for this instance.","title":"StorageConfiguration.path: string"},{"location":"guide/conductor_instances/#example","text":"[[instances]] id = \"app spec instance 1\" agent = \"test agent 1\" dna = \"app spec rust\" [instances.storage] type = \"file\" path = \"example-config/tmp-storage\"","title":"Example"},{"location":"guide/conductor_interfaces/","text":"Interfaces \u00b6 interfaces is an array of configurations of the channels (e.g. http or websockets) that the Conductor will use to send information to and from instances and users. Interfaces are user facing and make Zome functions, info, and optionally admin functions available to GUIs, browser based web UIs, local native UIs, and other local applications and scripts. The following implementations are already developed: WebSockets HTTP The instances (referenced by ID) that are to be made available via that interface should be listed. An admin flag can enable special Conductor functions for programatically changing the configuration (e.g. installing apps), which even persists back to the configuration file. Optional Properties \u00b6 id : string \u00b6 Give an ID of your choice to this interface driver : InterfaceDriver \u00b6 A table which should provide info regarding the protocol and port over which this interface should run InterfaceDriver.type : enum \u00b6 Select between different protocols for serving the API. There are two so far: websocket : serve the API as JSON-RPC via WebSockets http : serve the API as JSON-RPC via HTTP These are discussed in great detail in Intro to JSON-RPC Interfaces , and the following articles. InterfaceDriver.port : u16 \u00b6 An integer value representing the port on the device to run this interface over admin : bool Optional \u00b6 Whether to expose admin level functions for dynamically administering the Conductor via this JSON-RPC interface. Defaults to false. instances : array of InstanceReferenceConfiguration \u00b6 An array of tables which should provide the IDs of instances to serve over this interface. Only the ones which are listed here will be served. InstanceReferenceConfiguration.id : string \u00b6 A reference to the given ID of a defined instance Example Without Admin \u00b6 [[interfaces]] id = \"websocket interface\" [[interfaces.instances]] id = \"app spec instance 1\" [interfaces.driver] type = \"websocket\" port = 4000 Example With Admin \u00b6 [[interfaces]] id = \"http interface\" admin = true [[interfaces.instances]] id = \"app spec instance 1\" [interfaces.driver] type = \"http\" port = 4000","title":"Interfaces"},{"location":"guide/conductor_interfaces/#interfaces","text":"interfaces is an array of configurations of the channels (e.g. http or websockets) that the Conductor will use to send information to and from instances and users. Interfaces are user facing and make Zome functions, info, and optionally admin functions available to GUIs, browser based web UIs, local native UIs, and other local applications and scripts. The following implementations are already developed: WebSockets HTTP The instances (referenced by ID) that are to be made available via that interface should be listed. An admin flag can enable special Conductor functions for programatically changing the configuration (e.g. installing apps), which even persists back to the configuration file. Optional","title":"Interfaces"},{"location":"guide/conductor_interfaces/#properties","text":"","title":"Properties"},{"location":"guide/conductor_interfaces/#id-string","text":"Give an ID of your choice to this interface","title":"id: string"},{"location":"guide/conductor_interfaces/#driver-interfacedriver","text":"A table which should provide info regarding the protocol and port over which this interface should run","title":"driver: InterfaceDriver"},{"location":"guide/conductor_interfaces/#interfacedrivertype-enum","text":"Select between different protocols for serving the API. There are two so far: websocket : serve the API as JSON-RPC via WebSockets http : serve the API as JSON-RPC via HTTP These are discussed in great detail in Intro to JSON-RPC Interfaces , and the following articles.","title":"InterfaceDriver.type: enum"},{"location":"guide/conductor_interfaces/#interfacedriverport-u16","text":"An integer value representing the port on the device to run this interface over","title":"InterfaceDriver.port: u16"},{"location":"guide/conductor_interfaces/#admin-bool-optional","text":"Whether to expose admin level functions for dynamically administering the Conductor via this JSON-RPC interface. Defaults to false.","title":"admin: bool Optional"},{"location":"guide/conductor_interfaces/#instances-array-of-instancereferenceconfiguration","text":"An array of tables which should provide the IDs of instances to serve over this interface. Only the ones which are listed here will be served.","title":"instances: array of InstanceReferenceConfiguration"},{"location":"guide/conductor_interfaces/#instancereferenceconfigurationid-string","text":"A reference to the given ID of a defined instance","title":"InstanceReferenceConfiguration.id: string"},{"location":"guide/conductor_interfaces/#example-without-admin","text":"[[interfaces]] id = \"websocket interface\" [[interfaces.instances]] id = \"app spec instance 1\" [interfaces.driver] type = \"websocket\" port = 4000","title":"Example Without Admin"},{"location":"guide/conductor_interfaces/#example-with-admin","text":"[[interfaces]] id = \"http interface\" admin = true [[interfaces.instances]] id = \"app spec instance 1\" [interfaces.driver] type = \"http\" port = 4000","title":"Example With Admin"},{"location":"guide/conductor_json_rpc_api/","text":"Conductor JSON-RPC API \u00b6 Querying Running DNA Instances \u00b6 Holochain Conductors expose a method info/instances . This method returns a list of the running DNA instances in the Conductor. For each running instance, it provides the instance \"ID\", the name of the DNA, and the agent \"id\". The instance IDs will be particularly useful in other circumstances. The method info/instances doesn't require any input parameters, so params can be left off the request. Example \u00b6 example request { \"jsonrpc\" : \"2.0\" , \"id\" : \"0\" , \"method\" : \"info/instances\" } example response { \"jsonrpc\" : \"2.0\" , \"result\" : [{ \"id\" : \"test-instance\" , \"dna\" : \"hc-run-dna\" , \"agent\" : \"hc-run-agent\" }], \"id\" : \"0\" } Calling Zome Functions \u00b6 The following explains the general JSON-RPC pattern for how to call a Zome function. Unlike info/instances , a Zome function call also expects arguments. We will need to include a JSON-RPC args field in our RPC call. To call a Zome function, use \"call\" as the JSON-RPC method , and a params object with four items: 1. instance_id : The instance ID, corresponding to the instance IDs returned by info/instances 2. zome : The name of the Zome 3. function : The name of the function 4. args : The actual parameters of the zome function call In the last example, the instance ID \"test-instance\" was returned, which can be used here as the instance ID. Say there was a Zome in a DNA called \"blogs\", this is the Zome name. That Zome has a function called \"create_blog\", that is the function name. Any top level keys of the args field should correspond exactly with the name of an argument expected by the Zome method being called. Example Zome Function Arguments { \"blog\" : { \"content\" : \"sample content\" }} Example Request \u00b6 example request { \"jsonrpc\" : \"2.0\" , \"id\" : \"0\" , \"method\" : \"call\" , \"params\" : { \"instance_id\" : \"test-instance\" , \"zome\" : \"blog\" , \"function\" : \"create_blog\" , \"args\" : { \"blog\" : { \"content\" : \"sample content\" } } } } example response { \"jsonrpc\" : \"2.0\" , \"result\" : \"{\\\"Ok\\\":\\\"QmUwoQAtmg7frBjcn1GZX5fwcPf3ENiiMhPPro6DBM4V19\\\"}\" , \"id\" : \"0\" } This response suggests that the function call was successful (\"Ok\") and provides the DHT address of the freshly committed blog entry (\"QmU...\").","title":"Conductor JSON-RPC API"},{"location":"guide/conductor_json_rpc_api/#conductor-json-rpc-api","text":"","title":"Conductor JSON-RPC API"},{"location":"guide/conductor_json_rpc_api/#querying-running-dna-instances","text":"Holochain Conductors expose a method info/instances . This method returns a list of the running DNA instances in the Conductor. For each running instance, it provides the instance \"ID\", the name of the DNA, and the agent \"id\". The instance IDs will be particularly useful in other circumstances. The method info/instances doesn't require any input parameters, so params can be left off the request.","title":"Querying Running DNA Instances"},{"location":"guide/conductor_json_rpc_api/#example","text":"example request { \"jsonrpc\" : \"2.0\" , \"id\" : \"0\" , \"method\" : \"info/instances\" } example response { \"jsonrpc\" : \"2.0\" , \"result\" : [{ \"id\" : \"test-instance\" , \"dna\" : \"hc-run-dna\" , \"agent\" : \"hc-run-agent\" }], \"id\" : \"0\" }","title":"Example"},{"location":"guide/conductor_json_rpc_api/#calling-zome-functions","text":"The following explains the general JSON-RPC pattern for how to call a Zome function. Unlike info/instances , a Zome function call also expects arguments. We will need to include a JSON-RPC args field in our RPC call. To call a Zome function, use \"call\" as the JSON-RPC method , and a params object with four items: 1. instance_id : The instance ID, corresponding to the instance IDs returned by info/instances 2. zome : The name of the Zome 3. function : The name of the function 4. args : The actual parameters of the zome function call In the last example, the instance ID \"test-instance\" was returned, which can be used here as the instance ID. Say there was a Zome in a DNA called \"blogs\", this is the Zome name. That Zome has a function called \"create_blog\", that is the function name. Any top level keys of the args field should correspond exactly with the name of an argument expected by the Zome method being called. Example Zome Function Arguments { \"blog\" : { \"content\" : \"sample content\" }}","title":"Calling Zome Functions"},{"location":"guide/conductor_json_rpc_api/#example-request","text":"example request { \"jsonrpc\" : \"2.0\" , \"id\" : \"0\" , \"method\" : \"call\" , \"params\" : { \"instance_id\" : \"test-instance\" , \"zome\" : \"blog\" , \"function\" : \"create_blog\" , \"args\" : { \"blog\" : { \"content\" : \"sample content\" } } } } example response { \"jsonrpc\" : \"2.0\" , \"result\" : \"{\\\"Ok\\\":\\\"QmUwoQAtmg7frBjcn1GZX5fwcPf3ENiiMhPPro6DBM4V19\\\"}\" , \"id\" : \"0\" } This response suggests that the function call was successful (\"Ok\") and provides the DHT address of the freshly committed blog entry (\"QmU...\").","title":"Example Request"},{"location":"guide/conductor_logging/","text":"Logging \u00b6 logger is a table for the configuration of how logging should behave in the Conductor. Select between types of loggers and setup rules for nicer display of the logs. There is only one logger per Conductor. Optional Properties \u00b6 type : enum Optional \u00b6 Select which type of logger to use for the Conductor. If you leave this off, \"simple\" logging is the default debug : enables more sophisticated logging with color coding and filters simple : a most minimal logger, no color coding or filtering rules : LogRules Optional \u00b6 A table for optionally adding a set of rules to the logger LogRules.rules : LogRule \u00b6 An array of tables containing the rules for the logger LogRule.pattern : Regex string \u00b6 A Regex pattern as a string to match a log message against, to see whether this rule should apply to it. LogRule.exclude : bool Optional \u00b6 Whether to use this pattern to exclude things that match from the logs. Defaults to false . This option is useful for when the logs seem noisy. LogRule.color : enum Optional \u00b6 What color to use in the terminal output for logs that match this pattern. Options: black, red, green, yellow, blue, magenta, cyan, white Example \u00b6 [logger] type = \"debug\" [[logger.rules.rules]] color = \"red\" exclude = false pattern = \"^err/\" [[logger.rules.rules]] color = \"white\" exclude = false pattern = \"^debug/dna\" [[logger.rules.rules]] exclude = true pattern = \"^debug/reduce\" [[logger.rules.rules]] exclude = false pattern = \".*\"","title":"Logging"},{"location":"guide/conductor_logging/#logging","text":"logger is a table for the configuration of how logging should behave in the Conductor. Select between types of loggers and setup rules for nicer display of the logs. There is only one logger per Conductor. Optional","title":"Logging"},{"location":"guide/conductor_logging/#properties","text":"","title":"Properties"},{"location":"guide/conductor_logging/#type-enum-optional","text":"Select which type of logger to use for the Conductor. If you leave this off, \"simple\" logging is the default debug : enables more sophisticated logging with color coding and filters simple : a most minimal logger, no color coding or filtering","title":"type: enum Optional"},{"location":"guide/conductor_logging/#rules-logrules-optional","text":"A table for optionally adding a set of rules to the logger","title":"rules: LogRules Optional"},{"location":"guide/conductor_logging/#logrulesrules-logrule","text":"An array of tables containing the rules for the logger","title":"LogRules.rules: LogRule"},{"location":"guide/conductor_logging/#logrulepattern-regex-string","text":"A Regex pattern as a string to match a log message against, to see whether this rule should apply to it.","title":"LogRule.pattern: Regex string"},{"location":"guide/conductor_logging/#logruleexclude-bool-optional","text":"Whether to use this pattern to exclude things that match from the logs. Defaults to false . This option is useful for when the logs seem noisy.","title":"LogRule.exclude: bool Optional"},{"location":"guide/conductor_logging/#logrulecolor-enum-optional","text":"What color to use in the terminal output for logs that match this pattern. Options: black, red, green, yellow, blue, magenta, cyan, white","title":"LogRule.color: enum Optional"},{"location":"guide/conductor_logging/#example","text":"[logger] type = \"debug\" [[logger.rules.rules]] color = \"red\" exclude = false pattern = \"^err/\" [[logger.rules.rules]] color = \"white\" exclude = false pattern = \"^debug/dna\" [[logger.rules.rules]] exclude = true pattern = \"^debug/reduce\" [[logger.rules.rules]] exclude = false pattern = \".*\"","title":"Example"},{"location":"guide/conductor_networking/","text":"Networking \u00b6 network is a table for the configuration of how networking should behave in the Conductor. The Conductor currently uses mock networking by default. To network with other nodes Holochain will automatically setup the n3h networking component . How n3h behaves can be configured with the following properties in a Conductor configuration file. Optional Properties \u00b6 n3h_persistence_path : string \u00b6 Absolute path to the directory that n3h uses to store persisted data. Each Conductor should have a separate folder that n3h_persistence_path should be set to, because each should be assigned a custom network ID which will be persisted within that folder, thus they need to be distinct. bootstrap_nodes : array of string Optional \u00b6 List of URIs that point to other nodes to bootstrap p2p connections. n3h_log_level : char \u00b6 Set the logging level used globally by N3H. Must be one of the following: 't', 'd', 'i', 'w', 'e' Each value corresponding to the industry standard log level: Trace, Debug, Info, Warning, Error. n3h_ipc_uri : string Optional \u00b6 URI pointing to an n3h process that is already running and not managed by this Conductor. If this is set the Conductor does not spawn n3h itself and ignores the path configs above. Default is this value is empty. Example \u00b6 [network] type = \"n3h\" n3h_persistence_path = \"./c1_network_files\" bootstrap_nodes = []","title":"Networking"},{"location":"guide/conductor_networking/#networking","text":"network is a table for the configuration of how networking should behave in the Conductor. The Conductor currently uses mock networking by default. To network with other nodes Holochain will automatically setup the n3h networking component . How n3h behaves can be configured with the following properties in a Conductor configuration file. Optional","title":"Networking"},{"location":"guide/conductor_networking/#properties","text":"","title":"Properties"},{"location":"guide/conductor_networking/#n3h_persistence_path-string","text":"Absolute path to the directory that n3h uses to store persisted data. Each Conductor should have a separate folder that n3h_persistence_path should be set to, because each should be assigned a custom network ID which will be persisted within that folder, thus they need to be distinct.","title":"n3h_persistence_path: string"},{"location":"guide/conductor_networking/#bootstrap_nodes-array-of-string-optional","text":"List of URIs that point to other nodes to bootstrap p2p connections.","title":"bootstrap_nodes: array of string Optional"},{"location":"guide/conductor_networking/#n3h_log_level-char","text":"Set the logging level used globally by N3H. Must be one of the following: 't', 'd', 'i', 'w', 'e' Each value corresponding to the industry standard log level: Trace, Debug, Info, Warning, Error.","title":"n3h_log_level: char"},{"location":"guide/conductor_networking/#n3h_ipc_uri-string-optional","text":"URI pointing to an n3h process that is already running and not managed by this Conductor. If this is set the Conductor does not spawn n3h itself and ignores the path configs above. Default is this value is empty.","title":"n3h_ipc_uri: string Optional"},{"location":"guide/conductor_networking/#example","text":"[network] type = \"n3h\" n3h_persistence_path = \"./c1_network_files\" bootstrap_nodes = []","title":"Example"},{"location":"guide/conductor_persistence_dir/","text":"Persistence Directory \u00b6 This is a simple key/value pair specifying a directory on the device to persist the config file, DNAs, and UI bundles, if changes are made dynamically over the JSON-RPC admin API. This is only relevant if you are running one of the interfaces with admin = true . The default value is in a subdirectory of the $HOME directory, $HOME/.holochain/conductor . Optional If you start a Conductor that has this value set, but then make no changes via the JSON-RPC admin interface, the persistence directory will not be utilized and the Conductor config file you started with will not be moved into that directory. On the other hand, if you do make any changes to the configuration by calling one of the dynamic admin functions then whatever the value of the persistence_dir is for that Conductor config, it will create that directory, and then persist the modified Conductor configuration file there. It would then be wise to utilize that Conductor config in the future, instead of the original. Within this persistence_dir that is now on the device, there are a number of possible files and folders. conductor-config.toml is the new configuration file, which will be repeatedly written to with any further dynamic updates. This is useful so that when the Conductor is stopped, or if it dies for some reason, when you restart it will behave the same as before. storage is a directory used for persisting the data for instances , in particular when new instances are added via the admin/instance/add admin function. dna is a directory used for copying DNA package files into if the admin/dna/install_from_file admin function is called. static is a directory used for copying UI Bundle files into if the admin/ui/install admin function is called. Example \u00b6 persistence_dir = \"/home/user/my_holochain\"","title":"Persistence Directory"},{"location":"guide/conductor_persistence_dir/#persistence-directory","text":"This is a simple key/value pair specifying a directory on the device to persist the config file, DNAs, and UI bundles, if changes are made dynamically over the JSON-RPC admin API. This is only relevant if you are running one of the interfaces with admin = true . The default value is in a subdirectory of the $HOME directory, $HOME/.holochain/conductor . Optional If you start a Conductor that has this value set, but then make no changes via the JSON-RPC admin interface, the persistence directory will not be utilized and the Conductor config file you started with will not be moved into that directory. On the other hand, if you do make any changes to the configuration by calling one of the dynamic admin functions then whatever the value of the persistence_dir is for that Conductor config, it will create that directory, and then persist the modified Conductor configuration file there. It would then be wise to utilize that Conductor config in the future, instead of the original. Within this persistence_dir that is now on the device, there are a number of possible files and folders. conductor-config.toml is the new configuration file, which will be repeatedly written to with any further dynamic updates. This is useful so that when the Conductor is stopped, or if it dies for some reason, when you restart it will behave the same as before. storage is a directory used for persisting the data for instances , in particular when new instances are added via the admin/instance/add admin function. dna is a directory used for copying DNA package files into if the admin/dna/install_from_file admin function is called. static is a directory used for copying UI Bundle files into if the admin/ui/install admin function is called.","title":"Persistence Directory"},{"location":"guide/conductor_persistence_dir/#example","text":"persistence_dir = \"/home/user/my_holochain\"","title":"Example"},{"location":"guide/conductor_ui_bundles/","text":"UI Bundles \u00b6 ui_bundles is an array of configurations of folders containing static assets, like HTML, CSS, and Javascript files, that will be accessed through a browser and used as a user interface for one or more DNA instances. These are served via UI Interfaces , which is covered next. Optional Properties \u00b6 id : string \u00b6 Give an ID of your choice to this UI Bundle root_dir : string \u00b6 Path to the folder containing the static files to serve hash : string Optional \u00b6 A hash can optionally be provided, which could be used to validate that the UI being installed is the UI bundle that was intended to be installed. Example \u00b6 [[ui_bundles]] id = \"bundle1\" root_dir = \"ui\"","title":"UI Bundles"},{"location":"guide/conductor_ui_bundles/#ui-bundles","text":"ui_bundles is an array of configurations of folders containing static assets, like HTML, CSS, and Javascript files, that will be accessed through a browser and used as a user interface for one or more DNA instances. These are served via UI Interfaces , which is covered next. Optional","title":"UI Bundles"},{"location":"guide/conductor_ui_bundles/#properties","text":"","title":"Properties"},{"location":"guide/conductor_ui_bundles/#id-string","text":"Give an ID of your choice to this UI Bundle","title":"id: string"},{"location":"guide/conductor_ui_bundles/#root_dir-string","text":"Path to the folder containing the static files to serve","title":"root_dir: string"},{"location":"guide/conductor_ui_bundles/#hash-string-optional","text":"A hash can optionally be provided, which could be used to validate that the UI being installed is the UI bundle that was intended to be installed.","title":"hash: string Optional"},{"location":"guide/conductor_ui_bundles/#example","text":"[[ui_bundles]] id = \"bundle1\" root_dir = \"ui\"","title":"Example"},{"location":"guide/conductor_ui_interfaces/","text":"UI Interfaces \u00b6 ui_interfaces is an array of configurations for \"UI Interfaces\", meaning there can be multiple within one Conductor. UI Interfaces serve UI Bundles over HTTP. Optional Properties \u00b6 id : string \u00b6 Give an ID of your choice to this UI Interface bundle : string \u00b6 A reference to the given ID of a defined ui_bundle to serve over this interface port : u16 \u00b6 An integer value representing the port on the device to run this interface over. Must not conflict with any of the interface ports, nor another UI Interface port. dna_interface : string Optional \u00b6 A reference to the given ID of a defined interface this UI is allowed to make calls to. This is used to set the CORS headers and also to provide an extra virtual file endpoint at /_dna_config/ that allows hc-web-client or another solution to redirect Holochain calls to the correct ip/port/protocol Example \u00b6 [[ui_interfaces]] id = \"ui-interface-1\" bundle = \"bundle1\" port = 3000 dna_interface = \"websocket_interface\"","title":"UI Interfaces"},{"location":"guide/conductor_ui_interfaces/#ui-interfaces","text":"ui_interfaces is an array of configurations for \"UI Interfaces\", meaning there can be multiple within one Conductor. UI Interfaces serve UI Bundles over HTTP. Optional","title":"UI Interfaces"},{"location":"guide/conductor_ui_interfaces/#properties","text":"","title":"Properties"},{"location":"guide/conductor_ui_interfaces/#id-string","text":"Give an ID of your choice to this UI Interface","title":"id: string"},{"location":"guide/conductor_ui_interfaces/#bundle-string","text":"A reference to the given ID of a defined ui_bundle to serve over this interface","title":"bundle: string"},{"location":"guide/conductor_ui_interfaces/#port-u16","text":"An integer value representing the port on the device to run this interface over. Must not conflict with any of the interface ports, nor another UI Interface port.","title":"port: u16"},{"location":"guide/conductor_ui_interfaces/#dna_interface-string-optional","text":"A reference to the given ID of a defined interface this UI is allowed to make calls to. This is used to set the CORS headers and also to provide an extra virtual file endpoint at /_dna_config/ that allows hc-web-client or another solution to redirect Holochain calls to the correct ip/port/protocol","title":"dna_interface: string Optional"},{"location":"guide/conductor_ui_interfaces/#example","text":"[[ui_interfaces]] id = \"ui-interface-1\" bundle = \"bundle1\" port = 3000 dna_interface = \"websocket_interface\"","title":"Example"},{"location":"guide/conductors/","text":"Running Holochain Apps: Conductors \u00b6 To introduce Conductors, it is useful to zoom out for a moment to the level of how Holochain runs on devices. Holochain was designed to be highly platform and system compatible. The core logic that runs a DNA instance was written in such a way that it could be included into many different codebases as a library, thus making it easier to build different implementations on the same platform as well as across platforms (MacOSX, Linux, Windows, Android, iOS, and more). Architecturally, Holochain DNAs are intended to be small composable units that provide bits of distributed data integrity functionality. Thus most Holochain based applications will actually be assemblages of many \"bridged\" DNA instances. For this to work we needed a distinct layer that orchestrates the data flow (i.e. zome function call requests and responses), between the transport layer (i.e. HTTP, Websockets, Unix domain sockets, etc) and the DNA instances. We call the layer that performs these two crucial functions, the Conductor , and we have written a conductor_api library to make it easy to build actual Conductor implementations. Conductors play quite a number of important roles: - installing, uninstalling, configuring, starting and stopping instances of DNA - exposing APIs to securely make function calls into the Zome functions of DNA instances - accepting information concerning the cryptographic keys and agent info to be used for identity and signing, and passing it into Holochain - establishing \"bridging\" between DNA instances - serving files for web based user interfaces that connect to these DNA instances over the interfaces Those are the basic functions of a Conductor, but in addition to that, a Conductor also allows for the configuration of the networking module for Holochain, enables logging, and if you choose to, exposes APIs at a special 'admin' level that allows for the dynamic configuration of the Conductor while it runs. By default, configuration of the Conductor is done via a static configuration file, written in TOML . In regards to the Zome functions APIs, Conductors can implement a diversity of interfaces to perform these function calls, creating an abundance of opportunity. Another way to build Holochain into an application is to use language bindings from the Rust built version of the Conductor, to another language, that then allows for the direct use of Holochain in that language. There are currently three Conductor implementations: - Nodejs - this is built using the language bindings approach, using neon - hc run - this is a zero config quick Conductor for development - holochain executable - this is a highly configurable sophisticated Conductor for running DNA instances long term The articles that follow discuss these different Conductors in greater detail. What is now known as a \"Conductor\" used to be called a \"Container\", so if you see the language of Container from other versions know that these refer to the same thing. Fun fact: because this component has such a variety of functions, there was some difficulty in naming it. The word \"Conductor\" was finally chosen because it actually implies multiple metaphors, each of which resonates with an aspect of what the Conductor does. Like an orchestra conductor, it helps several parts work together as a whole. Like a train conductor, it oversees and instructs how the engine runs. Like an electricity conductor, it allows information to pass through it.","title":"Conductors"},{"location":"guide/conductors/#running-holochain-apps-conductors","text":"To introduce Conductors, it is useful to zoom out for a moment to the level of how Holochain runs on devices. Holochain was designed to be highly platform and system compatible. The core logic that runs a DNA instance was written in such a way that it could be included into many different codebases as a library, thus making it easier to build different implementations on the same platform as well as across platforms (MacOSX, Linux, Windows, Android, iOS, and more). Architecturally, Holochain DNAs are intended to be small composable units that provide bits of distributed data integrity functionality. Thus most Holochain based applications will actually be assemblages of many \"bridged\" DNA instances. For this to work we needed a distinct layer that orchestrates the data flow (i.e. zome function call requests and responses), between the transport layer (i.e. HTTP, Websockets, Unix domain sockets, etc) and the DNA instances. We call the layer that performs these two crucial functions, the Conductor , and we have written a conductor_api library to make it easy to build actual Conductor implementations. Conductors play quite a number of important roles: - installing, uninstalling, configuring, starting and stopping instances of DNA - exposing APIs to securely make function calls into the Zome functions of DNA instances - accepting information concerning the cryptographic keys and agent info to be used for identity and signing, and passing it into Holochain - establishing \"bridging\" between DNA instances - serving files for web based user interfaces that connect to these DNA instances over the interfaces Those are the basic functions of a Conductor, but in addition to that, a Conductor also allows for the configuration of the networking module for Holochain, enables logging, and if you choose to, exposes APIs at a special 'admin' level that allows for the dynamic configuration of the Conductor while it runs. By default, configuration of the Conductor is done via a static configuration file, written in TOML . In regards to the Zome functions APIs, Conductors can implement a diversity of interfaces to perform these function calls, creating an abundance of opportunity. Another way to build Holochain into an application is to use language bindings from the Rust built version of the Conductor, to another language, that then allows for the direct use of Holochain in that language. There are currently three Conductor implementations: - Nodejs - this is built using the language bindings approach, using neon - hc run - this is a zero config quick Conductor for development - holochain executable - this is a highly configurable sophisticated Conductor for running DNA instances long term The articles that follow discuss these different Conductors in greater detail. What is now known as a \"Conductor\" used to be called a \"Container\", so if you see the language of Container from other versions know that these refer to the same thing. Fun fact: because this component has such a variety of functions, there was some difficulty in naming it. The word \"Conductor\" was finally chosen because it actually implies multiple metaphors, each of which resonates with an aspect of what the Conductor does. Like an orchestra conductor, it helps several parts work together as a whole. Like a train conductor, it oversees and instructs how the engine runs. Like an electricity conductor, it allows information to pass through it.","title":"Running Holochain Apps: Conductors"},{"location":"guide/configuration_alternatives/","text":"Configuration Alternatives \u00b6 It is possible to use the same configuration as you would for the holochain Conductor , and pass it to the constructor for Conductor . The configuration may be a string of valid TOML, or a JavaScript object with the equivalent structure. To review the configuration, go here . To see some examples of what these configuration files can look like, you can check out this folder on GitHub . Using a Plain Old Javascript Object \u00b6 const { Conductor } = require ( '@holochain/holochain-nodejs' ) const conductor = new Conductor ({ agents : [], dnas : [], instances : [], bridges : [], // etc... }) Using TOML \u00b6 const { Conductor } = require ( '@holochain/holochain-nodejs' ) const toml = ` [[agents]] <agent config> [[dnas]] <dna config> [[instances]] ...etc... ` const conductor = new Conductor ( toml )","title":"Configuration Alternatives"},{"location":"guide/configuration_alternatives/#configuration-alternatives","text":"It is possible to use the same configuration as you would for the holochain Conductor , and pass it to the constructor for Conductor . The configuration may be a string of valid TOML, or a JavaScript object with the equivalent structure. To review the configuration, go here . To see some examples of what these configuration files can look like, you can check out this folder on GitHub .","title":"Configuration Alternatives"},{"location":"guide/configuration_alternatives/#using-a-plain-old-javascript-object","text":"const { Conductor } = require ( '@holochain/holochain-nodejs' ) const conductor = new Conductor ({ agents : [], dnas : [], instances : [], bridges : [], // etc... })","title":"Using a Plain Old Javascript Object"},{"location":"guide/configuration_alternatives/#using-toml","text":"const { Conductor } = require ( '@holochain/holochain-nodejs' ) const toml = ` [[agents]] <agent config> [[dnas]] <dna config> [[instances]] ...etc... ` const conductor = new Conductor ( toml )","title":"Using TOML"},{"location":"guide/configuring_an_app/","text":"Configuring an App \u00b6 As mentioned in Intro to DNA: Configuration at the top level of a Holochain app source code folder there should be a file named app.json . This file is useful for two primary things: When executing your application, Holochain can adopt specific behaviours, that can be configured in the app.json file. These mostly relate to how the Distributed Hash Table and P2P gossip functions. You can give app users, and other developers background info about your application, such as the name of the app, and the author. Here are the properties currently in use: Property Description name Give this application or service a name. description Describe this application or service for other people to read. authors Optionally provide contact details for the app developer(s). It is an array, so multiple people can be referenced. authors.identifier A string including a name, and a public email for the contact person. authors.public_key_source Can reference a publicly hosted cryptographic \"public key\" from a private-public key-pair. authors.signature The app developer can optionally add a string that is signed by their private key, so that app users could verify the authenticity of the application. version Provides a version number for this application. Version numbers are incredibly important for distributed apps, so use this property wisely. dht This is a placeholder for the configuration options that Holochain will implement, regarding the Distributed Hash Table. It will provide a number of ways that the DHT behaviour can be customized. properties Properties, if used, can be an object which implements numerous app specific configuration values. These can be up to the app developer to define, and, when implemented, will be able to be called using the property function of the Zome API. The minimum recommended values to set when you initialize a new project folder are: name description authors version To edit them, just open app.json in a text editor (preferably one with syntax highlighting for JSON), change the values, and save the file.","title":"Configuring an App"},{"location":"guide/configuring_an_app/#configuring-an-app","text":"As mentioned in Intro to DNA: Configuration at the top level of a Holochain app source code folder there should be a file named app.json . This file is useful for two primary things: When executing your application, Holochain can adopt specific behaviours, that can be configured in the app.json file. These mostly relate to how the Distributed Hash Table and P2P gossip functions. You can give app users, and other developers background info about your application, such as the name of the app, and the author. Here are the properties currently in use: Property Description name Give this application or service a name. description Describe this application or service for other people to read. authors Optionally provide contact details for the app developer(s). It is an array, so multiple people can be referenced. authors.identifier A string including a name, and a public email for the contact person. authors.public_key_source Can reference a publicly hosted cryptographic \"public key\" from a private-public key-pair. authors.signature The app developer can optionally add a string that is signed by their private key, so that app users could verify the authenticity of the application. version Provides a version number for this application. Version numbers are incredibly important for distributed apps, so use this property wisely. dht This is a placeholder for the configuration options that Holochain will implement, regarding the Distributed Hash Table. It will provide a number of ways that the DHT behaviour can be customized. properties Properties, if used, can be an object which implements numerous app specific configuration values. These can be up to the app developer to define, and, when implemented, will be able to be called using the property function of the Zome API. The minimum recommended values to set when you initialize a new project folder are: name description authors version To edit them, just open app.json in a text editor (preferably one with syntax highlighting for JSON), change the values, and save the file.","title":"Configuring an App"},{"location":"guide/core_api/","text":"Core API \u00b6","title":"(E) Core API"},{"location":"guide/core_api/#core-api","text":"","title":"Core API"},{"location":"guide/creating_versioned_releases/","text":"Creating Versioned Releases \u00b6","title":"(E) Creating Versioned Releases"},{"location":"guide/creating_versioned_releases/#creating-versioned-releases","text":"","title":"Creating Versioned Releases"},{"location":"guide/development_conductor/","text":"Development Conductor \u00b6 The easiest Conductor to run is built right into the development command line tools . It has no required configuration and is launched via the hc run command. Meant primarily for accelerating the development process it is useful for testing APIs or prototyping user interfaces. The hc run command expects to be executed from inside a directory with valid DNA source files: The command is simply: hc run This will start the DNA instance in a Conductor and open, by default, a WebSocket JSON-RPC server on port 8888 . You can find more details on how to use the API in your UI in the JSON-RPC interfaces article . The following are the options for configuring hc run , should you need something besides the defaults. Packaging \u00b6 -b / --package Package your DNA before running it. Recall that to package is to build the yourapp.dna.json file from the source files. hc run always looks for a DNA package file in the root of your DNA folder that should have the same name as the directory itself with suffix: .dna.json , so make sure that one exists there when trying to use it. hc run --package will do this, or run hc package beforehand. example hc run --package Storage \u00b6 --persist Persist source chain and DHT data onto the file system. By default, none of the data being written to the source chain gets persisted beyond the running of the server. This will store data in the same directory as your DNA source code, in a hidden folder called .hc . example hc run --persist Interfaces \u00b6 --interface Select a particular JSON-RPC interface to serve your DNA instance over. The JSON-RPC interface will expose, via a port on your device, a WebSocket or an HTTP server. It can be used to make function calls to the Zomes of a DNA instance. These are covered in depth in the JSON-RPC interfaces article . The default interface is websocket . examples To run it as HTTP, run: hc run --interface http To explicitly run it as WebSockets, run: hc run --interface websocket Port \u00b6 -p / --port Customize the port number that the server runs on. example hc run --port 3400 Networking \u00b6 --networked Select whether the Conductor should network with other nodes that are running instances of the same DNA. By default this does not occur, instead the instance runs in isolation from the network, allowing only the developer to locally access it. This option requires more configuration, which can be read about in the configuring networking article . Stopping the Server \u00b6 Once you are done with the server, to quit type exit then press Enter , or press Ctrl-C .","title":"Development Conductor"},{"location":"guide/development_conductor/#development-conductor","text":"The easiest Conductor to run is built right into the development command line tools . It has no required configuration and is launched via the hc run command. Meant primarily for accelerating the development process it is useful for testing APIs or prototyping user interfaces. The hc run command expects to be executed from inside a directory with valid DNA source files: The command is simply: hc run This will start the DNA instance in a Conductor and open, by default, a WebSocket JSON-RPC server on port 8888 . You can find more details on how to use the API in your UI in the JSON-RPC interfaces article . The following are the options for configuring hc run , should you need something besides the defaults.","title":"Development Conductor"},{"location":"guide/development_conductor/#packaging","text":"-b / --package Package your DNA before running it. Recall that to package is to build the yourapp.dna.json file from the source files. hc run always looks for a DNA package file in the root of your DNA folder that should have the same name as the directory itself with suffix: .dna.json , so make sure that one exists there when trying to use it. hc run --package will do this, or run hc package beforehand. example hc run --package","title":"Packaging"},{"location":"guide/development_conductor/#storage","text":"--persist Persist source chain and DHT data onto the file system. By default, none of the data being written to the source chain gets persisted beyond the running of the server. This will store data in the same directory as your DNA source code, in a hidden folder called .hc . example hc run --persist","title":"Storage"},{"location":"guide/development_conductor/#interfaces","text":"--interface Select a particular JSON-RPC interface to serve your DNA instance over. The JSON-RPC interface will expose, via a port on your device, a WebSocket or an HTTP server. It can be used to make function calls to the Zomes of a DNA instance. These are covered in depth in the JSON-RPC interfaces article . The default interface is websocket . examples To run it as HTTP, run: hc run --interface http To explicitly run it as WebSockets, run: hc run --interface websocket","title":"Interfaces"},{"location":"guide/development_conductor/#port","text":"-p / --port Customize the port number that the server runs on. example hc run --port 3400","title":"Port"},{"location":"guide/development_conductor/#networking","text":"--networked Select whether the Conductor should network with other nodes that are running instances of the same DNA. By default this does not occur, instead the instance runs in isolation from the network, allowing only the developer to locally access it. This option requires more configuration, which can be read about in the configuring networking article .","title":"Networking"},{"location":"guide/development_conductor/#stopping-the-server","text":"Once you are done with the server, to quit type exit then press Enter , or press Ctrl-C .","title":"Stopping the Server"},{"location":"guide/distributed_hash_table/","text":"Distributed Hash Table \u00b6 Local Hash Table \u00b6 implementation details \u00b6 First, read about state actors . The 1:1 API implementation between actors and their inner table is achieved by internally blocking on an ask from riker patterns. https://github.com/riker-rs/riker-patterns The actor ref methods implementing HashTable sends messages to itself. Calling table_actor_ref.commit(entry) looks like this: the actor ref constructs a Protocol::PutPair message including the entry the actor ref calls its own ask method, which builds a future using riker's ask the actor ref blocks on its internal future the referenced actor receives the Commit message and matches/destructures this into the entry the entry is passed to the commit() method of the inner table the actor's inner table, implementing HashTable , does something with commit (e.g. MemTable inserts into a standard Rust, in-memory HashMap ) the return value of the inner table commit is inserted into a CommitResult message the CommitResult message is sent by the actor back to the actor ref's internal future the actor ref stops blocking the CommitResult message is destructured by the actor ref so that the return of commit satisfies the HashTable trait implementation Riker ask returns a future from the futures 0.2.2 crate. table_actor.block_on_ask() calls block_on and unwrap against this ask. Both the block and the unwrap should be handled better in the future.","title":"Distributed Hash Table"},{"location":"guide/distributed_hash_table/#distributed-hash-table","text":"","title":"Distributed Hash Table"},{"location":"guide/distributed_hash_table/#local-hash-table","text":"","title":"Local Hash Table"},{"location":"guide/distributed_hash_table/#implementation-details","text":"First, read about state actors . The 1:1 API implementation between actors and their inner table is achieved by internally blocking on an ask from riker patterns. https://github.com/riker-rs/riker-patterns The actor ref methods implementing HashTable sends messages to itself. Calling table_actor_ref.commit(entry) looks like this: the actor ref constructs a Protocol::PutPair message including the entry the actor ref calls its own ask method, which builds a future using riker's ask the actor ref blocks on its internal future the referenced actor receives the Commit message and matches/destructures this into the entry the entry is passed to the commit() method of the inner table the actor's inner table, implementing HashTable , does something with commit (e.g. MemTable inserts into a standard Rust, in-memory HashMap ) the return value of the inner table commit is inserted into a CommitResult message the CommitResult message is sent by the actor back to the actor ref's internal future the actor ref stops blocking the CommitResult message is destructured by the actor ref so that the return of commit satisfies the HashTable trait implementation Riker ask returns a future from the futures 0.2.2 crate. table_actor.block_on_ask() calls block_on and unwrap against this ask. Both the block and the unwrap should be handled better in the future.","title":"implementation details"},{"location":"guide/dna/","text":"DNA \u00b6","title":"(E) DNA"},{"location":"guide/dna/#dna","text":"","title":"DNA"},{"location":"guide/dpki/","text":"Distributed Public Key Infrastructure (DPKI) \u00b6 Context \u00b6 Like most other distributed systems, Holochain fundamentally relies on public key cryptography. Among other uses, Holochain nodes are identified by their public keys, and thus provenance of messages from nodes can be checked simply by checking a message signature against the node's identifier. This fundamental use requires nodes to keep the private key secret, lest the node's very agency be compromised. Keeping such secrets in the digital age is a non-trivial problem. Additionally, distributed systems imply an abundance of nodes with public keys, many of which may wish to be identified as being under the control of a single actor. Solutions to this need create yet another layer of keys which sign other keys, and the management of all this can become quite complex. Addressing these needs is the function of Public Key Infrastructure, and in our case, because we use the power of Holochain itself to do this, a Distributed Public Key Infrastructure: DPKI. Requirements \u00b6 DPKI needs to fulfill at least the following design requirements: Provide a way to create new keys for nodes. Provide a way to revoke compromised keys and re-issue keys for a node. Provide a way to verify the provenance of keys by grouping them as originating from a single actor. Securely manage the private keys. Reliably distribute and make available information about public keys. In designing a solution for DPKI we recognize that this is a complex and difficult enough problem that any solution will need to evolve, and in fact there will be multiple solutions necessary for different contexts. Thus, we have built into the Holochain conductor a simple interface for the fundamental needed functions, e.g. creating new keys when installing a DNA for the first time, that can then be implemented by specialized DPKI applications. Furthermore we've implemented a reference implementation of a Holochain based DPKI application, which we call DeepKey. DeepKey \u00b6 To deliver on the basics of Distributed Public Key Infrastructure, we need a way to generate keys of various types (revocation, identity, encryption, signing) from seeds, and we need to be able to generate such seeds from primary seeds, so that a human agent can create related \"device agents\" provably under their control. After studying a number of uses cases, including initial sign-up, key revocation, etc, the central insight we came to was the need to create a Hierarchical Deterministic Key generation system, based on a Primary Seed, from which additional seeds can be generated which then are in turn used to actually generate many key-pairs. This allows us, by-convention, to use the first seed generated by the Primary seed as the seed for revocation keys, and subsequent seeds as seeds for keys of separate Holochain devices that can be proven to be under the control of the holder of Primary Seed. DeepKey TODO: merge the various docs we developed to explain DeepKey here. - https://medium.com/holochain/part-2-holochain-holo-accounts-cryptographic-key-management-and-deepkey-bf32ee91af65 - https://hackmd.io/UbfvwQdJRKaAHI9Xa7F3VA?view - https://hackmd.io/8c8rZCyaTTqH_7TIBVtEUQ - https://hackmd.io/oobu0sKMSMadLXza4rHY_g DPKI Implementation Technical Details \u00b6 Keystore \u00b6 For each Holochain DNA instance, the Conductor maintains a Keystore, which holds \"secrets\" (seeds and keys) needed for cryptographic signing and encrypting. Each of the secrets in the Keystore is associated with a string which is a handle needed when using that secret for some cryptographic operation. Our cryptographic implementation is based on libsodium, and the seeds use their notions of context and index for key derivation paths. This implementation allows DNA developers to securely call cryptographic functions from WASM which will be executed in the conductor's secure memory space when actually doing the cryptographic processing. Decrypting a secret from the keystore invokes a passphrase manager service, which is used to collect the passphrase from some end-user. This service is implementation specific. Currently we have implementations for command-line passphrase collection for use in the hc keygen command, and also a command-line implementation within the conductor. Standalone Mode \u00b6 If the Conductor config does not include a DPKI section, then the conductor assumes it's in standalone mode and takes responsibility for adding generating new agent secrets when it receives admin/add_agent requests through the admin interface. This mode is also used by the hc command-line tool. DPKI Mode \u00b6 If the Conductor config specifies a DPKI instance, then the conductor will initially bootstrap the DPKI instance, and also delegate any admin/add_agent requests to the DPKI app for processing. Note that the Conductor does assume that basic agent key will be created by the DPKI app so that it can actually create the agent keystore file on behalf of the DPKI app. The Holochain conductor expects the following exposed functions to exist in any DPKI application as a trait so that it can call them at various times to manage keys and identity. init(params) : Called during bootstrap with initialization parameters retrieved from the DPKI configuration. This function is only called if a prior call to is_initialied() returned false. is_initialized() -> bool : Should return a boolean value if the DPKI DNA has been initialized or not create_agent_key(agent_name) : Called any time the conductor creates a new DNA instance. Should create a keystore record for the instance. TODO: add more functions for the trait.","title":"Key Management (DPKI)"},{"location":"guide/dpki/#distributed-public-key-infrastructure-dpki","text":"","title":"Distributed Public Key Infrastructure (DPKI)"},{"location":"guide/dpki/#context","text":"Like most other distributed systems, Holochain fundamentally relies on public key cryptography. Among other uses, Holochain nodes are identified by their public keys, and thus provenance of messages from nodes can be checked simply by checking a message signature against the node's identifier. This fundamental use requires nodes to keep the private key secret, lest the node's very agency be compromised. Keeping such secrets in the digital age is a non-trivial problem. Additionally, distributed systems imply an abundance of nodes with public keys, many of which may wish to be identified as being under the control of a single actor. Solutions to this need create yet another layer of keys which sign other keys, and the management of all this can become quite complex. Addressing these needs is the function of Public Key Infrastructure, and in our case, because we use the power of Holochain itself to do this, a Distributed Public Key Infrastructure: DPKI.","title":"Context"},{"location":"guide/dpki/#requirements","text":"DPKI needs to fulfill at least the following design requirements: Provide a way to create new keys for nodes. Provide a way to revoke compromised keys and re-issue keys for a node. Provide a way to verify the provenance of keys by grouping them as originating from a single actor. Securely manage the private keys. Reliably distribute and make available information about public keys. In designing a solution for DPKI we recognize that this is a complex and difficult enough problem that any solution will need to evolve, and in fact there will be multiple solutions necessary for different contexts. Thus, we have built into the Holochain conductor a simple interface for the fundamental needed functions, e.g. creating new keys when installing a DNA for the first time, that can then be implemented by specialized DPKI applications. Furthermore we've implemented a reference implementation of a Holochain based DPKI application, which we call DeepKey.","title":"Requirements"},{"location":"guide/dpki/#deepkey","text":"To deliver on the basics of Distributed Public Key Infrastructure, we need a way to generate keys of various types (revocation, identity, encryption, signing) from seeds, and we need to be able to generate such seeds from primary seeds, so that a human agent can create related \"device agents\" provably under their control. After studying a number of uses cases, including initial sign-up, key revocation, etc, the central insight we came to was the need to create a Hierarchical Deterministic Key generation system, based on a Primary Seed, from which additional seeds can be generated which then are in turn used to actually generate many key-pairs. This allows us, by-convention, to use the first seed generated by the Primary seed as the seed for revocation keys, and subsequent seeds as seeds for keys of separate Holochain devices that can be proven to be under the control of the holder of Primary Seed. DeepKey TODO: merge the various docs we developed to explain DeepKey here. - https://medium.com/holochain/part-2-holochain-holo-accounts-cryptographic-key-management-and-deepkey-bf32ee91af65 - https://hackmd.io/UbfvwQdJRKaAHI9Xa7F3VA?view - https://hackmd.io/8c8rZCyaTTqH_7TIBVtEUQ - https://hackmd.io/oobu0sKMSMadLXza4rHY_g","title":"DeepKey"},{"location":"guide/dpki/#dpki-implementation-technical-details","text":"","title":"DPKI Implementation Technical Details"},{"location":"guide/dpki/#keystore","text":"For each Holochain DNA instance, the Conductor maintains a Keystore, which holds \"secrets\" (seeds and keys) needed for cryptographic signing and encrypting. Each of the secrets in the Keystore is associated with a string which is a handle needed when using that secret for some cryptographic operation. Our cryptographic implementation is based on libsodium, and the seeds use their notions of context and index for key derivation paths. This implementation allows DNA developers to securely call cryptographic functions from WASM which will be executed in the conductor's secure memory space when actually doing the cryptographic processing. Decrypting a secret from the keystore invokes a passphrase manager service, which is used to collect the passphrase from some end-user. This service is implementation specific. Currently we have implementations for command-line passphrase collection for use in the hc keygen command, and also a command-line implementation within the conductor.","title":"Keystore"},{"location":"guide/dpki/#standalone-mode","text":"If the Conductor config does not include a DPKI section, then the conductor assumes it's in standalone mode and takes responsibility for adding generating new agent secrets when it receives admin/add_agent requests through the admin interface. This mode is also used by the hc command-line tool.","title":"Standalone Mode"},{"location":"guide/dpki/#dpki-mode","text":"If the Conductor config specifies a DPKI instance, then the conductor will initially bootstrap the DPKI instance, and also delegate any admin/add_agent requests to the DPKI app for processing. Note that the Conductor does assume that basic agent key will be created by the DPKI app so that it can actually create the agent keystore file on behalf of the DPKI app. The Holochain conductor expects the following exposed functions to exist in any DPKI application as a trait so that it can call them at various times to manage keys and identity. init(params) : Called during bootstrap with initialization parameters retrieved from the DPKI configuration. This function is only called if a prior call to is_initialied() returned false. is_initialized() -> bool : Should return a boolean value if the DPKI DNA has been initialized or not create_agent_key(agent_name) : Called any time the conductor creates a new DNA instance. Should create a keystore record for the instance. TODO: add more functions for the trait.","title":"DPKI Mode"},{"location":"guide/embedding_holochain/","text":"Embedding Holochain \u00b6 Core API is a library for embedding a Holochain instance (an hApp) in your own code. So this is for the use case of writing your own software that runs hApps. A common use case might be \"glueing\" several hApps together or adding centralized services, like file storage, on top of a hApp.","title":"Embedding Holochain"},{"location":"guide/embedding_holochain/#embedding-holochain","text":"Core API is a library for embedding a Holochain instance (an hApp) in your own code. So this is for the use case of writing your own software that runs hApps. A common use case might be \"glueing\" several hApps together or adding centralized services, like file storage, on top of a hApp.","title":"Embedding Holochain"},{"location":"guide/extending_holochain/","text":"Extending Holochain \u00b6","title":"Extending Holochain"},{"location":"guide/extending_holochain/#extending-holochain","text":"","title":"Extending Holochain"},{"location":"guide/faq/","text":"Frequently Asked Questions \u00b6 How is Holochain different from blockchain? Why do you call it \"Holochain\"? How is Holochain different from a DHT (Distributed Hash Table)? What kind of projects is Holochain good for? What is Holochain not good for? What is Holochain's consensus algorithm? Can you run a cryptocurrency on Holochain? How is Holochain different from _ ___? What language is Holochain written in? What languages can I use to make Holochain apps? Is Holochain open source? 10 How is Holochain more environmentally ethical than blockchain? How are data validated on Holochain? What happens to data when a node leaves the network? Should I build my coin/token on Holochain? What does \u201cagent-centric\u201d mean? How is this different from \u201cdata-centric\u201d? What is the TPS (Transactions Per Second) on Holochain? How is Holochain different from blockchain? \u00b6 Holochain and blockchain are built for fundamentally different use cases. Blockchain is relatively good for systems where it\u2019s absolutely necessary to maintain global consensus. Holochain is much better than blockchain at anything that requires less than universal consensus (most things): It\u2019s faster, more efficient, more scalable, adaptable, and extendable. Long before blockchains were hash chains and hash trees . These structures can be used to ensure tamper-proof data integrity as progressive versions or additions to data are made. These kinds of hashes are often used as reference points to ensure data hasn't been messed with\u2014like making sure you're getting the program you meant to download, not some virus in its place. Instead of trying to manage global consensus for every change to a huge blockchain ledger, every participant has their own signed hash chain ( countersigned for transactions involving others). After data is signed to local chains, it is shared to a DHT where every node runs the same validation rules (like blockchain nodes all run the same validation rules . If someone breaks those rules, the DHT rejects their data\u2014their chain has forked away from the holochain. The initial Bitcoin white paper introduced a blockchain as an architecture for decentralized production of a chain of digital currency transactions. This solved two problems (time/sequence of transactions, and randomizing who writes to the chain) with one main innovation of bundling transactions into blocks which somebody wins the prize of being able to commit to the chain if they solve a busywork problem faster than others. Now Bitcoin and blockchain have pervaded people's consciousness and many perceive it as a solution for all sorts of decentralized applications. However, when the problems are framed slightly differently, there are much more efficient and elegant solutions (like holochains) which don't have the processing bottlenecks of global consensus, storage requirements of everyone having a FULL copy of all the data, or wasting so much electricity on busywork. Why do you call it \"Holochain\"? \u00b6 A variety of reasons: it's a composed whole of other technologies, it's structurally holographic, and it empowers holistic patterns. A unified cryptographic whole \u00b6 Holochain is made from multiple cryptographic technologies composed into a new whole. Hashchains: Hashchains provide immutable data integrity and definitive time sequence from the vantage point of each node. Technically, we're using hash trees\u2014blockchains do too, but they're not called blocktrees, so we're not calling these holotrees. Cryptographic signing of chains, messages, and validation confirmations maintain authorship, provenance, and accountability. Countersigning of transactions/interactions between multiple parties provide non-repudiation and \"locking\" of chains. DHT (Distributed Hash Table) leverages cryptographic hashes for content addressable storage, while randomizing of interactions by hashing into neighborhoods to impede collusion, and processing validation #1 and #2 to store data on the DHT. Holo graphic storage \u00b6 Every node has a resilient sample of the whole. Like cutting a hologram, if you were to cut a Holochain network in half (make it so half the nodes were isolated from the other half), you would have two whole, functioning systems, not two partial, broken systems. This seems to be the strategy used to create resilience in natural systems. For example, where is your DNA stored? Every cell carries its own copy, with different functions expressed based on the role of that cell. Where is the English language stored? Every speaker carries it. People have different areas of expertise, or exposure to different slang or specialized vocabularies. Nobody has a complete copy, nor is anyone's version exactly the same as anyone else, If you disappeared half of the English speakers, it would not degrade the language much. If you keep cutting a hologram smaller and smaller eventually the image degrades enough to stop being recognizable, and depending on the resiliency rules for DHT neighborhoods, holochains would likely share a similar fate. Although, if the process of killing off the nodes was not instantaneous, the network may be able to keep reshuffling data per redundancy requirements to keep it alive. Hol archy \u00b6 Holochains are composable with each other into new levels of unification. In other words, Holochains can build on decentralized capacities provided by other Holochains, making new holistic patterns possible. Like bodies build new unity on holographic storage patterns that cells use for DNA, and a society build new unity on the holographic storage patterns of language, and so on. How is Holochain different from a DHT (Distributed Hash Table)? \u00b6 DHTs enable key/value pair storage and retrieval across many machines. The only validation rules they have is the hash of the data itself to confirm what you're getting is probably what you intended to get. They have no other means to confirm authenticity, provenance, timelines, or integrity of data sources. In fact, since many DHTs are used for illegal file sharing (Napster, Bittorrent, Sharezaa, etc.), they are designed to protect anonymity of uploaders so they won't get in trouble. File sharing DHTs frequently serve virus infected files, planted by uploaders trying to infect digital pirates. There's no accountability for actions or reliable way to ensure bad data doesn't spread. By embedding validation rules as a condition for the propagation of data, our DHT keeps its data bound to signed source chains. This can provide similar consistency and rule enforcement as blockchain ledgers asynchronously so bottlenecks of immediate consensus become a thing of the past. The DHT leverages the signed source chains to ensure tamper-proof immutability of data, as well as cryptographic signatures to verify its origins and provenance. The Holochain DHT also emulates aspects of a graph database by enabling people to connect links to other hashes in the DHT tagged with semantic markers. This helps solve the problem of finding the hashes that you want to retrieve from the DHT. For example, if I have the hash of your user identity, I could query it for links to blogs you've published to a holochain so that I can find them without knowing either the hash or the content. This is part of how we eliminate the need for tracking nodes that many DHTs rely on. What kind of projects is Holochain good for? \u00b6 Sharing collaborative data without centralized control. Imagine a completely decentralized Wikipedia, DNS without root servers, or the ability to have fast reliable queries on a fully distributed PKI, etc. Social Networks, Social Media & VRM: You want to run a social network without a company like Facebook in the middle. You want to share, post, publish, or tweet to shared space, while automatically keeping a copy of these things on your own device. Supply Chains & Open Value Networks: You want to have information that crosses the boundaries of companies, organizations, countries, which is collaboratively shared and managed, but not under the central control of any one of those organizations. Cooperatives and New Commons: You want to create something which is truly held collectively and not by any particular individual. This is especially good for digital assets. P2P Platforms: Peer-to-Peer applications where every person has similar capabilities, access, responsibilities, and value is produced collectively. Collective Intelligence: Governance, decision-making frameworks, feedback systems, ratings, currencies, annotations, or work flow systems. Collaborative Applications: Chats, Discussion Boards, Scheduling Apps, Wikis, Documentation, etc. Reputational or Mutual Credit Cryptocurrencies: Currencies where issuance can be accounted for by actions of peers (like ratings), or through double-entry accounting are well-suited for holochains. Fiat currencies where tokens are thought to exist independent of accountability by agents are more challenging to implement on holochains. What is Holochain not good for? \u00b6 You probably should not use Holochain for: Just yourself: You generally don't need distributed tools to just run something for yourself. The exception would be if you want to run a holochain to synchronize certain data across a bunch of your devices (phone, laptop, desktop, cloud server, etc.) Anonymous, secret, or private data: Not only do we need to do a security audit of our encryption and permissions, but you're publishing to a shared DHT space, so unless you really know what you're doing, you should not assume data is private. Some time in the future, I'm sure some applications will add an anonymization layer (like TOR), but that is not native. Large files: Think of holochains more like a database than a file system. Nobody wants to be forced to load and host your big files on their devices just because they are in the neighborhood of its hash. Use something like IPFS if you want a decentralized file system. Data positivist-oriented apps: If you have built all of your application logic around the idea that data exists as an absolute truth, not as an assertion by an agent at a time, then you would need to rethink your whole approach before putting it in a Holochain app. This is why most existing cryptocurrencies would need significant refactoring to move from blockchain to Holochain, since they are organized around managing the existence of cryptographic tokens. What is Holochain's consensus algorithm? \u00b6 Holochains don't manage consensus, at least not about some absolute perspective on data or sequence of events. They manage distributed data integrity. Holochains do rely on consensus about the validation rules (DNA) which define that integrity, but so does every blockchain or blockchain alternative (e.g. Bitcoin Core). If you have different validation rules, you're not on the same chain. These validation rules establish the \"data physics,\" and then applications are built on that foundation. In making Holochain, our goal is to keep it \"as simple as possible, but no simpler\" for providing data integrity for fully distributed applications. As we understand it, information integrity does not require consensus about an absolute order of events. You know how we know? Because the real world works this way\u2014meaning, the physically distributed systems outside of computers. Atoms, molecules, cells, bodies each maintain the integrity of their individual and collective state just fine without consensus on a global ledger. Not only is there no consensus about an absolute order of events, but if you understand the General Theory of Relativity, then you'll understand there is in fact no real sequence of events, only sequences relative to a particular vantage point. That's how holochains are implemented. Each source chain for each person/agent/participant in a Holochain preserves the immutable data integrity and order of events of that agent's actions from their vantage point. As data is published from a source chain to the validating DHT, then other agents sign their validation, per the shared \"physics\" encoded into the DNA of that Holochain. The minor exception to the singular vantage point of each chain, is the case when a multi-party transaction is signed to each party's chain. That is an act of consensus\u2014but consensus on a very small scale\u2014just between the parties involved in the transaction. Each party signs the exact same transaction with links to each of their previous chain entries. Luckily, it's pretty easy to reach consensus between 2 or 3 parties. In fact, that is already why they're doing a transaction together, because they all agree to it. Holochains do sign every change of data and timestamp (without a universal time synchronization solution). This provides ample foundation for most applications which need solid data integrity for shared data in a fully distributed multi-agent system. Surely, there will be people who will build consensus algorithms on top of that foundation (maybe like rounds, witnesses, supermajorities of Swirlds ). However, if your system is designed around data having one absolute true state, not one which is dynamic and varied based on vantage point, we would suggest you rethink your design. So far, for every problem space where people thought they needed an absolute sequence of events or global consensus, we have been able to map an alternate approach without those requirements. Also, we already know this is how the world outside of computers works, so to design your system to require (or construct) an artificial reality is probably setting yourself up for failure, or at the very least for massive amounts of unnecessary computation, communication, and fragility within your system. How is Holochain more environmentally ethical than blockchain? \u00b6 Holochain removes the need for global consensus, and with it the expenditure of massive amounts of electricity to synchronize millions of nodes about data that aren't relevant to them. There are two reasons Holochain is vastly more efficient than blockchain and more ethical in a green sense: It eliminates the need for all nodes to be synchronized with each other in global consensus. Sharding is usually enabled on Holochain. This means that when two nodes make a transaction, each node saves a countersigned record of that transaction. Additionally, the transaction is published to the Distributed Hash Table (sent to and saved by some unpredictably random nodes that can be looked up later for retrieval). Sharding is configurable by app, and in some cases it's a good idea to turn it off. For example, imagine a distributed Slack-like team messaging app. With only 40-50 members, full synchronization would be worth the extra bandwidth requirement for the benefit of offline messages and reduced load times. But for most applications, global synchronization isn't really needed and sharding is kept on. Because of DHTs, and the sharding they enable, Holochain actually doesn't rely on the transfer of large amounts of redundant information, and uses vastly less bandwidth than blockchain. There's no mining on Holochain. Blockchain's proof-of-work system provides a hefty incentive for thousands of people to spend the processing power of their CPUs and GPUs using up huge amounts of electricity on solving a meaningless cryptographic puzzle. Holochain doesn't have mining. How is Holochain different from _ ___? \u00b6 TODO: Update with reference to Rust project. Please see the Comparisons page . What language is Holochain written in? What languages can I use to make Holochain apps? \u00b6 Holochain is written in the Rust programming language. At a low level, Holochain runs WebAssembly code, but for all practical purposes developers will write applications in a language that compiles to WebAssembly such as Rust, C, C++, Go, etc. For now, only Rust has tier 1 support for writing apps, because it has a \"Holochain Development Kit\" library which makes writing WebAssembly apps easy. Is Holochain open source? \u00b6 Yes, it has an open source license . Can you run a cryptocurrency on Holochain? \u00b6 Theoretically, yes\u2014but for the moment, we'd discourage it. If you don't know how to issue currencies through mutual credit, or how to account for them through double entry accounting, then you probably shouldn't build one on Holochain. If you do understand those key principles, than it is not very difficult to build a cryptocurrency for which Holochain provides ample accounting and data integrity. However, you probably shouldn't try to do it in the way everyone is used to building cryptocurrencies on a global ledger of cryptographic tokens. Determining the status of tokens/coins is what create the need for global consensus (about the existence/status/validity of the token or coin). However, there are other approaches to making currencies which, for example, involve issuance via mutual credit instead of issuance by fiat. Unfortunately, this is a hotly contested topic by many who often don't have a deep understanding of currency design nor cryptography, so we're not going to go too deep in this FAQ. We intend to publish a white paper on this topic soon, as well as launch some currencies built this way. How are data validated on Holochain? \u00b6 On Holochain, each node that receives a record of a transaction validates it against the shared application rules and gossips it to their peers. If the rules are broken, that transaction is rejected by the validator. There is no overall, global \"correctness\" (or consensus) built in to Holochain. Instead, each node that receives a record of a transaction validates it against the shared application rules and gossips it to their peers. If the rules are broken, that transaction is rejected by the validator. If foul play is detected on a node's part (the node is either propagating or validating bad data) that node is blocked and a warning is sent to others. Here's an infographic describing this process. In summary, instead of a global consensus system, Holochain uses an accountability-based system with data validation by peers. Applying this to the example of 'Ourbnb', an imaginary distributed version ofAirbnb: The Ourbnb Holochain app would certainly be written with a rule, \"don't rent your apartment to two parties at the same time.\" So the moment a user rents to two parties at the same time, nodes receiving that datum on the DHT attempt to validate it against the app rules, detect a collision, and reject it. Holochain's gossip protocol is designed to operate at a rate at which collisions will be detected nearly immediately by gossiping peers. And since Holochain doesn't have a coin built into it, it incentivizes users to cooperate and co-create. As a user, you don't need to trust the provider of the application you're using, only agree with the shared protocols that make up the application itself. Aside from being responsible for the maintenance and security of apps they provide, application providers on Holochain are not like traditional application providers today (think Facebook, Twitter, etc.). They don't host your data because your data is stored by you and a random subset of the users of the application. What happens to data when a node leaves the network? \u00b6 The DHT of a Holochain app makes sure that there are always enough nodes on the network that hold a given datum. When people running Holochain apps turn off their device, they leave the network. What happens to their data and the data of other people they were storing? There are always enough nodes that hold a given piece of data in the network so as to prevent data loss when nodes leave. The DHT and Holochain gossip protocol are designed this way. Also, the redundancy factor of data on a given DHT is configurable so it can be fine-tuned for any purpose. For example, a chat app for a small team might set a redundancy factor of 100% in order to prevent long loading times, while an app with thousands of users might have a very small redundancy factor. Should I build my coin/token on Holochain? \u00b6 Since it's agent-centric instead of data-centric like traditional blockchains, Holochain isn't the best platform on which to build a token or coin. The idea of tokens or coins is a direct representation of a system being data-centric. While theoretically it would be possible to create a token on Holochain, it would be taking a step back instead of a step forward. The more exciting possibility is creating mutual credit currencies on Holochain. These are agent-centric currencies that are designed to facilitate active exchange of value and flourishing ecosystems instead of hoarding. What does \u201cagent-centric\u201d mean? How is this different from \u201cdata-centric?\u201d \u00b6 Agent-centric systems view data not as an object, but as a shared experience. Traditional blockchains are data-centric: they rely on and are built around the concept that data is a thing\u2014an object. Holochain transitions to agent-centricism: the idea that data is shared experiences seen from many points of view. It's not a thing. It's a collection of shared, relative experiences. Einstein discovered this about the physical world a hundred years ago\u2014Relativity. So why are modern blockchains that are supposedly \"cutting edge\" still falling back on this antiquated idea that data is an object, and for two agents to have different views of one piece of data is wrong? Holochain is deeply agent-centric. Using tech that embodies this mindset enables vastly richer interactions and collaboration to happen through its technology while at the same time being thousands of times more efficient. What is the TPS (Transactions Per Second) on Holochain? \u00b6 Holochain doesn't have a set TPS (transactions per second) like other blockchain-based or blockchain-derived projects might because there's central point through which all transactions must pass. Instead, Holochain is a generalized protocol for distributed computing. It's common to ask a blockchain project, \"How much can your technology handle? What's its TPS?\" This is because nearly all of these projects are built around the limiting idea of a global ledger. But you are not asking, how many posts per second Facebook can do. Why? Because there is no technical problem, adding more servers to Facebook's data center (only maybe monetary problems). You are not asking how many emails per second the internet can handle, because there is no single bottleneck for email-sending, like there would be with a centralized approach. Why are we seeing a transaction limit with blockchain networks? Because blockchain in a strange way marries a decentralized p2p network of nodes with the logical notion of one absolute truth, i.e. the blockchain being one big decentralized database of transactions. It tries to maintain this way of thinking about apps that we are used to from centralized servers. It forces every node into the same \"consensus\". That is implemented by having everybody share and validate everything. That does work, and maybe there are few usecases (like a global naming system maybe?) where it might be advantageous.. but applying that for everything is nonsensical. Holochain is not forcing such a model. Instead it allows for building applications that are like email. The application is rather like a protocol, or grammar, or (I prefer this language) like a dance. If you know the dance (If you have a copy of the validation rules of the app) you can tell who else is dancing that dance and who is not. The difference between Holochain and something like email is that (similarly to blockhain) Holochain is applying 1. cryptographic signatures and 2. tamper proof hash-chains (hence Holo chain ) so that you can build a distributed system you can trust in. You know it is impossible (I'd rather say: very very hard) to game somebody. This so far was only possible by having trusted authorities like banks or Facebook. So, Holochain as an app framework does not pose any limit of transactions per second because there is no place where all transactions have to go through. It is like asking, \"how many words can humanity speak per second?\" Well, with every human being born, that number increases. Same for Holochain.","title":"FAQ"},{"location":"guide/faq/#frequently-asked-questions","text":"How is Holochain different from blockchain? Why do you call it \"Holochain\"? How is Holochain different from a DHT (Distributed Hash Table)? What kind of projects is Holochain good for? What is Holochain not good for? What is Holochain's consensus algorithm? Can you run a cryptocurrency on Holochain? How is Holochain different from _ ___? What language is Holochain written in? What languages can I use to make Holochain apps? Is Holochain open source? 10 How is Holochain more environmentally ethical than blockchain? How are data validated on Holochain? What happens to data when a node leaves the network? Should I build my coin/token on Holochain? What does \u201cagent-centric\u201d mean? How is this different from \u201cdata-centric\u201d? What is the TPS (Transactions Per Second) on Holochain?","title":"Frequently Asked Questions"},{"location":"guide/faq/#how-is-holochain-different-from-blockchain","text":"Holochain and blockchain are built for fundamentally different use cases. Blockchain is relatively good for systems where it\u2019s absolutely necessary to maintain global consensus. Holochain is much better than blockchain at anything that requires less than universal consensus (most things): It\u2019s faster, more efficient, more scalable, adaptable, and extendable. Long before blockchains were hash chains and hash trees . These structures can be used to ensure tamper-proof data integrity as progressive versions or additions to data are made. These kinds of hashes are often used as reference points to ensure data hasn't been messed with\u2014like making sure you're getting the program you meant to download, not some virus in its place. Instead of trying to manage global consensus for every change to a huge blockchain ledger, every participant has their own signed hash chain ( countersigned for transactions involving others). After data is signed to local chains, it is shared to a DHT where every node runs the same validation rules (like blockchain nodes all run the same validation rules . If someone breaks those rules, the DHT rejects their data\u2014their chain has forked away from the holochain. The initial Bitcoin white paper introduced a blockchain as an architecture for decentralized production of a chain of digital currency transactions. This solved two problems (time/sequence of transactions, and randomizing who writes to the chain) with one main innovation of bundling transactions into blocks which somebody wins the prize of being able to commit to the chain if they solve a busywork problem faster than others. Now Bitcoin and blockchain have pervaded people's consciousness and many perceive it as a solution for all sorts of decentralized applications. However, when the problems are framed slightly differently, there are much more efficient and elegant solutions (like holochains) which don't have the processing bottlenecks of global consensus, storage requirements of everyone having a FULL copy of all the data, or wasting so much electricity on busywork.","title":"How is Holochain different from blockchain?"},{"location":"guide/faq/#why-do-you-call-it-holochain","text":"A variety of reasons: it's a composed whole of other technologies, it's structurally holographic, and it empowers holistic patterns.","title":"Why do you call it \"Holochain\"?"},{"location":"guide/faq/#a-unified-cryptographic-whole","text":"Holochain is made from multiple cryptographic technologies composed into a new whole. Hashchains: Hashchains provide immutable data integrity and definitive time sequence from the vantage point of each node. Technically, we're using hash trees\u2014blockchains do too, but they're not called blocktrees, so we're not calling these holotrees. Cryptographic signing of chains, messages, and validation confirmations maintain authorship, provenance, and accountability. Countersigning of transactions/interactions between multiple parties provide non-repudiation and \"locking\" of chains. DHT (Distributed Hash Table) leverages cryptographic hashes for content addressable storage, while randomizing of interactions by hashing into neighborhoods to impede collusion, and processing validation #1 and #2 to store data on the DHT.","title":"A unified cryptographic whole"},{"location":"guide/faq/#holographic-storage","text":"Every node has a resilient sample of the whole. Like cutting a hologram, if you were to cut a Holochain network in half (make it so half the nodes were isolated from the other half), you would have two whole, functioning systems, not two partial, broken systems. This seems to be the strategy used to create resilience in natural systems. For example, where is your DNA stored? Every cell carries its own copy, with different functions expressed based on the role of that cell. Where is the English language stored? Every speaker carries it. People have different areas of expertise, or exposure to different slang or specialized vocabularies. Nobody has a complete copy, nor is anyone's version exactly the same as anyone else, If you disappeared half of the English speakers, it would not degrade the language much. If you keep cutting a hologram smaller and smaller eventually the image degrades enough to stop being recognizable, and depending on the resiliency rules for DHT neighborhoods, holochains would likely share a similar fate. Although, if the process of killing off the nodes was not instantaneous, the network may be able to keep reshuffling data per redundancy requirements to keep it alive.","title":"Holographic storage"},{"location":"guide/faq/#holarchy","text":"Holochains are composable with each other into new levels of unification. In other words, Holochains can build on decentralized capacities provided by other Holochains, making new holistic patterns possible. Like bodies build new unity on holographic storage patterns that cells use for DNA, and a society build new unity on the holographic storage patterns of language, and so on.","title":"Holarchy"},{"location":"guide/faq/#how-is-holochain-different-from-a-dht-distributed-hash-table","text":"DHTs enable key/value pair storage and retrieval across many machines. The only validation rules they have is the hash of the data itself to confirm what you're getting is probably what you intended to get. They have no other means to confirm authenticity, provenance, timelines, or integrity of data sources. In fact, since many DHTs are used for illegal file sharing (Napster, Bittorrent, Sharezaa, etc.), they are designed to protect anonymity of uploaders so they won't get in trouble. File sharing DHTs frequently serve virus infected files, planted by uploaders trying to infect digital pirates. There's no accountability for actions or reliable way to ensure bad data doesn't spread. By embedding validation rules as a condition for the propagation of data, our DHT keeps its data bound to signed source chains. This can provide similar consistency and rule enforcement as blockchain ledgers asynchronously so bottlenecks of immediate consensus become a thing of the past. The DHT leverages the signed source chains to ensure tamper-proof immutability of data, as well as cryptographic signatures to verify its origins and provenance. The Holochain DHT also emulates aspects of a graph database by enabling people to connect links to other hashes in the DHT tagged with semantic markers. This helps solve the problem of finding the hashes that you want to retrieve from the DHT. For example, if I have the hash of your user identity, I could query it for links to blogs you've published to a holochain so that I can find them without knowing either the hash or the content. This is part of how we eliminate the need for tracking nodes that many DHTs rely on.","title":"How is Holochain different from a DHT (Distributed Hash Table)?"},{"location":"guide/faq/#what-kind-of-projects-is-holochain-good-for","text":"Sharing collaborative data without centralized control. Imagine a completely decentralized Wikipedia, DNS without root servers, or the ability to have fast reliable queries on a fully distributed PKI, etc. Social Networks, Social Media & VRM: You want to run a social network without a company like Facebook in the middle. You want to share, post, publish, or tweet to shared space, while automatically keeping a copy of these things on your own device. Supply Chains & Open Value Networks: You want to have information that crosses the boundaries of companies, organizations, countries, which is collaboratively shared and managed, but not under the central control of any one of those organizations. Cooperatives and New Commons: You want to create something which is truly held collectively and not by any particular individual. This is especially good for digital assets. P2P Platforms: Peer-to-Peer applications where every person has similar capabilities, access, responsibilities, and value is produced collectively. Collective Intelligence: Governance, decision-making frameworks, feedback systems, ratings, currencies, annotations, or work flow systems. Collaborative Applications: Chats, Discussion Boards, Scheduling Apps, Wikis, Documentation, etc. Reputational or Mutual Credit Cryptocurrencies: Currencies where issuance can be accounted for by actions of peers (like ratings), or through double-entry accounting are well-suited for holochains. Fiat currencies where tokens are thought to exist independent of accountability by agents are more challenging to implement on holochains.","title":"What kind of projects is Holochain good for?"},{"location":"guide/faq/#what-is-holochain-not-good-for","text":"You probably should not use Holochain for: Just yourself: You generally don't need distributed tools to just run something for yourself. The exception would be if you want to run a holochain to synchronize certain data across a bunch of your devices (phone, laptop, desktop, cloud server, etc.) Anonymous, secret, or private data: Not only do we need to do a security audit of our encryption and permissions, but you're publishing to a shared DHT space, so unless you really know what you're doing, you should not assume data is private. Some time in the future, I'm sure some applications will add an anonymization layer (like TOR), but that is not native. Large files: Think of holochains more like a database than a file system. Nobody wants to be forced to load and host your big files on their devices just because they are in the neighborhood of its hash. Use something like IPFS if you want a decentralized file system. Data positivist-oriented apps: If you have built all of your application logic around the idea that data exists as an absolute truth, not as an assertion by an agent at a time, then you would need to rethink your whole approach before putting it in a Holochain app. This is why most existing cryptocurrencies would need significant refactoring to move from blockchain to Holochain, since they are organized around managing the existence of cryptographic tokens.","title":"What is Holochain not good for?"},{"location":"guide/faq/#what-is-holochains-consensus-algorithm","text":"Holochains don't manage consensus, at least not about some absolute perspective on data or sequence of events. They manage distributed data integrity. Holochains do rely on consensus about the validation rules (DNA) which define that integrity, but so does every blockchain or blockchain alternative (e.g. Bitcoin Core). If you have different validation rules, you're not on the same chain. These validation rules establish the \"data physics,\" and then applications are built on that foundation. In making Holochain, our goal is to keep it \"as simple as possible, but no simpler\" for providing data integrity for fully distributed applications. As we understand it, information integrity does not require consensus about an absolute order of events. You know how we know? Because the real world works this way\u2014meaning, the physically distributed systems outside of computers. Atoms, molecules, cells, bodies each maintain the integrity of their individual and collective state just fine without consensus on a global ledger. Not only is there no consensus about an absolute order of events, but if you understand the General Theory of Relativity, then you'll understand there is in fact no real sequence of events, only sequences relative to a particular vantage point. That's how holochains are implemented. Each source chain for each person/agent/participant in a Holochain preserves the immutable data integrity and order of events of that agent's actions from their vantage point. As data is published from a source chain to the validating DHT, then other agents sign their validation, per the shared \"physics\" encoded into the DNA of that Holochain. The minor exception to the singular vantage point of each chain, is the case when a multi-party transaction is signed to each party's chain. That is an act of consensus\u2014but consensus on a very small scale\u2014just between the parties involved in the transaction. Each party signs the exact same transaction with links to each of their previous chain entries. Luckily, it's pretty easy to reach consensus between 2 or 3 parties. In fact, that is already why they're doing a transaction together, because they all agree to it. Holochains do sign every change of data and timestamp (without a universal time synchronization solution). This provides ample foundation for most applications which need solid data integrity for shared data in a fully distributed multi-agent system. Surely, there will be people who will build consensus algorithms on top of that foundation (maybe like rounds, witnesses, supermajorities of Swirlds ). However, if your system is designed around data having one absolute true state, not one which is dynamic and varied based on vantage point, we would suggest you rethink your design. So far, for every problem space where people thought they needed an absolute sequence of events or global consensus, we have been able to map an alternate approach without those requirements. Also, we already know this is how the world outside of computers works, so to design your system to require (or construct) an artificial reality is probably setting yourself up for failure, or at the very least for massive amounts of unnecessary computation, communication, and fragility within your system.","title":"What is Holochain's consensus algorithm?"},{"location":"guide/faq/#how-is-holochain-more-environmentally-ethical-than-blockchain","text":"Holochain removes the need for global consensus, and with it the expenditure of massive amounts of electricity to synchronize millions of nodes about data that aren't relevant to them. There are two reasons Holochain is vastly more efficient than blockchain and more ethical in a green sense: It eliminates the need for all nodes to be synchronized with each other in global consensus. Sharding is usually enabled on Holochain. This means that when two nodes make a transaction, each node saves a countersigned record of that transaction. Additionally, the transaction is published to the Distributed Hash Table (sent to and saved by some unpredictably random nodes that can be looked up later for retrieval). Sharding is configurable by app, and in some cases it's a good idea to turn it off. For example, imagine a distributed Slack-like team messaging app. With only 40-50 members, full synchronization would be worth the extra bandwidth requirement for the benefit of offline messages and reduced load times. But for most applications, global synchronization isn't really needed and sharding is kept on. Because of DHTs, and the sharding they enable, Holochain actually doesn't rely on the transfer of large amounts of redundant information, and uses vastly less bandwidth than blockchain. There's no mining on Holochain. Blockchain's proof-of-work system provides a hefty incentive for thousands of people to spend the processing power of their CPUs and GPUs using up huge amounts of electricity on solving a meaningless cryptographic puzzle. Holochain doesn't have mining.","title":"How is Holochain more environmentally ethical than blockchain?"},{"location":"guide/faq/#how-is-holochain-different-from-____","text":"TODO: Update with reference to Rust project. Please see the Comparisons page .","title":"How is Holochain different from ____?"},{"location":"guide/faq/#what-language-is-holochain-written-in-what-languages-can-i-use-to-make-holochain-apps","text":"Holochain is written in the Rust programming language. At a low level, Holochain runs WebAssembly code, but for all practical purposes developers will write applications in a language that compiles to WebAssembly such as Rust, C, C++, Go, etc. For now, only Rust has tier 1 support for writing apps, because it has a \"Holochain Development Kit\" library which makes writing WebAssembly apps easy.","title":"What language is Holochain written in? What languages can I use to make Holochain apps?"},{"location":"guide/faq/#is-holochain-open-source","text":"Yes, it has an open source license .","title":"Is Holochain open source?"},{"location":"guide/faq/#can-you-run-a-cryptocurrency-on-holochain","text":"Theoretically, yes\u2014but for the moment, we'd discourage it. If you don't know how to issue currencies through mutual credit, or how to account for them through double entry accounting, then you probably shouldn't build one on Holochain. If you do understand those key principles, than it is not very difficult to build a cryptocurrency for which Holochain provides ample accounting and data integrity. However, you probably shouldn't try to do it in the way everyone is used to building cryptocurrencies on a global ledger of cryptographic tokens. Determining the status of tokens/coins is what create the need for global consensus (about the existence/status/validity of the token or coin). However, there are other approaches to making currencies which, for example, involve issuance via mutual credit instead of issuance by fiat. Unfortunately, this is a hotly contested topic by many who often don't have a deep understanding of currency design nor cryptography, so we're not going to go too deep in this FAQ. We intend to publish a white paper on this topic soon, as well as launch some currencies built this way.","title":"Can you run a cryptocurrency on Holochain?"},{"location":"guide/faq/#how-are-data-validated-on-holochain","text":"On Holochain, each node that receives a record of a transaction validates it against the shared application rules and gossips it to their peers. If the rules are broken, that transaction is rejected by the validator. There is no overall, global \"correctness\" (or consensus) built in to Holochain. Instead, each node that receives a record of a transaction validates it against the shared application rules and gossips it to their peers. If the rules are broken, that transaction is rejected by the validator. If foul play is detected on a node's part (the node is either propagating or validating bad data) that node is blocked and a warning is sent to others. Here's an infographic describing this process. In summary, instead of a global consensus system, Holochain uses an accountability-based system with data validation by peers. Applying this to the example of 'Ourbnb', an imaginary distributed version ofAirbnb: The Ourbnb Holochain app would certainly be written with a rule, \"don't rent your apartment to two parties at the same time.\" So the moment a user rents to two parties at the same time, nodes receiving that datum on the DHT attempt to validate it against the app rules, detect a collision, and reject it. Holochain's gossip protocol is designed to operate at a rate at which collisions will be detected nearly immediately by gossiping peers. And since Holochain doesn't have a coin built into it, it incentivizes users to cooperate and co-create. As a user, you don't need to trust the provider of the application you're using, only agree with the shared protocols that make up the application itself. Aside from being responsible for the maintenance and security of apps they provide, application providers on Holochain are not like traditional application providers today (think Facebook, Twitter, etc.). They don't host your data because your data is stored by you and a random subset of the users of the application.","title":"How are data validated on Holochain?"},{"location":"guide/faq/#what-happens-to-data-when-a-node-leaves-the-network","text":"The DHT of a Holochain app makes sure that there are always enough nodes on the network that hold a given datum. When people running Holochain apps turn off their device, they leave the network. What happens to their data and the data of other people they were storing? There are always enough nodes that hold a given piece of data in the network so as to prevent data loss when nodes leave. The DHT and Holochain gossip protocol are designed this way. Also, the redundancy factor of data on a given DHT is configurable so it can be fine-tuned for any purpose. For example, a chat app for a small team might set a redundancy factor of 100% in order to prevent long loading times, while an app with thousands of users might have a very small redundancy factor.","title":"What happens to data when a node leaves the network?"},{"location":"guide/faq/#should-i-build-my-cointoken-on-holochain","text":"Since it's agent-centric instead of data-centric like traditional blockchains, Holochain isn't the best platform on which to build a token or coin. The idea of tokens or coins is a direct representation of a system being data-centric. While theoretically it would be possible to create a token on Holochain, it would be taking a step back instead of a step forward. The more exciting possibility is creating mutual credit currencies on Holochain. These are agent-centric currencies that are designed to facilitate active exchange of value and flourishing ecosystems instead of hoarding.","title":"Should I build my coin/token on Holochain?"},{"location":"guide/faq/#what-does-agent-centric-mean-how-is-this-different-from-data-centric","text":"Agent-centric systems view data not as an object, but as a shared experience. Traditional blockchains are data-centric: they rely on and are built around the concept that data is a thing\u2014an object. Holochain transitions to agent-centricism: the idea that data is shared experiences seen from many points of view. It's not a thing. It's a collection of shared, relative experiences. Einstein discovered this about the physical world a hundred years ago\u2014Relativity. So why are modern blockchains that are supposedly \"cutting edge\" still falling back on this antiquated idea that data is an object, and for two agents to have different views of one piece of data is wrong? Holochain is deeply agent-centric. Using tech that embodies this mindset enables vastly richer interactions and collaboration to happen through its technology while at the same time being thousands of times more efficient.","title":"What does \u201cagent-centric\u201d mean? How is this different from \u201cdata-centric?\u201d"},{"location":"guide/faq/#what-is-the-tps-transactions-per-second-on-holochain","text":"Holochain doesn't have a set TPS (transactions per second) like other blockchain-based or blockchain-derived projects might because there's central point through which all transactions must pass. Instead, Holochain is a generalized protocol for distributed computing. It's common to ask a blockchain project, \"How much can your technology handle? What's its TPS?\" This is because nearly all of these projects are built around the limiting idea of a global ledger. But you are not asking, how many posts per second Facebook can do. Why? Because there is no technical problem, adding more servers to Facebook's data center (only maybe monetary problems). You are not asking how many emails per second the internet can handle, because there is no single bottleneck for email-sending, like there would be with a centralized approach. Why are we seeing a transaction limit with blockchain networks? Because blockchain in a strange way marries a decentralized p2p network of nodes with the logical notion of one absolute truth, i.e. the blockchain being one big decentralized database of transactions. It tries to maintain this way of thinking about apps that we are used to from centralized servers. It forces every node into the same \"consensus\". That is implemented by having everybody share and validate everything. That does work, and maybe there are few usecases (like a global naming system maybe?) where it might be advantageous.. but applying that for everything is nonsensical. Holochain is not forcing such a model. Instead it allows for building applications that are like email. The application is rather like a protocol, or grammar, or (I prefer this language) like a dance. If you know the dance (If you have a copy of the validation rules of the app) you can tell who else is dancing that dance and who is not. The difference between Holochain and something like email is that (similarly to blockhain) Holochain is applying 1. cryptographic signatures and 2. tamper proof hash-chains (hence Holo chain ) so that you can build a distributed system you can trust in. You know it is impossible (I'd rather say: very very hard) to game somebody. This so far was only possible by having trusted authorities like banks or Facebook. So, Holochain as an app framework does not pose any limit of transactions per second because there is no place where all transactions have to go through. It is like asking, \"how many words can humanity speak per second?\" Well, with every human being born, that number increases. Same for Holochain.","title":"What is the TPS (Transactions Per Second) on Holochain?"},{"location":"guide/first_steps/","text":"First steps writing Holochain hApps with Rust \u00b6 This tutorial builds for the 0.0.18-alpha1 release but as the API and HDK are changing it will likely fail under newer releases. Holochain hApps are made of compiled WebAssembly that encodes the rules of the hApp, the data it can store and how users will interact with it. This means that any language that can compile to WebAssembly can one day be used for Holochain. Writing WebAssembly that complies with the Holochain runtime can be tricky. To make development as streamlined as possible the core team has been developing a Holochain-dev-kit (HDK) for the first supported language, Rust! In the near future the community is encouraged to develop an HDK for their language of choice. In this article we will walk through the steps of creating a simple hApp using Rust. Requirements \u00b6 First step is to download the appropriate dev preview release for your OS. If you decide to build the latest version from source, be warned that the API is undergoing rapid change, so some of the steps in this article may not work. The release contains the binary for the holochain developer command line tool, hc , which is used to generate a skeleton app, run tests and build the app package. Follow the installations on this page to install the required dependencies. Ensure that hc is available on your path. If you instead decide to build from source cargo will ensure the binaries are on your path automatically. If you want to jump ahead to see what the completed project will look like, the full source code is available on GitHub . First steps \u00b6 We will be making a classic to-do list hApp. A user can create new lists and add items to a list. They should also be able to retrieve a list by its address and all of the items on each list. Let's begin by generating an empty hApp skeleton by running: hc init holochain-rust-todo This will generate the following directory structure: holochain-rust-todo/ \u251c\u2500\u2500 app.json \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 \u2026 \u2514\u2500\u2500 zomes Notice the zomes directory. All Holochain hApps are comprised of one or more zomes. They can be thought of as similar to modules in JavaScript, each one should provide some self-contained functionality. Every zome has its own build system so it is possible to combine zomes written in different languages to produce a single hApp. We will create a single zome called lists that uses a Rust build system: cd holochain-rust-todo hc generate zomes/lists rust The project structure should now be as follows: \u251c\u2500\u2500 app.json \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 \u2026 \u2514\u2500\u2500 zomes \u2514\u2500\u2500 lists \u251c\u2500\u2500 code \u2502 \u251c\u2500\u2500 .hcbuild \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 src \u2502 \u2514\u2500\u2500 lib.rs \u2514\u2500\u2500 zome.json Writing the lists zome \u00b6 The Rust HDK makes use of Rust macros to reduce the need for boilerplate code. The most important of which is the define_zome! macro. Every zome must use this to define the structure of the zome, what entries it contains, which functions it exposes and what to do on first start-up (init). Open up lib.rs and replace its contents with the following: #[macro_use] extern crate hdk ; define_zome ! { entries : [ ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [ ] traits : { } } This is the simplest possible zome with no entries and no exposed functions. Adding some Entries \u00b6 Unlike in holochain-proto, where you needed to define a JSON schema to validate entries, holochain entries in Rust map to a native struct type. We can define our list and listItem structs as follows: #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct List { name : String } #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct ListItem { text : String , completed : bool } #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct GetListResponse { name : String , items : Vec < ListItem > } You might notice that the List struct does not contain a field that holds a collection of ListItem s. This will be implemented using links, which we will discuss later. Also be sure to add the following to the list of imports: #[macro_use] extern crate hdk ; #[macro_use] extern crate serde_derive ; #[macro_use] extern crate holochain_persistence_derive ; use hdk :: { error :: ZomeApiResult , holochain_core_types :: { hash :: HashString , error :: HolochainError , dna :: entry_types :: Sharing , json :: JsonString , cas :: content :: Address , entry :: Entry , } }; The Serialize and Deserialize derived traits allow the structs to be converted to and from JSON, which is how entries are managed internally in Holochain. The DefaultJson derived trait comes from the holochain HDK itself and allows for seamless converting between data stored in the DHT and rust structs. These structs on their own are not yet valid Holochain entries. To create these we must include them in the define_zome! macro by using the entry! macro: // -- SNIP-- // define_zome ! { entries : [ entry ! ( name : \"list\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < List >| { Ok (()) }, links : [ to ! ( \"listItem\" , link_type : \"items\" , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | _validation_data : hdk :: LinkValidationData | { Ok (()) } ) ] ), entry ! ( name : \"listItem\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < ListItem >| { Ok (()) } ) ] // -- SNIP-- // Take note of the native_type field of the macro which gives which Rust struct represents the entry type. The validation_package field is a function that defines what data should be passed to the validation function through the ctx argument. In this case we use a predefined function to only include the entry itself, but it is also possible to pass chain headers, chain entries or the full local chain. The validation field is a function that performs custom validation for the entry. In both our cases we are just returning Ok(()) . Take note also of the links field. As we will see later links are the main way to encode relational data in holochain. The links section of the entry macro defines what other types of entries are allowed to link to and from this type. This also includes a validation function for fine grain control over linking. Adding Functions \u00b6 Finally we need a way to interact with the hApp. We will define the following functions: create_list , add_item and get_list . get_list will retrieve a list and all the items linked to each list. For each of these functions we must define a handler, which is a Rust function that will be executed when the conductor calls the function. (For more on conductors, read Nico's recent post.) It is best practice for functions to always return a ZomeApiResult<T> , where T is the type the function should return if it runs without error. This is an extension of the Rust Result type and allows zome functions to abort early on errors using the ? operator. At the moment the handler function names cannot be the same as the function itself so we will prefix them with handle_ . This will be fixed in an upcoming release. The handler for create_list could be written as: fn handle_create_list ( list : List ) -> ZomeApiResult < Address > { // define the entry let list_entry = Entry :: App ( \"list\" . into (), list . into () ); // commit the entry and return the address hdk :: commit_entry ( & list_entry ) } The hdk::commit_entry function is how a zome can interact with holochain core to add entries to the DHT or local chain. This will trigger the validation function for the entry and if successful will store the entry and return its hash/address. The add_item function requires the use of holochain links to associate two entries. In holochain-proto this required the use of a commit with a special Links entry but it can now be done using the HDK function link_entries(address1, address2, link_type, tag) . The link_type must exactly match one of the types of links defined in an entry! macro for this base (e.g. link_type: \"items\" in this case). The tag can be any string we wish to associate with this individual link. We will just use an empty string for this example. The add item handler accepts a ListItem and an address of a list, commits the ListItem , then links it to the list address: fn handle_add_item ( list_item : ListItem , list_addr : HashString ) -> ZomeApiResult < Address > { // define the entry let list_item_entry = Entry :: App ( \"listItem\" . into (), list_item . into () ); let item_addr = hdk :: commit_entry ( & list_item_entry ) ? ; // commit the list item hdk :: link_entries ( & list_addr , & item_addr , \"items\" , \"\" ) ? ; // if successful, link to list address Ok ( item_addr ) } At the moment there is no validation done on the link entries. This will be added soon with an additional validation callback. Finally, get_list requires us to use the HDK function get_links(base_address, link_type, tag) . As you may have guessed, this will return the addresses of all the entries that are linked to the base_address with a given link_type and a given tag. Both link_type and tag are LinkMatch types, which is an enum for matching anything, matching exactly, or matching with a regular expression. Passing LinkMatch::Exactly(\"string\") means retrieve links that match the type/tag string exactly and passing LinkMatch::Any to either of them means to retrieve all links regardless of the type/tag. As this only returns the addresses, we must then map over each of then and load the required entry. fn handle_get_list ( list_addr : HashString ) -> ZomeApiResult < GetListResponse > { // load the list entry. Early return error if it cannot load or is wrong type let list = hdk :: utils :: get_as_type :: < List > ( list_addr . clone ()) ? ; // try and load the list items, filter out errors and collect in a vector let list_items = hdk :: get_links ( & list_addr , LinkMatch :: Exactly ( \"items\" ), LinkMatch :: Any ) ? . addresses () . iter () . map ( | item_address | { hdk :: utils :: get_as_type :: < ListItem > ( item_address . to_owned ()) }) . filter_map ( Result :: ok ) . collect :: < Vec < ListItem >> (); // if this was successful then return the list items Ok ( GetListResponse { name : list . name , items : list_items }) } Phew! That is all the handlers set up. Finally the function definitions must be added to the define_zome! macro. Before doing that, it is worth briefly discussing a new concept in Holochain, traits . Traits allow functions to be grouped to control access and in the future will allow hApps to connect to other hApps that implement a particular trait. At this time the only trait we need to consider is the hc_public trait. This is a special named trait that exposes all of the contained functions to the outside world. The function field of our zome definition should be updated to: define_zome! { // -- SNIP-- // functions: [ create_list: { inputs: |list: List|, outputs: |result: ZomeApiResult<Address>|, handler: handle_create_list } add_item: { inputs: |list_item: ListItem, list_addr: HashString|, outputs: |result: ZomeApiResult<Address>|, handler: handle_add_item } get_list: { inputs: |list_addr: HashString|, outputs: |result: ZomeApiResult<GetListResponse>|, handler: handle_get_list } ] traits: { hc_public [create_list, add_item, get_list] } } and there we have it! If you are coding along the full lib.rs should now look like this: #[macro_use] extern crate hdk ; #[macro_use] extern crate serde_derive ; #[macro_use] extern crate holochain_persistence_derive ; use hdk :: { error :: ZomeApiResult , holochain_core_types :: { hash :: HashString , error :: HolochainError , dna :: entry_types :: Sharing , json :: JsonString , cas :: content :: Address , entry :: Entry , } }; define_zome ! { entries : [ entry ! ( name : \"list\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < List >| { Ok (()) }, links : [ to ! ( \"listItem\" , link_type : \"items\" , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | _validation_data : hdk :: LinkValidationData | { Ok (()) } ) ] ), entry ! ( name : \"listItem\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < ListItem >| { Ok (()) } ) ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [ create_list : { inputs : | list : List | , outputs : | result : ZomeApiResult < Address >| , handler : handle_create_list } add_item : { inputs : | list_item : ListItem , list_addr : HashString | , outputs : | result : ZomeApiResult < Address >| , handler : handle_add_item } get_list : { inputs : | list_addr : HashString | , outputs : | result : ZomeApiResult < GetListResponse >| , handler : handle_get_list } ] traits : { hc_public [ create_list , add_item , get_list ] } } #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct List { name : String } #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct ListItem { text : String , completed : bool } #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct GetListResponse { name : String , items : Vec < ListItem > } fn handle_create_list ( list : List ) -> ZomeApiResult < Address > { // define the entry let list_entry = Entry :: App ( \"list\" . into (), list . into () ); // commit the entry and return the address hdk :: commit_entry ( & list_entry ) } fn handle_add_item ( list_item : ListItem , list_addr : HashString ) -> ZomeApiResult < Address > { // define the entry let list_item_entry = Entry :: App ( \"listItem\" . into (), list_item . into () ); let item_addr = hdk :: commit_entry ( & list_item_entry ) ? ; // commit the list item hdk :: link_entries ( & list_addr , & item_addr , \"items\" ) ? ; // if successful, link to list address Ok ( item_addr ) } fn handle_get_list ( list_addr : HashString ) -> ZomeApiResult < GetListResponse > { // load the list entry. Early return error if it cannot load or is wrong type let list = hdk :: utils :: get_as_type :: < List > ( list_addr . clone ()) ? ; // try and load the list items, filter out errors and collect in a vector let list_items = hdk :: get_links ( & list_addr , LinkMatch :: Exactly ( \"items\" ), LinkMatch :: Any ) ? . addresses () . iter () . map ( | item_address | { hdk :: utils :: get_as_type :: < ListItem > ( item_address . to_owned ()) }) . filter_map ( Result :: ok ) . collect :: < Vec < ListItem >> (); // if this was successful then return the list items Ok ( GetListResponse { name : list . name , items : list_items }) } The Zome we created should now build if we run: hc package from the root directory. This will compile the Rust to WebAssembly and produce a holochain-rust-todo.dna.json file in the dist folder which contains the compiled WASM code and the required metadata. This is the file that we can load and run using hc . Writing tests \u00b6 The testing framework is built on JavaScript around Tape.js and allows for writing single agent and muti-agent tests using javascript async/await syntax. Opening up the test/index.js file you will see a skeleton test file already created: // This test file uses the tape testing framework. // To learn more, go here: https://github.com/substack/tape const { Config , Scenario } = require ( \"@holochain/holochain-nodejs\" ) Scenario . setTape ( require ( \"tape\" )) const dnaPath = \"./dist/holochain-rust-todo.dna.json\" const agentAlice = Config . agent ( \"alice\" ) const dna = Config . dna ( dnaPath ) const instanceAlice = Config . instance ( agentAlice , dna ) const scenario = new Scenario ([ instanceAlice ]) scenario . runTape ( \"description of example test\" , async ( t , { alice }) => { // Make a call to a Zome function // indicating the function, and passing it an input const addr = alice . call ( \"my_zome\" , \"create_my_entry\" , { \"entry\" : { \"content\" : \"sample content\" }}) const result = alice . call ( \"my_zome\" , \"get_my_entry\" , { \"address\" : addr . Ok }) // check for equality of the actual and expected results t . deepEqual ( result , { Ok : { App : [ 'my_entry' , '{\"content\":\"sample content\"}' ] } }) }) This illustrates the app.call function that is exposed by the conductor for each app and that can be used to call our functions. Take note that the input-data should be a JSON object that matches the function signature. call will also return a JSON object. Lets add some tests for our todo list: const { Config , Scenario } = require ( '@holochain/holochain-nodejs' ) Scenario . setTape ( require ( 'tape' )) const dnaPath = \"./dist/holochain-rust-todo.dna.json\" const dna = Config . dna ( dnaPath , 'happs' ) const agentAlice = Config . agent ( 'alice' ) const instanceAlice = Config . instance ( agentAlice , dna ) const scenario = new Scenario ([ instanceAlice ]) scenario . runTape ( 'Can create a list' , async ( t , { alice }) => { const createResult = await alice . callSync ( 'lists' , 'create_list' , { list : { name : 'test list' } }) console . log ( createResult ) t . notEqual ( createResult . Ok , undefined ) }) scenario . runTape ( 'Can add some items' , async ( t , { alice }) => { const createResult = await alice . callSync ( 'lists' , 'create_list' , { list : { name : 'test list' } }) const listAddr = createResult . Ok const result1 = await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Learn Rust' , completed : true }, list_addr : listAddr }) const result2 = await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Master Holochain' , completed : false }, list_addr : listAddr }) console . log ( result1 ) console . log ( result2 ) t . notEqual ( result1 . Ok , undefined ) t . notEqual ( result2 . Ok , undefined ) }) scenario . runTape ( 'Can get a list with items' , async ( t , { alice }) => { const createResult = await alice . callSync ( 'lists' , 'create_list' , { list : { name : 'test list' } }) const listAddr = createResult . Ok await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Learn Rust' , completed : true }, list_addr : listAddr }) await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Master Holochain' , completed : false }, list_addr : listAddr }) const getResult = await alice . callSync ( 'lists' , 'get_list' , { list_addr : listAddr }) console . log ( getResult ) t . equal ( getResult . Ok . items . length , 2 , 'there should be 2 items in the list' ) }) Running hc test will build the test file and run it using node which is able to load and execute holochain hApps via the holochain node conductor. If everything has worked correctly you should see some test output with everything passing. Pro tip: Pipe the output to tap-spec (which must be installed via npm first) to get beautifully formatted test output. Conclusion \u00b6 And there we have it! A simple Zome created with Holochain using the Rust HDK. The complete working version of this project is available on github . This builds under the 0.0.9-alpha release but as the API and HDK are changing it will likely fail under newer releases.","title":"Build A To-Do App!"},{"location":"guide/first_steps/#first-steps-writing-holochain-happs-with-rust","text":"This tutorial builds for the 0.0.18-alpha1 release but as the API and HDK are changing it will likely fail under newer releases. Holochain hApps are made of compiled WebAssembly that encodes the rules of the hApp, the data it can store and how users will interact with it. This means that any language that can compile to WebAssembly can one day be used for Holochain. Writing WebAssembly that complies with the Holochain runtime can be tricky. To make development as streamlined as possible the core team has been developing a Holochain-dev-kit (HDK) for the first supported language, Rust! In the near future the community is encouraged to develop an HDK for their language of choice. In this article we will walk through the steps of creating a simple hApp using Rust.","title":"First steps writing Holochain hApps with\u00a0Rust"},{"location":"guide/first_steps/#requirements","text":"First step is to download the appropriate dev preview release for your OS. If you decide to build the latest version from source, be warned that the API is undergoing rapid change, so some of the steps in this article may not work. The release contains the binary for the holochain developer command line tool, hc , which is used to generate a skeleton app, run tests and build the app package. Follow the installations on this page to install the required dependencies. Ensure that hc is available on your path. If you instead decide to build from source cargo will ensure the binaries are on your path automatically. If you want to jump ahead to see what the completed project will look like, the full source code is available on GitHub .","title":"Requirements"},{"location":"guide/first_steps/#first-steps","text":"We will be making a classic to-do list hApp. A user can create new lists and add items to a list. They should also be able to retrieve a list by its address and all of the items on each list. Let's begin by generating an empty hApp skeleton by running: hc init holochain-rust-todo This will generate the following directory structure: holochain-rust-todo/ \u251c\u2500\u2500 app.json \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 \u2026 \u2514\u2500\u2500 zomes Notice the zomes directory. All Holochain hApps are comprised of one or more zomes. They can be thought of as similar to modules in JavaScript, each one should provide some self-contained functionality. Every zome has its own build system so it is possible to combine zomes written in different languages to produce a single hApp. We will create a single zome called lists that uses a Rust build system: cd holochain-rust-todo hc generate zomes/lists rust The project structure should now be as follows: \u251c\u2500\u2500 app.json \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 \u2026 \u2514\u2500\u2500 zomes \u2514\u2500\u2500 lists \u251c\u2500\u2500 code \u2502 \u251c\u2500\u2500 .hcbuild \u2502 \u251c\u2500\u2500 Cargo.toml \u2502 \u2514\u2500\u2500 src \u2502 \u2514\u2500\u2500 lib.rs \u2514\u2500\u2500 zome.json","title":"First steps"},{"location":"guide/first_steps/#writing-the-lists-zome","text":"The Rust HDK makes use of Rust macros to reduce the need for boilerplate code. The most important of which is the define_zome! macro. Every zome must use this to define the structure of the zome, what entries it contains, which functions it exposes and what to do on first start-up (init). Open up lib.rs and replace its contents with the following: #[macro_use] extern crate hdk ; define_zome ! { entries : [ ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [ ] traits : { } } This is the simplest possible zome with no entries and no exposed functions.","title":"Writing the lists\u00a0zome"},{"location":"guide/first_steps/#adding-some-entries","text":"Unlike in holochain-proto, where you needed to define a JSON schema to validate entries, holochain entries in Rust map to a native struct type. We can define our list and listItem structs as follows: #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct List { name : String } #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct ListItem { text : String , completed : bool } #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct GetListResponse { name : String , items : Vec < ListItem > } You might notice that the List struct does not contain a field that holds a collection of ListItem s. This will be implemented using links, which we will discuss later. Also be sure to add the following to the list of imports: #[macro_use] extern crate hdk ; #[macro_use] extern crate serde_derive ; #[macro_use] extern crate holochain_persistence_derive ; use hdk :: { error :: ZomeApiResult , holochain_core_types :: { hash :: HashString , error :: HolochainError , dna :: entry_types :: Sharing , json :: JsonString , cas :: content :: Address , entry :: Entry , } }; The Serialize and Deserialize derived traits allow the structs to be converted to and from JSON, which is how entries are managed internally in Holochain. The DefaultJson derived trait comes from the holochain HDK itself and allows for seamless converting between data stored in the DHT and rust structs. These structs on their own are not yet valid Holochain entries. To create these we must include them in the define_zome! macro by using the entry! macro: // -- SNIP-- // define_zome ! { entries : [ entry ! ( name : \"list\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < List >| { Ok (()) }, links : [ to ! ( \"listItem\" , link_type : \"items\" , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | _validation_data : hdk :: LinkValidationData | { Ok (()) } ) ] ), entry ! ( name : \"listItem\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < ListItem >| { Ok (()) } ) ] // -- SNIP-- // Take note of the native_type field of the macro which gives which Rust struct represents the entry type. The validation_package field is a function that defines what data should be passed to the validation function through the ctx argument. In this case we use a predefined function to only include the entry itself, but it is also possible to pass chain headers, chain entries or the full local chain. The validation field is a function that performs custom validation for the entry. In both our cases we are just returning Ok(()) . Take note also of the links field. As we will see later links are the main way to encode relational data in holochain. The links section of the entry macro defines what other types of entries are allowed to link to and from this type. This also includes a validation function for fine grain control over linking.","title":"Adding some\u00a0Entries"},{"location":"guide/first_steps/#adding-functions","text":"Finally we need a way to interact with the hApp. We will define the following functions: create_list , add_item and get_list . get_list will retrieve a list and all the items linked to each list. For each of these functions we must define a handler, which is a Rust function that will be executed when the conductor calls the function. (For more on conductors, read Nico's recent post.) It is best practice for functions to always return a ZomeApiResult<T> , where T is the type the function should return if it runs without error. This is an extension of the Rust Result type and allows zome functions to abort early on errors using the ? operator. At the moment the handler function names cannot be the same as the function itself so we will prefix them with handle_ . This will be fixed in an upcoming release. The handler for create_list could be written as: fn handle_create_list ( list : List ) -> ZomeApiResult < Address > { // define the entry let list_entry = Entry :: App ( \"list\" . into (), list . into () ); // commit the entry and return the address hdk :: commit_entry ( & list_entry ) } The hdk::commit_entry function is how a zome can interact with holochain core to add entries to the DHT or local chain. This will trigger the validation function for the entry and if successful will store the entry and return its hash/address. The add_item function requires the use of holochain links to associate two entries. In holochain-proto this required the use of a commit with a special Links entry but it can now be done using the HDK function link_entries(address1, address2, link_type, tag) . The link_type must exactly match one of the types of links defined in an entry! macro for this base (e.g. link_type: \"items\" in this case). The tag can be any string we wish to associate with this individual link. We will just use an empty string for this example. The add item handler accepts a ListItem and an address of a list, commits the ListItem , then links it to the list address: fn handle_add_item ( list_item : ListItem , list_addr : HashString ) -> ZomeApiResult < Address > { // define the entry let list_item_entry = Entry :: App ( \"listItem\" . into (), list_item . into () ); let item_addr = hdk :: commit_entry ( & list_item_entry ) ? ; // commit the list item hdk :: link_entries ( & list_addr , & item_addr , \"items\" , \"\" ) ? ; // if successful, link to list address Ok ( item_addr ) } At the moment there is no validation done on the link entries. This will be added soon with an additional validation callback. Finally, get_list requires us to use the HDK function get_links(base_address, link_type, tag) . As you may have guessed, this will return the addresses of all the entries that are linked to the base_address with a given link_type and a given tag. Both link_type and tag are LinkMatch types, which is an enum for matching anything, matching exactly, or matching with a regular expression. Passing LinkMatch::Exactly(\"string\") means retrieve links that match the type/tag string exactly and passing LinkMatch::Any to either of them means to retrieve all links regardless of the type/tag. As this only returns the addresses, we must then map over each of then and load the required entry. fn handle_get_list ( list_addr : HashString ) -> ZomeApiResult < GetListResponse > { // load the list entry. Early return error if it cannot load or is wrong type let list = hdk :: utils :: get_as_type :: < List > ( list_addr . clone ()) ? ; // try and load the list items, filter out errors and collect in a vector let list_items = hdk :: get_links ( & list_addr , LinkMatch :: Exactly ( \"items\" ), LinkMatch :: Any ) ? . addresses () . iter () . map ( | item_address | { hdk :: utils :: get_as_type :: < ListItem > ( item_address . to_owned ()) }) . filter_map ( Result :: ok ) . collect :: < Vec < ListItem >> (); // if this was successful then return the list items Ok ( GetListResponse { name : list . name , items : list_items }) } Phew! That is all the handlers set up. Finally the function definitions must be added to the define_zome! macro. Before doing that, it is worth briefly discussing a new concept in Holochain, traits . Traits allow functions to be grouped to control access and in the future will allow hApps to connect to other hApps that implement a particular trait. At this time the only trait we need to consider is the hc_public trait. This is a special named trait that exposes all of the contained functions to the outside world. The function field of our zome definition should be updated to: define_zome! { // -- SNIP-- // functions: [ create_list: { inputs: |list: List|, outputs: |result: ZomeApiResult<Address>|, handler: handle_create_list } add_item: { inputs: |list_item: ListItem, list_addr: HashString|, outputs: |result: ZomeApiResult<Address>|, handler: handle_add_item } get_list: { inputs: |list_addr: HashString|, outputs: |result: ZomeApiResult<GetListResponse>|, handler: handle_get_list } ] traits: { hc_public [create_list, add_item, get_list] } } and there we have it! If you are coding along the full lib.rs should now look like this: #[macro_use] extern crate hdk ; #[macro_use] extern crate serde_derive ; #[macro_use] extern crate holochain_persistence_derive ; use hdk :: { error :: ZomeApiResult , holochain_core_types :: { hash :: HashString , error :: HolochainError , dna :: entry_types :: Sharing , json :: JsonString , cas :: content :: Address , entry :: Entry , } }; define_zome ! { entries : [ entry ! ( name : \"list\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < List >| { Ok (()) }, links : [ to ! ( \"listItem\" , link_type : \"items\" , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | _validation_data : hdk :: LinkValidationData | { Ok (()) } ) ] ), entry ! ( name : \"listItem\" , description : \"\" , sharing : Sharing :: Public , validation_package : || hdk :: ValidationPackageDefinition :: Entry , validation : | validation_data : hdk :: EntryValidationData < ListItem >| { Ok (()) } ) ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [ create_list : { inputs : | list : List | , outputs : | result : ZomeApiResult < Address >| , handler : handle_create_list } add_item : { inputs : | list_item : ListItem , list_addr : HashString | , outputs : | result : ZomeApiResult < Address >| , handler : handle_add_item } get_list : { inputs : | list_addr : HashString | , outputs : | result : ZomeApiResult < GetListResponse >| , handler : handle_get_list } ] traits : { hc_public [ create_list , add_item , get_list ] } } #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct List { name : String } #[derive(Serialize, Deserialize, Debug, Clone, DefaultJson)] struct ListItem { text : String , completed : bool } #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct GetListResponse { name : String , items : Vec < ListItem > } fn handle_create_list ( list : List ) -> ZomeApiResult < Address > { // define the entry let list_entry = Entry :: App ( \"list\" . into (), list . into () ); // commit the entry and return the address hdk :: commit_entry ( & list_entry ) } fn handle_add_item ( list_item : ListItem , list_addr : HashString ) -> ZomeApiResult < Address > { // define the entry let list_item_entry = Entry :: App ( \"listItem\" . into (), list_item . into () ); let item_addr = hdk :: commit_entry ( & list_item_entry ) ? ; // commit the list item hdk :: link_entries ( & list_addr , & item_addr , \"items\" ) ? ; // if successful, link to list address Ok ( item_addr ) } fn handle_get_list ( list_addr : HashString ) -> ZomeApiResult < GetListResponse > { // load the list entry. Early return error if it cannot load or is wrong type let list = hdk :: utils :: get_as_type :: < List > ( list_addr . clone ()) ? ; // try and load the list items, filter out errors and collect in a vector let list_items = hdk :: get_links ( & list_addr , LinkMatch :: Exactly ( \"items\" ), LinkMatch :: Any ) ? . addresses () . iter () . map ( | item_address | { hdk :: utils :: get_as_type :: < ListItem > ( item_address . to_owned ()) }) . filter_map ( Result :: ok ) . collect :: < Vec < ListItem >> (); // if this was successful then return the list items Ok ( GetListResponse { name : list . name , items : list_items }) } The Zome we created should now build if we run: hc package from the root directory. This will compile the Rust to WebAssembly and produce a holochain-rust-todo.dna.json file in the dist folder which contains the compiled WASM code and the required metadata. This is the file that we can load and run using hc .","title":"Adding Functions"},{"location":"guide/first_steps/#writing-tests","text":"The testing framework is built on JavaScript around Tape.js and allows for writing single agent and muti-agent tests using javascript async/await syntax. Opening up the test/index.js file you will see a skeleton test file already created: // This test file uses the tape testing framework. // To learn more, go here: https://github.com/substack/tape const { Config , Scenario } = require ( \"@holochain/holochain-nodejs\" ) Scenario . setTape ( require ( \"tape\" )) const dnaPath = \"./dist/holochain-rust-todo.dna.json\" const agentAlice = Config . agent ( \"alice\" ) const dna = Config . dna ( dnaPath ) const instanceAlice = Config . instance ( agentAlice , dna ) const scenario = new Scenario ([ instanceAlice ]) scenario . runTape ( \"description of example test\" , async ( t , { alice }) => { // Make a call to a Zome function // indicating the function, and passing it an input const addr = alice . call ( \"my_zome\" , \"create_my_entry\" , { \"entry\" : { \"content\" : \"sample content\" }}) const result = alice . call ( \"my_zome\" , \"get_my_entry\" , { \"address\" : addr . Ok }) // check for equality of the actual and expected results t . deepEqual ( result , { Ok : { App : [ 'my_entry' , '{\"content\":\"sample content\"}' ] } }) }) This illustrates the app.call function that is exposed by the conductor for each app and that can be used to call our functions. Take note that the input-data should be a JSON object that matches the function signature. call will also return a JSON object. Lets add some tests for our todo list: const { Config , Scenario } = require ( '@holochain/holochain-nodejs' ) Scenario . setTape ( require ( 'tape' )) const dnaPath = \"./dist/holochain-rust-todo.dna.json\" const dna = Config . dna ( dnaPath , 'happs' ) const agentAlice = Config . agent ( 'alice' ) const instanceAlice = Config . instance ( agentAlice , dna ) const scenario = new Scenario ([ instanceAlice ]) scenario . runTape ( 'Can create a list' , async ( t , { alice }) => { const createResult = await alice . callSync ( 'lists' , 'create_list' , { list : { name : 'test list' } }) console . log ( createResult ) t . notEqual ( createResult . Ok , undefined ) }) scenario . runTape ( 'Can add some items' , async ( t , { alice }) => { const createResult = await alice . callSync ( 'lists' , 'create_list' , { list : { name : 'test list' } }) const listAddr = createResult . Ok const result1 = await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Learn Rust' , completed : true }, list_addr : listAddr }) const result2 = await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Master Holochain' , completed : false }, list_addr : listAddr }) console . log ( result1 ) console . log ( result2 ) t . notEqual ( result1 . Ok , undefined ) t . notEqual ( result2 . Ok , undefined ) }) scenario . runTape ( 'Can get a list with items' , async ( t , { alice }) => { const createResult = await alice . callSync ( 'lists' , 'create_list' , { list : { name : 'test list' } }) const listAddr = createResult . Ok await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Learn Rust' , completed : true }, list_addr : listAddr }) await alice . callSync ( 'lists' , 'add_item' , { list_item : { text : 'Master Holochain' , completed : false }, list_addr : listAddr }) const getResult = await alice . callSync ( 'lists' , 'get_list' , { list_addr : listAddr }) console . log ( getResult ) t . equal ( getResult . Ok . items . length , 2 , 'there should be 2 items in the list' ) }) Running hc test will build the test file and run it using node which is able to load and execute holochain hApps via the holochain node conductor. If everything has worked correctly you should see some test output with everything passing. Pro tip: Pipe the output to tap-spec (which must be installed via npm first) to get beautifully formatted test output.","title":"Writing tests"},{"location":"guide/first_steps/#conclusion","text":"And there we have it! A simple Zome created with Holochain using the Rust HDK. The complete working version of this project is available on github . This builds under the 0.0.9-alpha release but as the API and HDK are changing it will likely fail under newer releases.","title":"Conclusion"},{"location":"guide/glossary/","text":"Glossary \u00b6","title":"Glossary"},{"location":"guide/glossary/#glossary","text":"","title":"Glossary"},{"location":"guide/handling_async/","text":"Handling Asynchronous Network Effects \u00b6 In the previous example, we used alice.call() to call a zome function. This returns immediately with a value, even though the test network created by the conductor is still running, sending messages back and forth between agents for purposes of validation and replication, etc. In many test cases, you will want to wait until all of this network activity has died down to advance to the next step. For instance, take the very common scenario as an example: Alice runs a zome function which commits an entry, then adds a link to that entry Bob runs a zome function which attempt to get links, which should include the link added by alice If the test just uses call() to call that zome function, there is no guarantee that the entries committed by alice.call will be available on the DHT by the time bob.call is started. Therefore, two other functions are available. alice.callSync returns a Promise instead of a simple value. The promise does not resolve until network activity has completed. alice.callWithPromise is a slightly lower-level version of the same thing. It splits the value apart from the promise into a tuple [value, promise] , so that the value can be acted on immediately and the promise waited upon separately. // If we make the closure `async`, we can use `await` syntax to keep things cleaner scenario . run ( async ( stop , { alice , bob }) => { tape ( \"test something\" , t => { // we can await on `callSync` immediately, causing it // to block until network activity has died down const result1 = await alice . callSync ( 'zome' , 'do_something_that_adds_links' , {}) // now bob can be sure he has access to the latest data const result2 = bob . call ( 'zome' , 'get_those_links' , {}) t . equal ( result , 'expected value' ) // the following two steps were not necessary when using runTape: t . end () // end the test stop () // use this injected function to stop the conductor }) }) Even though we can't solve the eventual consistency problem in real life networks, we can solve them in tests when we have total knowledge about what each agent is doing.","title":"Handle Network Asynchronicity"},{"location":"guide/handling_async/#handling-asynchronous-network-effects","text":"In the previous example, we used alice.call() to call a zome function. This returns immediately with a value, even though the test network created by the conductor is still running, sending messages back and forth between agents for purposes of validation and replication, etc. In many test cases, you will want to wait until all of this network activity has died down to advance to the next step. For instance, take the very common scenario as an example: Alice runs a zome function which commits an entry, then adds a link to that entry Bob runs a zome function which attempt to get links, which should include the link added by alice If the test just uses call() to call that zome function, there is no guarantee that the entries committed by alice.call will be available on the DHT by the time bob.call is started. Therefore, two other functions are available. alice.callSync returns a Promise instead of a simple value. The promise does not resolve until network activity has completed. alice.callWithPromise is a slightly lower-level version of the same thing. It splits the value apart from the promise into a tuple [value, promise] , so that the value can be acted on immediately and the promise waited upon separately. // If we make the closure `async`, we can use `await` syntax to keep things cleaner scenario . run ( async ( stop , { alice , bob }) => { tape ( \"test something\" , t => { // we can await on `callSync` immediately, causing it // to block until network activity has died down const result1 = await alice . callSync ( 'zome' , 'do_something_that_adds_links' , {}) // now bob can be sure he has access to the latest data const result2 = bob . call ( 'zome' , 'get_those_links' , {}) t . equal ( result , 'expected value' ) // the following two steps were not necessary when using runTape: t . end () // end the test stop () // use this injected function to stop the conductor }) }) Even though we can't solve the eventual consistency problem in real life networks, we can solve them in tests when we have total knowledge about what each agent is doing.","title":"Handling Asynchronous Network Effects"},{"location":"guide/hc_configuring_networking/","text":"Configuring Networking for hc run \u00b6 hc run uses mock networking by default and therefore doesn't talk to any other nodes. In order to have hc run spawn a real network instance, start it with the --networked option: hc run --networked You should see something like this: Network spawned with bindings: - ipc: wss://127.0.0.1:64518/ - p2p: [ \"wss://192.168.0.11:64519/?a=hkYW7TrZUS1hy-i374iRu5VbZP1sSw2mLxP4TSe_YI1H2BJM3v_LgAQnpmWA_iR1W5k-8_UoA1BNjzBSUTVNDSIcz9UG0uaM\" ] ... Starting A Second Node \u00b6 Starting up a second node is a little bit more work: Step 1 \u00b6 Set the HC_N3H_BOOTSTRAP_NODE environment variable to the external p2p bound address listed by the first node. Copy-paste it from the string from the terminal log of the first node, the one that starts with \"/ip4/192.168\". Step 2 \u00b6 Specify a different agent id than the first node, by setting the HC_AGENT environment variable. Since the first agent by default will be testAgent , testAgent2 is suitable. Step 3 \u00b6 Specify a different port than the first node to run on. Since the port for the first node by default will be 8888 , 8889 is suitable. Running the command could look like this: HC_AGENT = testAgent2 HC_N3H_BOOTSTRAP_NODE = wss://192.168.0.11:64519/?a = hkYW7TrZUS1hy-i374iRu5VbZP1sSw2mLxP4TSe_YI1H2BJM3v_LgAQnpmWA_iR1W5k-8_UoA1BNjzBSUTVNDSIcz9UG0uaM hc run --port 8889 In the terminal logs that follow, you should see: ( libp2p ) [ i ] QmUmUF..V71C new peer QmeDpQLchA9xeLDJ2jyXBwpe1JaQhFRrnWC2JfyyET2AAM ( libp2p ) [ i ] QmUmUF..V71C found QmeDpQLchA9xeLDJ2jyXBwpe1JaQhFRrnWC2JfyyET2AAM in 14 ms ( libp2p ) [ i ] QmUmUF..V71C ping round trip 37 ms ( libp2p ) [ i ] QmUmUF..V71C got ping, sending pong This means that the nodes are able to communicate! Watch the logs for gossip, as you take actions (that alter the source chain) in either node.","title":"Configuring Networking"},{"location":"guide/hc_configuring_networking/#configuring-networking-for-hc-run","text":"hc run uses mock networking by default and therefore doesn't talk to any other nodes. In order to have hc run spawn a real network instance, start it with the --networked option: hc run --networked You should see something like this: Network spawned with bindings: - ipc: wss://127.0.0.1:64518/ - p2p: [ \"wss://192.168.0.11:64519/?a=hkYW7TrZUS1hy-i374iRu5VbZP1sSw2mLxP4TSe_YI1H2BJM3v_LgAQnpmWA_iR1W5k-8_UoA1BNjzBSUTVNDSIcz9UG0uaM\" ] ...","title":"Configuring Networking for hc run"},{"location":"guide/hc_configuring_networking/#starting-a-second-node","text":"Starting up a second node is a little bit more work:","title":"Starting A Second Node"},{"location":"guide/hc_configuring_networking/#step-1","text":"Set the HC_N3H_BOOTSTRAP_NODE environment variable to the external p2p bound address listed by the first node. Copy-paste it from the string from the terminal log of the first node, the one that starts with \"/ip4/192.168\".","title":"Step 1"},{"location":"guide/hc_configuring_networking/#step-2","text":"Specify a different agent id than the first node, by setting the HC_AGENT environment variable. Since the first agent by default will be testAgent , testAgent2 is suitable.","title":"Step 2"},{"location":"guide/hc_configuring_networking/#step-3","text":"Specify a different port than the first node to run on. Since the port for the first node by default will be 8888 , 8889 is suitable. Running the command could look like this: HC_AGENT = testAgent2 HC_N3H_BOOTSTRAP_NODE = wss://192.168.0.11:64519/?a = hkYW7TrZUS1hy-i374iRu5VbZP1sSw2mLxP4TSe_YI1H2BJM3v_LgAQnpmWA_iR1W5k-8_UoA1BNjzBSUTVNDSIcz9UG0uaM hc run --port 8889 In the terminal logs that follow, you should see: ( libp2p ) [ i ] QmUmUF..V71C new peer QmeDpQLchA9xeLDJ2jyXBwpe1JaQhFRrnWC2JfyyET2AAM ( libp2p ) [ i ] QmUmUF..V71C found QmeDpQLchA9xeLDJ2jyXBwpe1JaQhFRrnWC2JfyyET2AAM in 14 ms ( libp2p ) [ i ] QmUmUF..V71C ping round trip 37 ms ( libp2p ) [ i ] QmUmUF..V71C got ping, sending pong This means that the nodes are able to communicate! Watch the logs for gossip, as you take actions (that alter the source chain) in either node.","title":"Step 3"},{"location":"guide/hcignore_files/","text":"Ignoring Files Using A .hcignore File \u00b6 Sometimes, you'll want to exclude files and folders in your project directory to get a straight .dna.json file that can be understood by Holochain. In order to do that, just create a .hcignore file. It has a similar structure to .gitignore files: README.md dist .DS_Store The hc package command includes patterns inside .gitignore files automatically, so you don't have to write everything twice. Also hidden files are ignored by default as well. Because hc package will attempt to package everything in the directory that is not explicitly ignored, Holochain will return an error if the DNA package is malformed. It is a common mistake to forget to exclude files or folders in the .hcignore file, so that your DNA will be valid.","title":".hcignore Files"},{"location":"guide/hcignore_files/#ignoring-files-using-a-hcignore-file","text":"Sometimes, you'll want to exclude files and folders in your project directory to get a straight .dna.json file that can be understood by Holochain. In order to do that, just create a .hcignore file. It has a similar structure to .gitignore files: README.md dist .DS_Store The hc package command includes patterns inside .gitignore files automatically, so you don't have to write everything twice. Also hidden files are ignored by default as well. Because hc package will attempt to package everything in the directory that is not explicitly ignored, Holochain will return an error if the DNA package is malformed. It is a common mistake to forget to exclude files or folders in the .hcignore file, so that your DNA will be valid.","title":"Ignoring Files Using A .hcignore File"},{"location":"guide/how_to_contribute/","text":"How to contribute \u00b6 This book uses a tool that builds HTML files from markdown files called 'mdbook' . The markdown files are stored on GitHub, in the main holochain-rust repository. Because they are on GitHub, they have built-in version control, meaning that it's easy for anyone to contribute, and to propose, track and merge changes. There are two main pathways to contribute. One is by editing files directly in GitHub, and the other is cloning the repository to your computer, running mdbook locally, and then submitting change requests. The following covers both scenarios. Writing Guidelines \u00b6 Please do not edit the SUMMARY.md file, which stores the raw chapter structure, without advanced signoff from the documentation team in https://chat.holochain.org/appsup/channels/hc-core . More forthcoming! How the Book Works \u00b6 Writing is in classic markdown, so a new page is a new markdown file. These files get rendered in the book panel. One markdown file, SUMMARY.md stores the structure of the book as an outline. The HTML files used in the Book get automatically built from the markdown files. Even though they're auto-generated, static HTML files, one can search within the text. What to Contribute \u00b6 For a current list of open documentation issues, check out the 'documentation' label for github issues . Contributing via GitHub \u00b6 Getting there \u00b6 1) Log on to your GitHub account 2) In the Holochain Rust repo, everything is under the doc/holochain_101/src folder. All markdown files are there, and some are nested in subfolders. Navigate to the following link: https://github.com/holochain/holochain-rust/tree/develop/doc/holochain_101/src 3) Determine whether you are making, editing, or reviewing an article. Access Rights \u00b6 If you don't have write access to the repository you need to create a fork to contribute. Forking is easy. Click the \"Fork\" button in the top right hand corner of the Github UI. Making a new article \u00b6 1) Click \"Create New File\" 2) Name this file what you intend to name the article, plus the .md extension, i.e. how_to_contribute.md 3) Use classic markdown to set up the page title, i.e. \"# How to contribute\" 4) Write the rest of your text, checking the \"Preview\" tab to see how it would look. 5) Scroll to the bottom of the page and select the option \"create a new branch for this commit and start a pull request\". You can name a branch, though GitHub will set one automatically. If you know it, mention the issue that the request addresses. 6) Click \"Propose New File\". Proceed to the Making Multiple Edits or Opening a Pull Request section. Editing an article \u00b6 1) Navigate to the article you want to edit. 2) Click the 'pencil' icon to edit the article. There's a built-in text editor in GitHub, where you can write a change and also why you changed it (so that a reviewer can understand the rationale for the change). 3) Select the branching method for making your change. (See Making Multiple Edits for clarification) 4) Click \"Propose File Change\". Proceed to the Making Multiple Edits or Opening a Pull Request section. Making Multiple Edits On One Branch & Pull Request \u00b6 A \"branch\" is a series of divergent changes from the main version. If you want to make multiple edits at once, you will need to make each of those changes on the same branch as you named your original edit. Check which branch you are on by looking for the \"Branch: ?\" dropdown. Use the dropdown to switch to your branch if you're on the wrong one. Opening a Pull Request \u00b6 1) Once redirected to the \"comparing changes\" page, prepend your pull request title with \"MDBOOK: \" and then a very short statement of what changed. 2) Add a more detailed description of what changes you made and why in the text box. 3) If there is an open issue related to the article you're submiting or editing, tag it by using the \"#\" plus the issue number. 4) Add the \"documentation\" label. 5) If appropriate, click \"Reviewers\" and select one or more people to request reviews from. 6) Click \"Create Pull Request\". Reviewing a Pull Request \u00b6 1) Under the Pull Request tab, look for ones starting with \"MDBOOK\". Go to the Pull Request of your choice, and then click on the \"Files Changed\" tab. 2) Start a review by hovering over a line and pressing the blue \"add\" symbol to add comments to a line 3) Click the green \"Review Changes\" button. If you approve of the changes, select \"Approve\". If you would like further changes to be made before it gets merged, select \"Request Changes\". If you are just weighing in, select \"Comment\". Then, click \"Submit Review\". Merging a Pull Request \u00b6 3) Under \"Conversation\" you can merge the pull request, which integrates it into the develop branch. Changes automatically deploy to https://holochain.github.io/holochain-rust within ~30 minutes. Merge the pull request once it has received two approved reviews. Contributing by Cloning and Running Mdbook (advanced) \u00b6 You will need to have cloned holochain-rust to your computer. You will also need Docker installed. There is a Docker build that allows local build, serve, watch and live reload for the book. From the root of the repo, run: . docker/build-mdbook-image && . docker/run-mdbook Once the book has built and is serving, visit http://localhost:3000 in the browser. You can edit the markdown files in doc/holochain_101/src and the book will live reload. To do a one-time build of the files to HTML, run: . docker/build-mdbook Edit the files to your satisfaction, commit them to a branch (or a fork) and then open a pull request on GitHub. Once its on GitHub, the same things as mentioned above apply.","title":"How to contribute"},{"location":"guide/how_to_contribute/#how-to-contribute","text":"This book uses a tool that builds HTML files from markdown files called 'mdbook' . The markdown files are stored on GitHub, in the main holochain-rust repository. Because they are on GitHub, they have built-in version control, meaning that it's easy for anyone to contribute, and to propose, track and merge changes. There are two main pathways to contribute. One is by editing files directly in GitHub, and the other is cloning the repository to your computer, running mdbook locally, and then submitting change requests. The following covers both scenarios.","title":"How to contribute"},{"location":"guide/how_to_contribute/#writing-guidelines","text":"Please do not edit the SUMMARY.md file, which stores the raw chapter structure, without advanced signoff from the documentation team in https://chat.holochain.org/appsup/channels/hc-core . More forthcoming!","title":"Writing Guidelines"},{"location":"guide/how_to_contribute/#how-the-book-works","text":"Writing is in classic markdown, so a new page is a new markdown file. These files get rendered in the book panel. One markdown file, SUMMARY.md stores the structure of the book as an outline. The HTML files used in the Book get automatically built from the markdown files. Even though they're auto-generated, static HTML files, one can search within the text.","title":"How the Book Works"},{"location":"guide/how_to_contribute/#what-to-contribute","text":"For a current list of open documentation issues, check out the 'documentation' label for github issues .","title":"What to Contribute"},{"location":"guide/how_to_contribute/#contributing-via-github","text":"","title":"Contributing via GitHub"},{"location":"guide/how_to_contribute/#getting-there","text":"1) Log on to your GitHub account 2) In the Holochain Rust repo, everything is under the doc/holochain_101/src folder. All markdown files are there, and some are nested in subfolders. Navigate to the following link: https://github.com/holochain/holochain-rust/tree/develop/doc/holochain_101/src 3) Determine whether you are making, editing, or reviewing an article.","title":"Getting there"},{"location":"guide/how_to_contribute/#access-rights","text":"If you don't have write access to the repository you need to create a fork to contribute. Forking is easy. Click the \"Fork\" button in the top right hand corner of the Github UI.","title":"Access Rights"},{"location":"guide/how_to_contribute/#making-a-new-article","text":"1) Click \"Create New File\" 2) Name this file what you intend to name the article, plus the .md extension, i.e. how_to_contribute.md 3) Use classic markdown to set up the page title, i.e. \"# How to contribute\" 4) Write the rest of your text, checking the \"Preview\" tab to see how it would look. 5) Scroll to the bottom of the page and select the option \"create a new branch for this commit and start a pull request\". You can name a branch, though GitHub will set one automatically. If you know it, mention the issue that the request addresses. 6) Click \"Propose New File\". Proceed to the Making Multiple Edits or Opening a Pull Request section.","title":"Making a new article"},{"location":"guide/how_to_contribute/#editing-an-article","text":"1) Navigate to the article you want to edit. 2) Click the 'pencil' icon to edit the article. There's a built-in text editor in GitHub, where you can write a change and also why you changed it (so that a reviewer can understand the rationale for the change). 3) Select the branching method for making your change. (See Making Multiple Edits for clarification) 4) Click \"Propose File Change\". Proceed to the Making Multiple Edits or Opening a Pull Request section.","title":"Editing an article"},{"location":"guide/how_to_contribute/#making-multiple-edits-on-one-branch-pull-request","text":"A \"branch\" is a series of divergent changes from the main version. If you want to make multiple edits at once, you will need to make each of those changes on the same branch as you named your original edit. Check which branch you are on by looking for the \"Branch: ?\" dropdown. Use the dropdown to switch to your branch if you're on the wrong one.","title":"Making Multiple Edits On One Branch &amp; Pull Request"},{"location":"guide/how_to_contribute/#opening-a-pull-request","text":"1) Once redirected to the \"comparing changes\" page, prepend your pull request title with \"MDBOOK: \" and then a very short statement of what changed. 2) Add a more detailed description of what changes you made and why in the text box. 3) If there is an open issue related to the article you're submiting or editing, tag it by using the \"#\" plus the issue number. 4) Add the \"documentation\" label. 5) If appropriate, click \"Reviewers\" and select one or more people to request reviews from. 6) Click \"Create Pull Request\".","title":"Opening a Pull Request"},{"location":"guide/how_to_contribute/#reviewing-a-pull-request","text":"1) Under the Pull Request tab, look for ones starting with \"MDBOOK\". Go to the Pull Request of your choice, and then click on the \"Files Changed\" tab. 2) Start a review by hovering over a line and pressing the blue \"add\" symbol to add comments to a line 3) Click the green \"Review Changes\" button. If you approve of the changes, select \"Approve\". If you would like further changes to be made before it gets merged, select \"Request Changes\". If you are just weighing in, select \"Comment\". Then, click \"Submit Review\".","title":"Reviewing a Pull Request"},{"location":"guide/how_to_contribute/#merging-a-pull-request","text":"3) Under \"Conversation\" you can merge the pull request, which integrates it into the develop branch. Changes automatically deploy to https://holochain.github.io/holochain-rust within ~30 minutes. Merge the pull request once it has received two approved reviews.","title":"Merging a Pull Request"},{"location":"guide/how_to_contribute/#contributing-by-cloning-and-running-mdbook-advanced","text":"You will need to have cloned holochain-rust to your computer. You will also need Docker installed. There is a Docker build that allows local build, serve, watch and live reload for the book. From the root of the repo, run: . docker/build-mdbook-image && . docker/run-mdbook Once the book has built and is serving, visit http://localhost:3000 in the browser. You can edit the markdown files in doc/holochain_101/src and the book will live reload. To do a one-time build of the files to HTML, run: . docker/build-mdbook Edit the files to your satisfaction, commit them to a branch (or a fork) and then open a pull request on GitHub. Once its on GitHub, the same things as mentioned above apply.","title":"Contributing by Cloning and Running Mdbook (advanced)"},{"location":"guide/intro_to_command_line_tools/","text":"Intro to Command Line Tools \u00b6 There are a set of custom-designed tools for working with Holochain that can be installed as utilities to your command line to simplify and accelerate the process of Holochain app development. These command line tools are required if you wish to be able to attempt what you are reading about as you continue through the articles on building an app. However, the command line tools are automatically installed when setting up a development environment, as detailed in the quick start installation guide . In addition to the installation instructions, there is also an architectural overview of the different functions of the command line tools. You will also learn these organically just by proceeding through the other articles in this chapter and the ones that follow.","title":"Intro to Command Line Tools"},{"location":"guide/intro_to_command_line_tools/#intro-to-command-line-tools","text":"There are a set of custom-designed tools for working with Holochain that can be installed as utilities to your command line to simplify and accelerate the process of Holochain app development. These command line tools are required if you wish to be able to attempt what you are reading about as you continue through the articles on building an app. However, the command line tools are automatically installed when setting up a development environment, as detailed in the quick start installation guide . In addition to the installation instructions, there is also an architectural overview of the different functions of the command line tools. You will also learn these organically just by proceeding through the other articles in this chapter and the ones that follow.","title":"Intro to Command Line Tools"},{"location":"guide/intro_to_dna_code/","text":"Intro to DNA: Code \u00b6 The functionality of Holochain applications is written as a collection of logical modules called \"Zomes\". Zomes are created inside a folder called zomes , and each Zome should have its own sub-folder within that, in which the configuration and code for that particular Zome should be placed. These Zomes can call and access the functionality of the others, but they are written independently. When the DNA file is being packaged, the code for these Zomes is encoded using Base64 encoding and combined with the configuration file associated with the Zome. The configuration file should be a JSON file, stored in the Zome folder. The file can be named anything, but the default is zome.json . This Zome file is extremely simplistic at this point, and contains only a description property, which is a human readable property that describes what the Zome is for. The only coding language that Holochain knows how to execute is WebAssembly. However, it is unlikely that you'll want to write WebAssembly code by hand. Instead, most people will write their Zomes' code in a language that can compile to WebAssembly , such as Rust or Assemblyscript, and then define a build step in which it is compiled to WebAssembly. There is already a large, and growing, number of languages that compile to WebAssembly. If this is sounding complex, don't worry. There are tools supplied to make this easy, and you'll be writing in a language that's familiar, or easy to learn. With this overview in mind, the details of app development can be explored.","title":"Intro to DNA - Code"},{"location":"guide/intro_to_dna_code/#intro-to-dna-code","text":"The functionality of Holochain applications is written as a collection of logical modules called \"Zomes\". Zomes are created inside a folder called zomes , and each Zome should have its own sub-folder within that, in which the configuration and code for that particular Zome should be placed. These Zomes can call and access the functionality of the others, but they are written independently. When the DNA file is being packaged, the code for these Zomes is encoded using Base64 encoding and combined with the configuration file associated with the Zome. The configuration file should be a JSON file, stored in the Zome folder. The file can be named anything, but the default is zome.json . This Zome file is extremely simplistic at this point, and contains only a description property, which is a human readable property that describes what the Zome is for. The only coding language that Holochain knows how to execute is WebAssembly. However, it is unlikely that you'll want to write WebAssembly code by hand. Instead, most people will write their Zomes' code in a language that can compile to WebAssembly , such as Rust or Assemblyscript, and then define a build step in which it is compiled to WebAssembly. There is already a large, and growing, number of languages that compile to WebAssembly. If this is sounding complex, don't worry. There are tools supplied to make this easy, and you'll be writing in a language that's familiar, or easy to learn. With this overview in mind, the details of app development can be explored.","title":"Intro to DNA: Code"},{"location":"guide/intro_to_dna_config/","text":"Introduction to DNA: Configuration \u00b6 As a developer, you won't have to interact directly with the contents of a DNA file that often. However, it is quite important to grasp its role and structure. Holochain DNA files are written in a data format known as JSON. It stores sets of key-value pairs, and allows a nested tree structure. It looks like this: { \"property_name\" : \"property_value\" , \"nest_name\" : { \"nested_property_name\" : \"nested_property_value\" } } JSON is usually used for configuration and static data, but in the case of Holochain, these DNA files also contain compiled code, which is executable by Holochain. As previously mentioned, you do not need to edit this \"master\" DNA file directly. Holochain command line tools can be used to build it from your raw files. Learn more about the package command which fulfills this function Configuration \u00b6 For the configuration-related parts of your DNA, they will come from actual JSON files stored in your application folder. There will be multiple JSON files nested in the folder structure. An application folder should have a file in its root called app.json . This file should define various properties of your application. Some of these properties Holochain fully expects and will not work without, others can be customised to your application. app.json Properties \u00b6 A default app.json file looks roughly like this: { \"name\" : \"Holochain App Name\" , \"description\" : \"A Holochain app\" , \"authors\" : [ { \"identifier\" : \"Author Name <author@name.com>\" , \"public_key_source\" : \"\" , \"signature\" : \"\" } ], \"version\" : \"0.0.1\" , \"dht\" : {}, \"properties\" : null }","title":"Intro to DNA - Configuration"},{"location":"guide/intro_to_dna_config/#introduction-to-dna-configuration","text":"As a developer, you won't have to interact directly with the contents of a DNA file that often. However, it is quite important to grasp its role and structure. Holochain DNA files are written in a data format known as JSON. It stores sets of key-value pairs, and allows a nested tree structure. It looks like this: { \"property_name\" : \"property_value\" , \"nest_name\" : { \"nested_property_name\" : \"nested_property_value\" } } JSON is usually used for configuration and static data, but in the case of Holochain, these DNA files also contain compiled code, which is executable by Holochain. As previously mentioned, you do not need to edit this \"master\" DNA file directly. Holochain command line tools can be used to build it from your raw files. Learn more about the package command which fulfills this function","title":"Introduction to DNA: Configuration"},{"location":"guide/intro_to_dna_config/#configuration","text":"For the configuration-related parts of your DNA, they will come from actual JSON files stored in your application folder. There will be multiple JSON files nested in the folder structure. An application folder should have a file in its root called app.json . This file should define various properties of your application. Some of these properties Holochain fully expects and will not work without, others can be customised to your application.","title":"Configuration"},{"location":"guide/intro_to_dna_config/#appjson-properties","text":"A default app.json file looks roughly like this: { \"name\" : \"Holochain App Name\" , \"description\" : \"A Holochain app\" , \"authors\" : [ { \"identifier\" : \"Author Name <author@name.com>\" , \"public_key_source\" : \"\" , \"signature\" : \"\" } ], \"version\" : \"0.0.1\" , \"dht\" : {}, \"properties\" : null }","title":"app.json Properties"},{"location":"guide/intro_to_holochain_nodejs/","text":"Intro to holochain-nodejs \u00b6 The purpose of the holochain-nodejs module is to make integration tests and scenario tests able to be written simply and with as little boilerplate as possible. However, the module also provides even more basic functionality, making it possible to build tests with whatever tradeoff between convenience and customization is right for your project. There are two primary capabilities of the module, which are introduced below. Simple, Single Node Integration Tests \u00b6 The point of this mode of testing is simply to call Zome functions, and ensure that they produce the result you expect. It is discussed further in calling zome functions and checking results . Scenario Tests \u00b6 The point of this mode of testing is to launch multiple instances, call functions in one, then another, and to ensure that processes involving multiple agents play out as intended. The module conveniently provides a way to sandbox the execution of these scenarios as well, so that you can test multiple without worrying about side effects. This is discussed further in scenario testing .","title":"Intro to holochain-nodejs"},{"location":"guide/intro_to_holochain_nodejs/#intro-to-holochain-nodejs","text":"The purpose of the holochain-nodejs module is to make integration tests and scenario tests able to be written simply and with as little boilerplate as possible. However, the module also provides even more basic functionality, making it possible to build tests with whatever tradeoff between convenience and customization is right for your project. There are two primary capabilities of the module, which are introduced below.","title":"Intro to holochain-nodejs"},{"location":"guide/intro_to_holochain_nodejs/#simple-single-node-integration-tests","text":"The point of this mode of testing is simply to call Zome functions, and ensure that they produce the result you expect. It is discussed further in calling zome functions and checking results .","title":"Simple, Single Node Integration Tests"},{"location":"guide/intro_to_holochain_nodejs/#scenario-tests","text":"The point of this mode of testing is to launch multiple instances, call functions in one, then another, and to ensure that processes involving multiple agents play out as intended. The module conveniently provides a way to sandbox the execution of these scenarios as well, so that you can test multiple without worrying about side effects. This is discussed further in scenario testing .","title":"Scenario Tests"},{"location":"guide/intro_to_testing/","text":"Building Holochain Apps: Testing \u00b6 In order to provide a familiar testing framework, a nodejs version of the Holochain framework has been compiled using Rust to nodejs bindings. It is called \"holochain-nodejs\" and is a publicly installable package on the NPM package manager for nodejs. It enables the execution of Holochain and DNA instances from nodejs. At a basic level, here is how testing the Holochain DNA you are developing works: - Use the hc test command to run a series of steps optimal for testing - call a JS file containing tests - In the JS file, import the nodejs Holochain Conductor - load your packaged DNA into the Conductor, and otherwise configure it - use exposed methods on the Conductor to make function calls to the DNA - check that the results are what you expect them to be For checking the results, a basic JavaScript test framework called Tape has received priority support thus far, but other test frameworks can be used. You have the flexibility to write tests in quite a variety of ways, open to you to explore. This chapter will overview how to approach testing Holochain DNA.","title":"Building Holochain Apps: Testing"},{"location":"guide/intro_to_testing/#building-holochain-apps-testing","text":"In order to provide a familiar testing framework, a nodejs version of the Holochain framework has been compiled using Rust to nodejs bindings. It is called \"holochain-nodejs\" and is a publicly installable package on the NPM package manager for nodejs. It enables the execution of Holochain and DNA instances from nodejs. At a basic level, here is how testing the Holochain DNA you are developing works: - Use the hc test command to run a series of steps optimal for testing - call a JS file containing tests - In the JS file, import the nodejs Holochain Conductor - load your packaged DNA into the Conductor, and otherwise configure it - use exposed methods on the Conductor to make function calls to the DNA - check that the results are what you expect them to be For checking the results, a basic JavaScript test framework called Tape has received priority support thus far, but other test frameworks can be used. You have the flexibility to write tests in quite a variety of ways, open to you to explore. This chapter will overview how to approach testing Holochain DNA.","title":"Building Holochain Apps: Testing"},{"location":"guide/intro_to_toml_config/","text":"Intro to TOML Config Files \u00b6 To configure the holochain Conductor, a configuration file format called TOML is used. It stands for \"Tom's Obvious Minimal Language\" and was created by Tom Preston-Werner, one of the original founders of GitHub. The documentation on GitHub for it is very good. holochain configuration files make heavy use of tables and arrays of tables . A table is actually a collection of key/value pairs, and it looks like this: [table-1] key1 = \"some string\" key2 = 123 An array of tables looks like this: [[products]] name = \"Hammer\" sku = 738594937 [[products]] name = \"Nail\" sku = 284758393 color = \"gray\" This represents two \"product\" items in an array. In the following articles, how to configure the various properties of the holochain Conductor using these will be expanded on. First, knowing how to reference the configuration file for use by holochain will be covered below. holochain Config Files \u00b6 holochain requires a configuration file to run, which must exist in the default location, or be provided as an explicit argument. holochain will return an error if neither is given. The default location for the configuration file is in a subdirectory of the HOME directory on a device, at the path: ```toml Unix (Mac & Linux) \u00b6 $HOME/.holochain/conductor/conductor-config.toml Windows \u00b6 %HOME%.holochain\\conductor\\conductor-config.toml ``` When executing holochain in a terminal, a path to a configuration file can be given. This can be done with the following option: --config or for short -c This could look like: holochain -c ./conductor-config.toml The holochain-nodejs Conductor also accepts the same TOML based configuration. Examples \u00b6 To jump ahead into what these configuration files can look like, you can check out this folder on GitHub which has a number of examples. Otherwise, read on to understand each part.","title":"Intro to TOML Config Files"},{"location":"guide/intro_to_toml_config/#intro-to-toml-config-files","text":"To configure the holochain Conductor, a configuration file format called TOML is used. It stands for \"Tom's Obvious Minimal Language\" and was created by Tom Preston-Werner, one of the original founders of GitHub. The documentation on GitHub for it is very good. holochain configuration files make heavy use of tables and arrays of tables . A table is actually a collection of key/value pairs, and it looks like this: [table-1] key1 = \"some string\" key2 = 123 An array of tables looks like this: [[products]] name = \"Hammer\" sku = 738594937 [[products]] name = \"Nail\" sku = 284758393 color = \"gray\" This represents two \"product\" items in an array. In the following articles, how to configure the various properties of the holochain Conductor using these will be expanded on. First, knowing how to reference the configuration file for use by holochain will be covered below.","title":"Intro to TOML Config Files"},{"location":"guide/intro_to_toml_config/#holochain-config-files","text":"holochain requires a configuration file to run, which must exist in the default location, or be provided as an explicit argument. holochain will return an error if neither is given. The default location for the configuration file is in a subdirectory of the HOME directory on a device, at the path: ```toml","title":"holochain Config Files"},{"location":"guide/intro_to_toml_config/#unix-mac-linux","text":"$HOME/.holochain/conductor/conductor-config.toml","title":"Unix (Mac &amp; Linux)"},{"location":"guide/intro_to_toml_config/#windows","text":"%HOME%.holochain\\conductor\\conductor-config.toml ``` When executing holochain in a terminal, a path to a configuration file can be given. This can be done with the following option: --config or for short -c This could look like: holochain -c ./conductor-config.toml The holochain-nodejs Conductor also accepts the same TOML based configuration.","title":"Windows"},{"location":"guide/intro_to_toml_config/#examples","text":"To jump ahead into what these configuration files can look like, you can check out this folder on GitHub which has a number of examples. Otherwise, read on to understand each part.","title":"Examples"},{"location":"guide/json_rpc_http/","text":"HTTP \u00b6 Any coding language, or tool, which can make HTTP requests can make requests to a running DNA instance. Based on the API exposed by Holochain, these must be POST requests, use the \"application/json\" Content-Type, and follow the JSON-RPC standard. The HTTP example below will demonstrate how easy it is to make calls to a running DNA instance, just using the cURL tool for HTTP requests from a terminal. Any of these methods could be similarly called from whatever client you are using, whether that is JS in the browser, nodejs, Ruby or any other language. For maximum ease of use, we recommend searching for a JSON-RPC helper library for your language of choice, there are lots of good ones out there. Starting an HTTP Server with hc run \u00b6 hc run --interface http Starting an HTTP Server with holochain \u00b6 To review how to start an HTTP Server with holochain , review the interfaces article. HTTP Example \u00b6 This whole example assumes that one of the methods listed above has been used to start an HTTP server on port 8888 with a valid DNA instance running in it. info/instances \u00b6 The following is a starter example, where a special utility function of Holochain is called, which accepts no parameters, and returns an array of the instances which are available on the HTTP server. In another terminal besides the server, we could run the following cURL command: curl -X POST -H \"Content-Type: application/json\" -d '{\"jsonrpc\": \"2.0\",\"id\": \"0\",\"method\": \"info/instances\"}' http://localhost:8888 A response something like the following might be returned: { \"jsonrpc\" : \"2.0\" , \"result\" : [{ \"id\" : \"test-instance\" , \"dna\" : \"hc-run-dna\" , \"agent\" : \"hc-run-agent\" }], \"id\" : \"0\" } Calling Zome Functions \u00b6 The following discusses how to use cURL (and thus HTTP generally) to make calls to Zome functions. The JSON-RPC \"method\" to use is simply \"call\". The instance ID (as seen in the info/instances example), the Zome name, and the function name all need to be given as values in the \"params\" value of the JSON-RPC, in addition to the arguments to pass that function. This part of the \"params\" object might look like this: {\"instance_id\": \"test-instance\", \"zome\": \"blog\", \"function\": \"create_post\"} Unlike info/instances , Zome functions usually expect arguments. To give arguments, a JSON object should be constructed, and given as \"args\" key of the \"params\" value. It may look like the following: \"args\": {\"content\": \"sample content\"} Combining these, a request like the following could be made via cURL from a terminal: curl -X POST -H \"Content-Type: application/json\" -d '{\"id\": \"0\", \"jsonrpc\": \"2.0\", \"method\": \"call\", \"params\": {\"instance_id\": \"test-instance\", \"zome\": \"blog\", \"function\": \"create_post\", \"args\": { \"content\": \"sample content\"} }}' http://127.0.0.1:8888 A response like the following might be returned: { \"jsonrpc\" : \"2.0\" , \"result\" : \"{\\\"Ok\\\":\\\"QmUwoQAtmg7frBjcn1GZX5fwcPf3ENiiMhPPro6DBM4V19\\\"}\" , \"id\" : \"0\" } This response suggests that the function call was successful (\"Ok\") and provides the DHT address of the freshly committed blog entry (\"QmU...\"). This demonstrates how easy it is to call into Zome function from clients and user interfaces!","title":"HTTP"},{"location":"guide/json_rpc_http/#http","text":"Any coding language, or tool, which can make HTTP requests can make requests to a running DNA instance. Based on the API exposed by Holochain, these must be POST requests, use the \"application/json\" Content-Type, and follow the JSON-RPC standard. The HTTP example below will demonstrate how easy it is to make calls to a running DNA instance, just using the cURL tool for HTTP requests from a terminal. Any of these methods could be similarly called from whatever client you are using, whether that is JS in the browser, nodejs, Ruby or any other language. For maximum ease of use, we recommend searching for a JSON-RPC helper library for your language of choice, there are lots of good ones out there.","title":"HTTP"},{"location":"guide/json_rpc_http/#starting-an-http-server-with-hc-run","text":"hc run --interface http","title":"Starting an HTTP Server with hc run"},{"location":"guide/json_rpc_http/#starting-an-http-server-with-holochain","text":"To review how to start an HTTP Server with holochain , review the interfaces article.","title":"Starting an HTTP Server with holochain"},{"location":"guide/json_rpc_http/#http-example","text":"This whole example assumes that one of the methods listed above has been used to start an HTTP server on port 8888 with a valid DNA instance running in it.","title":"HTTP Example"},{"location":"guide/json_rpc_http/#infoinstances","text":"The following is a starter example, where a special utility function of Holochain is called, which accepts no parameters, and returns an array of the instances which are available on the HTTP server. In another terminal besides the server, we could run the following cURL command: curl -X POST -H \"Content-Type: application/json\" -d '{\"jsonrpc\": \"2.0\",\"id\": \"0\",\"method\": \"info/instances\"}' http://localhost:8888 A response something like the following might be returned: { \"jsonrpc\" : \"2.0\" , \"result\" : [{ \"id\" : \"test-instance\" , \"dna\" : \"hc-run-dna\" , \"agent\" : \"hc-run-agent\" }], \"id\" : \"0\" }","title":"info/instances"},{"location":"guide/json_rpc_http/#calling-zome-functions","text":"The following discusses how to use cURL (and thus HTTP generally) to make calls to Zome functions. The JSON-RPC \"method\" to use is simply \"call\". The instance ID (as seen in the info/instances example), the Zome name, and the function name all need to be given as values in the \"params\" value of the JSON-RPC, in addition to the arguments to pass that function. This part of the \"params\" object might look like this: {\"instance_id\": \"test-instance\", \"zome\": \"blog\", \"function\": \"create_post\"} Unlike info/instances , Zome functions usually expect arguments. To give arguments, a JSON object should be constructed, and given as \"args\" key of the \"params\" value. It may look like the following: \"args\": {\"content\": \"sample content\"} Combining these, a request like the following could be made via cURL from a terminal: curl -X POST -H \"Content-Type: application/json\" -d '{\"id\": \"0\", \"jsonrpc\": \"2.0\", \"method\": \"call\", \"params\": {\"instance_id\": \"test-instance\", \"zome\": \"blog\", \"function\": \"create_post\", \"args\": { \"content\": \"sample content\"} }}' http://127.0.0.1:8888 A response like the following might be returned: { \"jsonrpc\" : \"2.0\" , \"result\" : \"{\\\"Ok\\\":\\\"QmUwoQAtmg7frBjcn1GZX5fwcPf3ENiiMhPPro6DBM4V19\\\"}\" , \"id\" : \"0\" } This response suggests that the function call was successful (\"Ok\") and provides the DHT address of the freshly committed blog entry (\"QmU...\"). This demonstrates how easy it is to call into Zome function from clients and user interfaces!","title":"Calling Zome Functions"},{"location":"guide/json_rpc_interfaces/","text":"Intro to JSON-RPC Interfaces \u00b6 The JSON-RPC interface will expose, via a port on your device, a WebSocket or an HTTP server, via which you can make function calls to the Zomes of your DNA. JSON-RPC \u00b6 JSON-RPC is a specification for using the JSON data format in a particular way, that follows the \"Remote Procedure Call\" pattern. Holochain uses the Version 2 specification of JSON-RPC. You can see general examples of JSON-RPC here . The format for the JSON-RPC request/response pattern is really simple. A request is a JSON object with just a few mandatory values which must be passed. jsonrpc : specifies the JSON-RPC spec this request follows. The JSON-RPC spec used by Holochain Conductors is 2.0 . id : specifies the ID for this particular request. This is so that the request and response can be matched, even if they get transmitted out of order. method : specifies the method on the \"remote\" (Holochain) to call. params : (optional) contains a JSON object which holds the data to be given as arguments to the method being called, if the method expects them.","title":"Intro to JSON-RPC Interfaces"},{"location":"guide/json_rpc_interfaces/#intro-to-json-rpc-interfaces","text":"The JSON-RPC interface will expose, via a port on your device, a WebSocket or an HTTP server, via which you can make function calls to the Zomes of your DNA.","title":"Intro to JSON-RPC Interfaces"},{"location":"guide/json_rpc_interfaces/#json-rpc","text":"JSON-RPC is a specification for using the JSON data format in a particular way, that follows the \"Remote Procedure Call\" pattern. Holochain uses the Version 2 specification of JSON-RPC. You can see general examples of JSON-RPC here . The format for the JSON-RPC request/response pattern is really simple. A request is a JSON object with just a few mandatory values which must be passed. jsonrpc : specifies the JSON-RPC spec this request follows. The JSON-RPC spec used by Holochain Conductors is 2.0 . id : specifies the ID for this particular request. This is so that the request and response can be matched, even if they get transmitted out of order. method : specifies the method on the \"remote\" (Holochain) to call. params : (optional) contains a JSON object which holds the data to be given as arguments to the method being called, if the method expects them.","title":"JSON-RPC"},{"location":"guide/json_rpc_websockets/","text":"WebSockets \u00b6 Any coding language which has WebSockets support can communicate with the WebSocket server interface for Holochain. Based on the API exposed by Holochain, the messages must follow the JSON-RPC standard. We recommend searching for a JSON-RPC Websockets library for the language of your choice. In this example, we will use a Javascript based JSON-RPC library. Starting a WebSocket Server with hc run \u00b6 hc run Starting a WebSocket Server with holochain \u00b6 To review how to start a WebSocket Server with holochain , check out the interfaces article. WebSocket Example \u00b6 This whole example assumes that one of the methods listed above has been used to start a WebSocket server on port 8888 with a valid DNA instance running in it. The JavaScript JSON-RPC library this example will use is rpc-websockets . The overall pattern this example illustrates should be very similar for other languages. For nodejs, and using NPM, install the rpc-websockets package by running: npm install rpc-websockets The following code snippet just does the setup for interacting with your running DNA instance: // import the rpc-websockets library let WebSocket = require ( 'rpc-websockets' ). Client // instantiate Client and connect to an RPC server let holochainUri = 'ws://localhost:8888' let ws = new WebSocket ( holochainUri ) // create an event listener, and a callback, for when the socket connection opens ws . on ( 'open' , function () { // do stuff in here }) info/instances \u00b6 The following is a starter example, where a special utility function of Holochain is called, which accepts no parameters, and returns an array of the instances which are available on the WebSocket server. The name of this special method is info/instances . The following code shows how to use rpc-websockets to call it. (Note the previous code is collapsed in the ellipsis for brevity) ... ws . on ( 'open' , function () { let method = 'info/instances' let params = {} // call an RPC method with parameters ws . call ( method , params ). then ( result => { console . log ( result ) }) }) If this code was run in nodejs, the output should be: [ { id: 'test-instance' , dna: 'hc-run-dna' , agent: 'hc-run-agent' } ] Calling Zome Functions \u00b6 The following discusses how to use rpc-websockets to make calls to Zome functions. The JSON-RPC \"method\" to use is simply \"call\". The instance ID (as seen in the info/instances example), the Zome name, and the function name all need to be given as values in the \"params\" value of the JSON-RPC, in addition to the arguments to pass that function. This part of the \"params\" object might look like this: {\"instance_id\": \"test-instance\", \"zome\": \"blog\", \"function\": \"create_post\"} Unlike info/instances , Zome functions usually expect arguments. To give arguments, a JSON object should be constructed, and given as \"args\" key of the \"params\" value. It may look like the following: { blog: { content: \"sample content\" }} The following code shows how to use rpc-websockets to call Zome functions. ... ws . on ( 'open' , function () { let method = 'call' let params = { instance_id : \"test-instance\" , zome : \"blog\" , function : \"create_post\" , args : { content : \"sample content\" } } // call an RPC method with parameters ws . call ( method , params ). then ( result => { console . log ( result ) }) }) If this code was run in nodejs, the output should be: { \"Ok\" : \"QmRjDTc8ZfnH9jucQJx3bzK5Jjcg21wm5ZNYAro9N4P7Bg\" } This response suggests that the function call was successful (\"Ok\") and provides the DHT address of the freshly committed blog entry (\"QmR...\"). Closing the WebSocket Connection \u00b6 When you are done permanently with the connection, it can be closed. ... // close a websocket connection ws.close() All in all, calling into Zome functions from clients and user interfaces is easy! hc-web-client \u00b6 To make it even easier in particular for web developers, there is a simple JavaScript library called hc-web-client developed which wraps the rpc-websockets library. Find it here, with instructions on how to use it: hc-web-client","title":"WebSockets"},{"location":"guide/json_rpc_websockets/#websockets","text":"Any coding language which has WebSockets support can communicate with the WebSocket server interface for Holochain. Based on the API exposed by Holochain, the messages must follow the JSON-RPC standard. We recommend searching for a JSON-RPC Websockets library for the language of your choice. In this example, we will use a Javascript based JSON-RPC library.","title":"WebSockets"},{"location":"guide/json_rpc_websockets/#starting-a-websocket-server-with-hc-run","text":"hc run","title":"Starting a WebSocket Server with hc run"},{"location":"guide/json_rpc_websockets/#starting-a-websocket-server-with-holochain","text":"To review how to start a WebSocket Server with holochain , check out the interfaces article.","title":"Starting a WebSocket Server with holochain"},{"location":"guide/json_rpc_websockets/#websocket-example","text":"This whole example assumes that one of the methods listed above has been used to start a WebSocket server on port 8888 with a valid DNA instance running in it. The JavaScript JSON-RPC library this example will use is rpc-websockets . The overall pattern this example illustrates should be very similar for other languages. For nodejs, and using NPM, install the rpc-websockets package by running: npm install rpc-websockets The following code snippet just does the setup for interacting with your running DNA instance: // import the rpc-websockets library let WebSocket = require ( 'rpc-websockets' ). Client // instantiate Client and connect to an RPC server let holochainUri = 'ws://localhost:8888' let ws = new WebSocket ( holochainUri ) // create an event listener, and a callback, for when the socket connection opens ws . on ( 'open' , function () { // do stuff in here })","title":"WebSocket Example"},{"location":"guide/json_rpc_websockets/#infoinstances","text":"The following is a starter example, where a special utility function of Holochain is called, which accepts no parameters, and returns an array of the instances which are available on the WebSocket server. The name of this special method is info/instances . The following code shows how to use rpc-websockets to call it. (Note the previous code is collapsed in the ellipsis for brevity) ... ws . on ( 'open' , function () { let method = 'info/instances' let params = {} // call an RPC method with parameters ws . call ( method , params ). then ( result => { console . log ( result ) }) }) If this code was run in nodejs, the output should be: [ { id: 'test-instance' , dna: 'hc-run-dna' , agent: 'hc-run-agent' } ]","title":"info/instances"},{"location":"guide/json_rpc_websockets/#calling-zome-functions","text":"The following discusses how to use rpc-websockets to make calls to Zome functions. The JSON-RPC \"method\" to use is simply \"call\". The instance ID (as seen in the info/instances example), the Zome name, and the function name all need to be given as values in the \"params\" value of the JSON-RPC, in addition to the arguments to pass that function. This part of the \"params\" object might look like this: {\"instance_id\": \"test-instance\", \"zome\": \"blog\", \"function\": \"create_post\"} Unlike info/instances , Zome functions usually expect arguments. To give arguments, a JSON object should be constructed, and given as \"args\" key of the \"params\" value. It may look like the following: { blog: { content: \"sample content\" }} The following code shows how to use rpc-websockets to call Zome functions. ... ws . on ( 'open' , function () { let method = 'call' let params = { instance_id : \"test-instance\" , zome : \"blog\" , function : \"create_post\" , args : { content : \"sample content\" } } // call an RPC method with parameters ws . call ( method , params ). then ( result => { console . log ( result ) }) }) If this code was run in nodejs, the output should be: { \"Ok\" : \"QmRjDTc8ZfnH9jucQJx3bzK5Jjcg21wm5ZNYAro9N4P7Bg\" } This response suggests that the function call was successful (\"Ok\") and provides the DHT address of the freshly committed blog entry (\"QmR...\").","title":"Calling Zome Functions"},{"location":"guide/json_rpc_websockets/#closing-the-websocket-connection","text":"When you are done permanently with the connection, it can be closed. ... // close a websocket connection ws.close() All in all, calling into Zome functions from clients and user interfaces is easy!","title":"Closing the WebSocket Connection"},{"location":"guide/json_rpc_websockets/#hc-web-client","text":"To make it even easier in particular for web developers, there is a simple JavaScript library called hc-web-client developed which wraps the rpc-websockets library. Find it here, with instructions on how to use it: hc-web-client","title":"hc-web-client"},{"location":"guide/json_string/","text":"Serialization and JsonString \u00b6 Why serialize anything? Why JSON? \u00b6 Holochain zomes are written in WASM. \u00b6 WASM only supports working directly with integers and manually allocating memory. This means that sharing any data between holochain core and zome functions must be serialized. There is no way that WASM functions can understand the Rust type system natively. Serialized data can be allocated for WASM to read out and deserialize into Rust types/structs/enums. Any developers using the Rust HDK get the serialization/deserialization and type handling almost \"for free\". The macros for defining entities and zomes automatically wrap the memory work and serialization round trips for anything that implements Into<JsonString> and TryFrom<JsonString> (see below). We use serde for our serialization round trips as it is by far the most popular and mature option for Rust. Many serialization formats other than JSON are supported by serde but JSON is a solid option. JSON allows us to easily bring the Rust type system across to WASM with decent performance. From the serde_json github repository README: It is fast. You should expect in the ballpark of 500 to 1000 megabytes per second deserialization and 600 to 900 megabytes per second serialization, depending on the characteristics of your data. This is competitive with the fastest C and C++ JSON libraries or even 30% faster for many use cases. Benchmarks live in the serde-rs/json-benchmark repo. Holochain aims to support all WASM languages not just Rust/JS \u00b6 The official Holochain HDK is Rust. The Rust HDK will always be the most tightly integrated HDK with core simply because Holochain itself is Rust based. Generally though, we are hoping and expecting many different WASM zome languages build an ecosystem over time. Personally I'm hoping for a decent LISP to appear ;) To encourage as many languages as possible we want to keep the minimum requirements for interacting with holochain core as minimal as possible. Currently the two requirements for writing zomes in <your favourite language> : Must compile to WASM Must be able to serialize UTF-8 data and allocate to memory read by core We can't do much about the first requirement but here are some lists to watch: https://github.com/appcypher/awesome-wasm-langs https://github.com/mbasso/awesome-wasm The second requirement means that we must be very mindful of choosing a serialization format that can round trip through as many languages as possible. In the end, this is the main reason we chose JSON for communication with core. Note that at the time of writing, the AssemblyScript (ostentisbly JavaScript) WASM implementation does not even provide a native JSON.parse() method! To do something as apparently simple as serialize JSON in JavaScript we have had to implement a custom JSON parser. At least JSON (naturally) maps very well to JavaScript native data, other serialization/language combinations are even further from maturity. WASM is very promising but very immature so esoteric serialization options are not really viable options right now, even if serde supports them in Rust. JSON serialization only pertains to communication with core \u00b6 Holochain often makes a distinction between \"app data\" and \"core data\". Following the biomimicry theme we sometimes call this \"conscious\" vs. \"subconscious\" when this data is used in zomes or core logic respectively. The most obvious example of this is the Entry enum that has an Entry::App variant explicitly for app data, and other variants for system logic. The Entry enum itself is serialized via JSON so that is has maximal compatibility across all zome languages (see above) across the core/wasm boundary. However, the contents of Entry::App(..) are treated as an opaque UTF-8 string by Holochain core. Naturally the HDK macros we offer provide sugar to work with the value of app entries but this is not enforced anywhere within core. Because the Rust serialization round tripping must work across both core and the HDK it must work equally well while treating the app entry values as opaque in the subconscious and meaningful structs in the conscious. This is achieved through a healthy dose of compiler and macro magic. This means that zome developers can implement their own serialization logic for their own data if they wish. Simply by wrapping a zome-serialized app entry value in \"\\\"...\\\"\" it becomes a string primitive from core's perspective. The zome can do anything needed with this, including custom validation logic, etc. The RawString type handles this automatically with JsonString (see below). Serialization through Rust types \u00b6 How Rust serializes: serde from 1000m \u00b6 The serde crate leans heavily on the Rust compiler for serialization round tripping. Using the \"vanilla\" serde_json crate affords this logic on the way in: let foo_json = serde_json :: to_string ( foo ). unwrap (); Notes: There is an unwrap but this can't fail for simple structs/enums in practise The unwrap can fail e.g. serializing streams but we don't do that The compiler enforces that everything we pass to serde can Serialize foo can be anything that implements Serialize we have no direct control over the structure of the JSON output the Serialize implementation of foo decides this for us in the case of nested data e.g. hash maps, Serialize works recursively OR using the manual json! macro: let foo_json = json ! ({ \"foo\" : foo . inner ()}); Notes: We no longer have an unwrap so there is slightly less boilerplate to type We have a lot of direct control over the structure of our output JSON For better or worse we avoid what the compiler says about Serialize on Foo We must now manually ensure that \"{\\\"foo\\\":...}\" is handled everywhere Including in crates we don't control Including when we change our JSON structure across future releases Including across WASM boundaries in HDK consumers AND on the way out: let foo : Foo = Foo :: try_from ( & hopefully_foo_json ) ? ; Notes: Serde relies on compiler info, the type Foo on the left, to deserialize Serde requires that hopefully_foo_json makes sense as Foo This definitely can fail as the json is just a String to the compiler In real code do not unwrap this, handle the Err carefully! JSON structure, the Rust compiler and you \u00b6 All this means that our JSON data MUST closely align with the types we define for the compiler. There is a lot of flexibility offered by serde for tweaking the output (e.g. lowercasing names of things, modifying strings, etc.) but the tweaks involve a lot of boilerplate and have limits. For example this can be awkard when handling Result values. The Result enum has two variants in Rust, Ok and Err . Both of these, like all enum variants in Rust, follow the title case convention. This means that in a JS conductor/HDK consuming JSON values returned from zome functions that return a Result (a good idea!) we see this JavaScript: const result = app . call (...) const myVar = result . Ok ... We get a result.Ok rather than the result.ok that we'd expect from idiomatic JavaScript. As the JSON structure comes from the Rust compiler, we have two options: Force serde to output JSON that follows the conventions of another language Force conductors/HDKs to provide sugar to map between Rust/XXX idioms Force developers to work with a very leaky abstraction over the Rust compiler As the first option requires a lot of boilerplate and isn't interoperable across all languages anyway (e.g. kebab case, snake case, etc.) we currently are pushing this sugar down to conductor/HDK implementations. Additionally, the serialized form of entries is used to calculate Address values for storage and retrieval from the local chain and DHT so we need to be very careful here as it will be hard to change in the future. That said, we are open to constructive feedback on what this sugar looks like and how it works! Ideally zome development is as idiomatic as possible across as many languages as possible \ud83d\udd76 Binary data as base64 \u00b6 We recommend base64 encoding binary data straight into an app entry string that you can use in your zome logic directly (see above). Yes this uses more space than binary data, 33% more to be specific :( But there are benefits: It is UTF-8 and web (e.g. data URI) friendly Simply wrapped in \"\\\"..\\\"\" it becomes valid JSON (see RawString below) It has wide language support (see above for why this is important) It will be supported by all persistence backends for the forseeable future At least these storage systems require base64 encoded data at some point: Browser based localStorage MongoDB Elasticsearch Amazon SimpleDB Amazon DynamoDB. The performance penalty can be minimal: https://lemire.me/blog/2018/01/17/ridiculously-fast-base64-encoding-and-decoding/ JSON is lame! Can Holochain support <my favourite serialization format> ? \u00b6 Yes... and no... It depends what you mean by \"support\". Right now, most serialization formats are supported in app/zome data simply by wrapping the output in double quotes so core sees it as a JSON string literal. Holochain core won't try to interpret/mangle any of that data so the zome can theoretically do whatever it wants at that point without a performance hit. In practise, there are some limitations as mentioned in this doc: WASM languages tend to have no or limited serialization options you may need to roll your own parse/stringify logic seriously... e.g. we pushed our own JSON.parse implementation upstream for the AssemblyScript team, that's JSON parsing in JavaScript ! don't underestimate how bleeding edge and limited the WASM tooling still is to work directly with WASM you must be prepared to bleed If you don't use JSON you can't use hdk macros for that part of your zome Only valid UTF-8 strings are supported (may change in the future) If you're looking for a way to provide core data in non-JSON format then NO that is not supported and won't be in the short-mid term future. Yes, serde supports many serialization options but: Not all data in core uses default serde serialization logic e.g. this document explaining non-default serde serialization logic Swapping to a different serializer in serde is not just a matter of passing config to serde we'd have to centralise/ match everywhere and swap out serde_json for analogous crates in each other format we'd want to use even using a SerialString instead of JsonString (see below) would not clear out every implementation without a lot of work Serde is already quite heavy in compilation/WASM files so we don't want to bloat that more with edge-case serialization needs every new format is a new crate We don't (yet) have any use-cases showing that JSON is a problem/bottleneck Adding more serialization options would exacerbate non-idiomatic conductor and HDK data structure mapping issues (see above) JsonString \u00b6 The problem and our solution \u00b6 Sometimes we want to nest serialization (e.g. hdk::call ) and sometimes we want to wrap serialization (e.g. Entry::App ), sometimes converting to a string uses entirely different logic (e.g. error values). Ideally we want the compiler to guide us through this process as mistakes are common and difficult to debug. We also want serialization logic to be as invisible as possible to zome developers using our HDKs. Serde will serialize anything that implements Serialize , including String so we added a type JsonString that does not automatically round trip to act as a logical \"checkpoint\" in our code. JsonString doesn't \"do\" anything beyond giving ourselves and the compiler a shared target while stepping through the serialization round trip. Essentially we trade this: // foo_a is a Foo // foo_json is a String // Foo implements Serialize and Deserialize let foo_json = serde_json :: to_string ( & foo_a ) ? ; let foo_b : Foo = serde_json :: from_str ( & foo_json ) ? ; for this: // foo_a is a Foo // JsonString implements From<Foo> // Foo implements TryFrom<JsonString> let foo_json = JsonString :: from ( foo_a ); let foo_b = Foo :: try_from ( hopefully_foo_json ) ? ; Which looks very similar but protects us from this bug: let foo_json = serde_json :: to_string ( & foo_a ) ? ; let foo_json = serde_json :: to_string ( & foo_json ) ? ; // <-- double serialized :/ let foo_b : Foo = serde_json :: from_str ( & foo_json ) ? ; // <-- will fail :( Because nesting JsonString::from() calls is a compiler error: let foo_json = JsonString :: from ( JsonString :: from ( foo_a )); // <-- compiler saves us :) and this bug: let foo_a : Foo = serde_json :: from_str ( & string_but_not_json ) ? ; // <-- runtime error :( Because calling Foo::try_from(String) is (probably) a compiler error: let foo_a = Foo :: try_from ( string_but_not_json ) ? ; // <-- compiler saves us again :) and this bug: type Foo = Result < String , String > ; let foo_json_a = json ! ({ \"Err\" : some_error . to_string ()}); // <-- good key `Err` // somewhere else... maybe a different crate or old crate version... let foo_json_b = json ! ({ \"error\" : some_error . to_string ()}); // <-- bad key `error` :/ let foo : Foo = serde_json :: from ( & foo_json_a ) ? ; // <-- works, key matches variant name let foo : Foo = serde_json :: from ( & foo_json_b ) ? ; // <-- runtime error! :( Because the structure of the JSON data is defined centrally at compile time: // Result<Into<JsonString>, Into<JsonString>> is implemented for you by HC core let foo_json_a = JsonString :: from ( Err ( some_error . to_string ())); // only one way to do things, automatically consistent across all crates // doing anything different is a compiler issue let foo_json_b = JsonString :: from ( Err ( some_error . to_string ())); Which is great for the majority of data that needs serializing. There are some important edge cases that we need to cover with additional techniques/tooling. String handling \u00b6 JsonString::from_json(&str) requires the &str passed to it is already a serialized JSON value. We may add the option to validate this for debug builds at runtime in the future. Previously JsonString implemented the From<String> trait but this was removed. Strings are a special case as they may either contain serialized json or be used as a JSON string primitive. JsonString::from_json makes it explicit that you mean the former. We can use serde_json::to_string and json! to create JSON data that we can then wrap in JsonString . // same end result for both of these... let foo_json = JsonString :: from_json ( & serde_json :: to_string ( & foo )); let foo_json = JsonString :: from ( foo ); More commonly useful, we can move back and forward between String and JsonString without incurring serialization overhead or human error: // this does a round trip through types without triggering any serde JsonString :: from_json ( & String :: from ( JsonString :: from ( foo ))); This is helpful when a function signature requires a String or JsonString argument and we have the inverse type. It also helps when manually building JSON data by wrapping already serialized data e.g. with format! . An example taken from core: fn result_to_json_string < T : Into < JsonString > , E : Into < JsonString >> ( result : Result < T , E > , ) -> JsonString { let is_ok = result . is_ok (); let inner_json : JsonString = match result { Ok ( inner ) => inner . into (), Err ( inner ) => inner . into (), }; let inner_string = String :: from ( inner_json ); JsonString :: from_json ( & format ! ( \"{{ \\\" {} \\\" :{}}}\" , if is_ok { \"Ok\" } else { \"Err\" }, inner_string )) } impl < T : Into < JsonString > , E : Into < JsonString > + JsonError > From < Result < T , E >> for JsonString { fn from ( result : Result < T , E > ) -> JsonString { result_to_json_string ( result ) } } Which looks like this: let result : Result < String , HolochainError > = Err ( HolochainError :: ErrorGeneric ( \"foo\" . into ())); assert_eq ! ( JsonString :: from ( result ), JsonString :: from ( \"{ \\\" Err \\\" :{ \\\" ErrorGeneric \\\" : \\\" foo \\\" }}\" ), ); When given a Result containing any value that can be turned into a JsonString (see below), we can convert it first, then wrap it with String::from + format! . String serialization \u00b6 Sometimes we want a String to be serialized as a JSON string primitive rather than simply wrapped in a JsonString struct. JsonString::from won't do what we need because it always wraps strings, we need to nest the String serialization. let foo = String :: from ( JsonString :: from_json ( \"foo\" )); // \"foo\" = not what we want let foo = ??? ; // \"\\\"foo\\\"\" = what we want To keep the type safety from JsonString and nest String serialization use RawString wrapped in JsonString . RawString wraps String and serializes it to a JSON string primitive when JsonString ified. // does what we need :) let foo = String :: from ( JsonString :: from ( RawString :: from ( \"foo\" ))); // \"\\\"foo\\\"\" An example of this can be seen in the core version of the Result serialization from above that deals with String error values: impl < T : Into < JsonString >> From < Result < T , String >> for JsonString { fn from ( result : Result < T , String > ) -> JsonString { let is_ok = result . is_ok (); let inner_json : JsonString = match result { Ok ( inner ) => inner . into (), // strings need this special handling c.f. Error Err ( inner ) => RawString :: from ( inner ). into (), // <-- RawString here! }; let inner_string = String :: from ( inner_json ); format ! ( \"{{ \\\" {} \\\" :{}}}\" , if is_ok { \"Ok\" } else { \"Err\" }, inner_string ) . into () } } Which looks like this: let result : Result < String , String > = Err ( String :: from ( \"foo\" )); assert_eq ! ( JsonString :: from ( result ), JsonString :: from ( \"{ \\\" Err \\\" : \\\" foo \\\" }\" ), ) If we didn't do this then the format! would return invalid JSON data with the String error value missing the wrapping double quotes. RawString is useful when working with types that have a .to_string() method or similar where the returned string is not valid JSON. Examples of when RawString could be useful: Error descriptions that return plain text in a string Base64 encoded binary data Enum variants with custom string representations \"Black boxing\" JSON data that Rust should not attempt to parse Implementing JsonString for custom types \u00b6 As mentioned above, there are two trait implementations that every struct or enum should implement to be compatible with core serialization logic: impl From<MyType> for JsonString to serialize MyType impl TryFrom<JsonString> for MyType to attempt to deserialize into MyType Note that TryFrom is currently an unstable Rust feature. To enable it add !#[feature(try_from)] to your crate/zome. Based on discussions in the Rust community issue queues/forums, we expect this feature to eventually stabilise and no longer require feature flags to use. The TryFrom trait will need to be added as use std::convert::TryFrom to each module/zome implementing it for a struct/enum. Boilerplate \u00b6 To defer all the logic to standard serde defaults with some sensible debug logic in the case of an error, there are two utility functions in core, default_to_json and default_try_from_json . The standard minimal boilerplate looks like this: struct MyType {} impl From < MyType > for JsonString { fn from ( my_type : MyType ) -> Self { default_to_json ( my_type ) } } impl TryFrom < JsonString > for MyType { type Error = HolochainError ; fn try_from ( json_string : JsonString ) -> Result < Self , Self :: Error > { default_try_from_json ( json_string ) } } Automatic derive \u00b6 The standard boilerplate has been implemented as a derive macro in the holochain_persistence_derive crate. Simply #[derive(DefaultJson)] to add the above boilerplate plus some extra conveniences (e.g. for references) to your type. DefaultJson requires: JsonString is included HolochainError is included MyType implements Serialize , Deserialize and Debug from serde/std use holochain_persistence_api :: json :: JsonString ; use holochain_core_types :: error :: HolochainError ; #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct MyType {} Using JsonString as the property of a struct/enum \u00b6 Because JsonString cannot automatically be round tripped with Serialize and Deserialize , the following can cause difficulty: #[derive(Serialize, Deserialize)] struct Foo { bar : JsonString , } The compiler will complain about this because anything deriving Serialize recursively must consist only of values that also implement Serialize . There are a few approaches here, each with benefits and tradeoffs. Swap out JsonString with String Use a serde attribute to manually serialize Bar Use a serde attribute to skip Bar Create a \"new type\" or wrapper/conversion struct Swap JsonString with String \u00b6 This approach is quick and dirty. Simply change the type of Bar to String . When prototyping or on deadline, this might be the most attractive option ;) This will likely cause problems upstream and downstream of what you are doing, or may be symptomatic of poorly handled JSON somewhere. This is roughly how Entry used to work, with a String valued SerializedEntry and JsonString valued Entry that could be swapped between using a From implementation. Done correctly we can \"onboard\" values to Foo by simply carefully wrapping and unwrapping the String . Done badly, we reintroduce the possibility for invalid wrap/nest/etc. logic to creep in. This works best when the fields on Foo are private and immutable, exposed only through getter/setter/new style methods that internally convert between JsonString and String . This option is less suitable if we want to double serialize the nested JSON data when serializing Foo . For an example of where we preserve JSON rather than trying to automatically deserialize or wrap it with structs, see the return values from hdk::call (not using structs, but similar ideas). Also consider that somebody reading your code might entirely miss the fact that Foo::bar is JSON data if all they read is the struct definition. It may be worthwhile adding methods to Foo to enforce this: #[derive(Serialize, Deserialize)] pub struct Foo { bar : String , } impl Foo { pub fn new ( bar : JsonString ) -> Foo { Foo { bar : String :: from ( bar )} } pub fn bar ( & self ) -> JsonString { JsonString :: from ( self . bar . clone ()) } } Treat bar as though it was going to be stored as a JsonString right until the last moment. Avoid this: let bar_json = json ! ({ \"bar\" : bar . inner ()}). to_string (); // somwhere later... let foo = Foo { bar : bar_json }; Because then everything that needs to use Foo must consistently implement the manual jsonification logic. This is especially important if Foo and/or bar is to be used across multiple crates. Instead, prefer this: #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Bar { bar : .. } let bar_json = JsonString :: from ( Bar { bar : ..}); let foo = Foo :: new ( bar ); // assuming impl Foo::new from above The result is still a raw String in Foo but the validity and consistency of the JSON data is enforced across all crates by JsonString::from(bar) . It is even possible to internalise the JsonString completely within the Foo methods using Into<JsonString> . This is covered in more detail below. Using serde attributes \u00b6 Serde allows us to set serialization logic at the field level for structs. The best example of this is handling of AppEntryValue in core. As all zome data is treated as JSON, assumed to line up with internal structs in the HDK but potentially opaque string primitives (see above) we simply alias AppEntryValue to JsonString . The Entry enum needs to be serialized for many reasons in different contexts, including for system entries that zome logic never handles directly. It looks something like this (at the time of writing): #[derive(Clone, Debug, Serialize, Deserialize, DefaultJson)] pub enum Entry { #[serde(serialize_with = \"serialize_app_entry\" )] #[serde(deserialize_with = \"deserialize_app_entry\" )] App ( AppEntryType , AppEntryValue ), Dna ( Dna ), AgentId ( AgentId ), Delete ( Delete ), LinkAdd ( LinkAdd ), LinkRemove ( LinkRemove ), LinkList ( LinkList ), ChainHeader ( ChainHeader ), ChainMigrate ( ChainMigrate ), } Note that Entry : Derives Serialize and Deserialize and even DefaultJson ! Contains AppEntryValue in a tuple, which is a JsonString Uses some serde serialization attributes This works because the serialization attributes tell serde how to handle the JsonString in this context . This is a double edged sword. We have explicit control over the serialization so we can never accidentally wrap/nest/etc. JSON data in an invalid way. We also only define the serialization for this type in this one place. If AppEntryValue was used in some other struct/enum, we would have to manually remember to use the same or compatible serialize/deserialize callbacks. This approach also gives a lot of control over the final JSON structure. We can avoid stutters and reams of redundant data in the final output. This can mitigate the verbosity and awkwardness of compiler-driven JSON structures when sending data to other languages (see above). The serde documentation explains in great (technical) detail how to implement custom serialization and deserialization logic for many different data types: https://serde.rs/field-attrs.html For reference, the callbacks used in Entry above look like this: pub type AppEntryValue = JsonString ; fn serialize_app_entry < S > ( app_entry_type : & AppEntryType , app_entry_value : & AppEntryValue , serializer : S , ) -> Result < S :: Ok , S :: Error > where S : Serializer , { let mut state = serializer . serialize_tuple ( 2 ) ? ; state . serialize_element ( & app_entry_type . to_string ()) ? ; state . serialize_element ( & app_entry_value . to_string ()) ? ; state . end () } fn deserialize_app_entry < 'de , D > ( deserializer : D ) -> Result < ( AppEntryType , AppEntryValue ), D :: Error > where D : Deserializer < 'de > , { #[derive(Deserialize)] struct SerializedAppEntry ( String , String ); let serialized_app_entry = SerializedAppEntry :: deserialize ( deserializer ) ? ; Ok (( AppEntryType :: from ( serialized_app_entry . 0 ), AppEntryValue :: from ( serialized_app_entry . 1 ), )) } Obviously this is a lot of boilerplate for one tuple, and is really only the tip of the iceberg for how complex custom serde implementations can get. Use this for surgical implementations along critical path type safety/ergonomics. Skip the attribute \u00b6 Serde also allows for attributes to be completely skipped during serialization. In the context of a JsonString this is unlikely to be the desired behaviour. If we are serializing the outer struct we probably want the inner JSON data to also be serialized, but not necessarily, or perhaps we don't need it and so can live without it. This option has very clear tradeoffs. We lose the JSON data when the outer struct is serialized but also don't have to worry about how it might be represented. This option is very handy during development/prototyping/debugging when you want to sketch out a larger idea without immediately tackling serde logic. Simply add the #[serde(skip)] attribute to your struct. #[derive(Serialize, Deserialize)] struct Foo { #[serde(skip)] bar : JsonString , } Wrap/convert to a new type or struct \u00b6 If it is possible to create a struct that better represents the data, or a new type to hold it, then that struct can implement to/try_from JsonString . This is very similar to the first option where we put a String into Foo but it provides semantics, information for the compiler and somewhere to hook into() for our code. // Bar as a new type #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Bar ( String ) #[derive(Serialize, Deserialize)] struct Foo { bar : Bar , } impl Foo { fn new ( bar : Bar ) -> Foo { Foo { bar } } fn bar ( & self ) -> Bar { self . bar . clone () } } // somewhere else... let json = JsonString :: from (..); let bar = Bar :: from ( json ); let foo = Foo :: new ( bar ); // or... let json = JsonString :: from (..); let foo = Foo :: new ( json . into ()); The biggest drawback to this approach is the potential for stutter. With lots of nested types we give the compiler more power but also can incidentally bloat the JSON output a lot. Many ad-hoc/once-off types can also become confusing for humans and lead to duplicated/redundant code over time. It is easy to end up with JSON like {\"Foo\":{\"bar\":{\"Bar\":[\"..\"]}}} with a poorly chosen combination of enum variants and tuples. As per all the considerations outlined for using String directly on Foo , avoid using json! or similar to build up the internal String of Bar . Hiding JsonString with Into<JsonString> \u00b6 It is possible in function signatures to simply leave an argument open to anything that can be converted to JsonString . This is exactly like using Into<String> but for JSON data. An even looser option is to only require TryInto<JsonString> but this makes little or no difference to us in practise. An example of this is the store_as_json used to pass native Rust typed data across the WASM boundary. This is used internally by the define_zome! macro for all zome funtions: pub fn store_as_json < J : TryInto < JsonString >> ( stack : & mut WasmStack , jsonable : J , ) -> Result < SinglePageAllocation , RibosomeErrorCode > { let j : JsonString = jsonable . try_into () . map_err ( | _ | RibosomeErrorCode :: ArgumentDeserializationFailed ) ? ; let json_bytes = j . into_bytes (); let json_bytes_len = json_bytes . len () as u32 ; if json_bytes_len > U16_MAX { return Err ( RibosomeErrorCode :: OutOfMemory ); } write_in_wasm_memory ( stack , & json_bytes , json_bytes_len as u16 ) } The relevant into() or try_into() method is called internally by the function accepting Into<JsonString> , meaning the caller needs to know almost nothing about how the serialization is done. Additionally, the caller could do its own custom serialization, passing a String through, which would be wrapped as-is into a JsonString . Unfortunately this doesn't work as well for structs because of the way trait bounds work (or don't work) without complex boxing etc. See above for simple strategies to cope with nested/wrapped serialization in nested native data structures. This approach can be combined with the \"quick and dirty\" Foo with private String internals to create a Foo that can store anything that round trips through JsonString : struct Foo { bar : String , } impl Foo { fn new < J : Into < JsonString >> ( bar : J ) -> Foo { Foo { bar : String :: from ( JsonString :: from ( bar )) } } fn bar < T : TryFrom < JsonString >> ( & self ) -> Result < T , HolochainError > { Ok ( JsonString :: from ( self . bar . clone ()). try_into () ? ) } } // somewhere later.. // we can build MyBar ad-hoc to send to Foo as long as it implements JsonString // we could create MyOtherBar in the same way and send to Foo in the same way #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct MyBar { .. } let my_bar = MyBar :: new (..); // auto stores as String via. JsonString internally let foo = Foo :: new ( my_bar ); // note we must provide the MyBar type at restore time because we destroyed // that type info during the serialization process let restored_bar : MyBar = foo . bar () ? ; This is how the ContentAddressableStorage trait used to work. It would \"magically\" restore the correct Content from storage based on an Address and type alone, provided the compiler had the type info available at compile time. We had to sacrifice this neat trick due to incompatible constraints from the type system elsewhereon the CAS, but it should work well in most scenarios :)","title":"Understanding JsonString"},{"location":"guide/json_string/#serialization-and-jsonstring","text":"","title":"Serialization and JsonString"},{"location":"guide/json_string/#why-serialize-anything-why-json","text":"","title":"Why serialize anything? Why JSON?"},{"location":"guide/json_string/#holochain-zomes-are-written-in-wasm","text":"WASM only supports working directly with integers and manually allocating memory. This means that sharing any data between holochain core and zome functions must be serialized. There is no way that WASM functions can understand the Rust type system natively. Serialized data can be allocated for WASM to read out and deserialize into Rust types/structs/enums. Any developers using the Rust HDK get the serialization/deserialization and type handling almost \"for free\". The macros for defining entities and zomes automatically wrap the memory work and serialization round trips for anything that implements Into<JsonString> and TryFrom<JsonString> (see below). We use serde for our serialization round trips as it is by far the most popular and mature option for Rust. Many serialization formats other than JSON are supported by serde but JSON is a solid option. JSON allows us to easily bring the Rust type system across to WASM with decent performance. From the serde_json github repository README: It is fast. You should expect in the ballpark of 500 to 1000 megabytes per second deserialization and 600 to 900 megabytes per second serialization, depending on the characteristics of your data. This is competitive with the fastest C and C++ JSON libraries or even 30% faster for many use cases. Benchmarks live in the serde-rs/json-benchmark repo.","title":"Holochain zomes are written in WASM."},{"location":"guide/json_string/#holochain-aims-to-support-all-wasm-languages-not-just-rustjs","text":"The official Holochain HDK is Rust. The Rust HDK will always be the most tightly integrated HDK with core simply because Holochain itself is Rust based. Generally though, we are hoping and expecting many different WASM zome languages build an ecosystem over time. Personally I'm hoping for a decent LISP to appear ;) To encourage as many languages as possible we want to keep the minimum requirements for interacting with holochain core as minimal as possible. Currently the two requirements for writing zomes in <your favourite language> : Must compile to WASM Must be able to serialize UTF-8 data and allocate to memory read by core We can't do much about the first requirement but here are some lists to watch: https://github.com/appcypher/awesome-wasm-langs https://github.com/mbasso/awesome-wasm The second requirement means that we must be very mindful of choosing a serialization format that can round trip through as many languages as possible. In the end, this is the main reason we chose JSON for communication with core. Note that at the time of writing, the AssemblyScript (ostentisbly JavaScript) WASM implementation does not even provide a native JSON.parse() method! To do something as apparently simple as serialize JSON in JavaScript we have had to implement a custom JSON parser. At least JSON (naturally) maps very well to JavaScript native data, other serialization/language combinations are even further from maturity. WASM is very promising but very immature so esoteric serialization options are not really viable options right now, even if serde supports them in Rust.","title":"Holochain aims to support all WASM languages not just Rust/JS"},{"location":"guide/json_string/#json-serialization-only-pertains-to-communication-with-core","text":"Holochain often makes a distinction between \"app data\" and \"core data\". Following the biomimicry theme we sometimes call this \"conscious\" vs. \"subconscious\" when this data is used in zomes or core logic respectively. The most obvious example of this is the Entry enum that has an Entry::App variant explicitly for app data, and other variants for system logic. The Entry enum itself is serialized via JSON so that is has maximal compatibility across all zome languages (see above) across the core/wasm boundary. However, the contents of Entry::App(..) are treated as an opaque UTF-8 string by Holochain core. Naturally the HDK macros we offer provide sugar to work with the value of app entries but this is not enforced anywhere within core. Because the Rust serialization round tripping must work across both core and the HDK it must work equally well while treating the app entry values as opaque in the subconscious and meaningful structs in the conscious. This is achieved through a healthy dose of compiler and macro magic. This means that zome developers can implement their own serialization logic for their own data if they wish. Simply by wrapping a zome-serialized app entry value in \"\\\"...\\\"\" it becomes a string primitive from core's perspective. The zome can do anything needed with this, including custom validation logic, etc. The RawString type handles this automatically with JsonString (see below).","title":"JSON serialization only pertains to communication with core"},{"location":"guide/json_string/#serialization-through-rust-types","text":"","title":"Serialization through Rust types"},{"location":"guide/json_string/#how-rust-serializes-serde-from-1000m","text":"The serde crate leans heavily on the Rust compiler for serialization round tripping. Using the \"vanilla\" serde_json crate affords this logic on the way in: let foo_json = serde_json :: to_string ( foo ). unwrap (); Notes: There is an unwrap but this can't fail for simple structs/enums in practise The unwrap can fail e.g. serializing streams but we don't do that The compiler enforces that everything we pass to serde can Serialize foo can be anything that implements Serialize we have no direct control over the structure of the JSON output the Serialize implementation of foo decides this for us in the case of nested data e.g. hash maps, Serialize works recursively OR using the manual json! macro: let foo_json = json ! ({ \"foo\" : foo . inner ()}); Notes: We no longer have an unwrap so there is slightly less boilerplate to type We have a lot of direct control over the structure of our output JSON For better or worse we avoid what the compiler says about Serialize on Foo We must now manually ensure that \"{\\\"foo\\\":...}\" is handled everywhere Including in crates we don't control Including when we change our JSON structure across future releases Including across WASM boundaries in HDK consumers AND on the way out: let foo : Foo = Foo :: try_from ( & hopefully_foo_json ) ? ; Notes: Serde relies on compiler info, the type Foo on the left, to deserialize Serde requires that hopefully_foo_json makes sense as Foo This definitely can fail as the json is just a String to the compiler In real code do not unwrap this, handle the Err carefully!","title":"How Rust serializes: serde from 1000m"},{"location":"guide/json_string/#json-structure-the-rust-compiler-and-you","text":"All this means that our JSON data MUST closely align with the types we define for the compiler. There is a lot of flexibility offered by serde for tweaking the output (e.g. lowercasing names of things, modifying strings, etc.) but the tweaks involve a lot of boilerplate and have limits. For example this can be awkard when handling Result values. The Result enum has two variants in Rust, Ok and Err . Both of these, like all enum variants in Rust, follow the title case convention. This means that in a JS conductor/HDK consuming JSON values returned from zome functions that return a Result (a good idea!) we see this JavaScript: const result = app . call (...) const myVar = result . Ok ... We get a result.Ok rather than the result.ok that we'd expect from idiomatic JavaScript. As the JSON structure comes from the Rust compiler, we have two options: Force serde to output JSON that follows the conventions of another language Force conductors/HDKs to provide sugar to map between Rust/XXX idioms Force developers to work with a very leaky abstraction over the Rust compiler As the first option requires a lot of boilerplate and isn't interoperable across all languages anyway (e.g. kebab case, snake case, etc.) we currently are pushing this sugar down to conductor/HDK implementations. Additionally, the serialized form of entries is used to calculate Address values for storage and retrieval from the local chain and DHT so we need to be very careful here as it will be hard to change in the future. That said, we are open to constructive feedback on what this sugar looks like and how it works! Ideally zome development is as idiomatic as possible across as many languages as possible \ud83d\udd76","title":"JSON structure, the Rust compiler and you"},{"location":"guide/json_string/#binary-data-as-base64","text":"We recommend base64 encoding binary data straight into an app entry string that you can use in your zome logic directly (see above). Yes this uses more space than binary data, 33% more to be specific :( But there are benefits: It is UTF-8 and web (e.g. data URI) friendly Simply wrapped in \"\\\"..\\\"\" it becomes valid JSON (see RawString below) It has wide language support (see above for why this is important) It will be supported by all persistence backends for the forseeable future At least these storage systems require base64 encoded data at some point: Browser based localStorage MongoDB Elasticsearch Amazon SimpleDB Amazon DynamoDB. The performance penalty can be minimal: https://lemire.me/blog/2018/01/17/ridiculously-fast-base64-encoding-and-decoding/","title":"Binary data as base64"},{"location":"guide/json_string/#json-is-lame-can-holochain-support-ltmy-favourite-serialization-formatgt","text":"Yes... and no... It depends what you mean by \"support\". Right now, most serialization formats are supported in app/zome data simply by wrapping the output in double quotes so core sees it as a JSON string literal. Holochain core won't try to interpret/mangle any of that data so the zome can theoretically do whatever it wants at that point without a performance hit. In practise, there are some limitations as mentioned in this doc: WASM languages tend to have no or limited serialization options you may need to roll your own parse/stringify logic seriously... e.g. we pushed our own JSON.parse implementation upstream for the AssemblyScript team, that's JSON parsing in JavaScript ! don't underestimate how bleeding edge and limited the WASM tooling still is to work directly with WASM you must be prepared to bleed If you don't use JSON you can't use hdk macros for that part of your zome Only valid UTF-8 strings are supported (may change in the future) If you're looking for a way to provide core data in non-JSON format then NO that is not supported and won't be in the short-mid term future. Yes, serde supports many serialization options but: Not all data in core uses default serde serialization logic e.g. this document explaining non-default serde serialization logic Swapping to a different serializer in serde is not just a matter of passing config to serde we'd have to centralise/ match everywhere and swap out serde_json for analogous crates in each other format we'd want to use even using a SerialString instead of JsonString (see below) would not clear out every implementation without a lot of work Serde is already quite heavy in compilation/WASM files so we don't want to bloat that more with edge-case serialization needs every new format is a new crate We don't (yet) have any use-cases showing that JSON is a problem/bottleneck Adding more serialization options would exacerbate non-idiomatic conductor and HDK data structure mapping issues (see above)","title":"JSON is lame! Can Holochain support &lt;my favourite serialization format&gt;?"},{"location":"guide/json_string/#jsonstring","text":"","title":"JsonString"},{"location":"guide/json_string/#the-problem-and-our-solution","text":"Sometimes we want to nest serialization (e.g. hdk::call ) and sometimes we want to wrap serialization (e.g. Entry::App ), sometimes converting to a string uses entirely different logic (e.g. error values). Ideally we want the compiler to guide us through this process as mistakes are common and difficult to debug. We also want serialization logic to be as invisible as possible to zome developers using our HDKs. Serde will serialize anything that implements Serialize , including String so we added a type JsonString that does not automatically round trip to act as a logical \"checkpoint\" in our code. JsonString doesn't \"do\" anything beyond giving ourselves and the compiler a shared target while stepping through the serialization round trip. Essentially we trade this: // foo_a is a Foo // foo_json is a String // Foo implements Serialize and Deserialize let foo_json = serde_json :: to_string ( & foo_a ) ? ; let foo_b : Foo = serde_json :: from_str ( & foo_json ) ? ; for this: // foo_a is a Foo // JsonString implements From<Foo> // Foo implements TryFrom<JsonString> let foo_json = JsonString :: from ( foo_a ); let foo_b = Foo :: try_from ( hopefully_foo_json ) ? ; Which looks very similar but protects us from this bug: let foo_json = serde_json :: to_string ( & foo_a ) ? ; let foo_json = serde_json :: to_string ( & foo_json ) ? ; // <-- double serialized :/ let foo_b : Foo = serde_json :: from_str ( & foo_json ) ? ; // <-- will fail :( Because nesting JsonString::from() calls is a compiler error: let foo_json = JsonString :: from ( JsonString :: from ( foo_a )); // <-- compiler saves us :) and this bug: let foo_a : Foo = serde_json :: from_str ( & string_but_not_json ) ? ; // <-- runtime error :( Because calling Foo::try_from(String) is (probably) a compiler error: let foo_a = Foo :: try_from ( string_but_not_json ) ? ; // <-- compiler saves us again :) and this bug: type Foo = Result < String , String > ; let foo_json_a = json ! ({ \"Err\" : some_error . to_string ()}); // <-- good key `Err` // somewhere else... maybe a different crate or old crate version... let foo_json_b = json ! ({ \"error\" : some_error . to_string ()}); // <-- bad key `error` :/ let foo : Foo = serde_json :: from ( & foo_json_a ) ? ; // <-- works, key matches variant name let foo : Foo = serde_json :: from ( & foo_json_b ) ? ; // <-- runtime error! :( Because the structure of the JSON data is defined centrally at compile time: // Result<Into<JsonString>, Into<JsonString>> is implemented for you by HC core let foo_json_a = JsonString :: from ( Err ( some_error . to_string ())); // only one way to do things, automatically consistent across all crates // doing anything different is a compiler issue let foo_json_b = JsonString :: from ( Err ( some_error . to_string ())); Which is great for the majority of data that needs serializing. There are some important edge cases that we need to cover with additional techniques/tooling.","title":"The problem and our solution"},{"location":"guide/json_string/#string-handling","text":"JsonString::from_json(&str) requires the &str passed to it is already a serialized JSON value. We may add the option to validate this for debug builds at runtime in the future. Previously JsonString implemented the From<String> trait but this was removed. Strings are a special case as they may either contain serialized json or be used as a JSON string primitive. JsonString::from_json makes it explicit that you mean the former. We can use serde_json::to_string and json! to create JSON data that we can then wrap in JsonString . // same end result for both of these... let foo_json = JsonString :: from_json ( & serde_json :: to_string ( & foo )); let foo_json = JsonString :: from ( foo ); More commonly useful, we can move back and forward between String and JsonString without incurring serialization overhead or human error: // this does a round trip through types without triggering any serde JsonString :: from_json ( & String :: from ( JsonString :: from ( foo ))); This is helpful when a function signature requires a String or JsonString argument and we have the inverse type. It also helps when manually building JSON data by wrapping already serialized data e.g. with format! . An example taken from core: fn result_to_json_string < T : Into < JsonString > , E : Into < JsonString >> ( result : Result < T , E > , ) -> JsonString { let is_ok = result . is_ok (); let inner_json : JsonString = match result { Ok ( inner ) => inner . into (), Err ( inner ) => inner . into (), }; let inner_string = String :: from ( inner_json ); JsonString :: from_json ( & format ! ( \"{{ \\\" {} \\\" :{}}}\" , if is_ok { \"Ok\" } else { \"Err\" }, inner_string )) } impl < T : Into < JsonString > , E : Into < JsonString > + JsonError > From < Result < T , E >> for JsonString { fn from ( result : Result < T , E > ) -> JsonString { result_to_json_string ( result ) } } Which looks like this: let result : Result < String , HolochainError > = Err ( HolochainError :: ErrorGeneric ( \"foo\" . into ())); assert_eq ! ( JsonString :: from ( result ), JsonString :: from ( \"{ \\\" Err \\\" :{ \\\" ErrorGeneric \\\" : \\\" foo \\\" }}\" ), ); When given a Result containing any value that can be turned into a JsonString (see below), we can convert it first, then wrap it with String::from + format! .","title":"String handling"},{"location":"guide/json_string/#string-serialization","text":"Sometimes we want a String to be serialized as a JSON string primitive rather than simply wrapped in a JsonString struct. JsonString::from won't do what we need because it always wraps strings, we need to nest the String serialization. let foo = String :: from ( JsonString :: from_json ( \"foo\" )); // \"foo\" = not what we want let foo = ??? ; // \"\\\"foo\\\"\" = what we want To keep the type safety from JsonString and nest String serialization use RawString wrapped in JsonString . RawString wraps String and serializes it to a JSON string primitive when JsonString ified. // does what we need :) let foo = String :: from ( JsonString :: from ( RawString :: from ( \"foo\" ))); // \"\\\"foo\\\"\" An example of this can be seen in the core version of the Result serialization from above that deals with String error values: impl < T : Into < JsonString >> From < Result < T , String >> for JsonString { fn from ( result : Result < T , String > ) -> JsonString { let is_ok = result . is_ok (); let inner_json : JsonString = match result { Ok ( inner ) => inner . into (), // strings need this special handling c.f. Error Err ( inner ) => RawString :: from ( inner ). into (), // <-- RawString here! }; let inner_string = String :: from ( inner_json ); format ! ( \"{{ \\\" {} \\\" :{}}}\" , if is_ok { \"Ok\" } else { \"Err\" }, inner_string ) . into () } } Which looks like this: let result : Result < String , String > = Err ( String :: from ( \"foo\" )); assert_eq ! ( JsonString :: from ( result ), JsonString :: from ( \"{ \\\" Err \\\" : \\\" foo \\\" }\" ), ) If we didn't do this then the format! would return invalid JSON data with the String error value missing the wrapping double quotes. RawString is useful when working with types that have a .to_string() method or similar where the returned string is not valid JSON. Examples of when RawString could be useful: Error descriptions that return plain text in a string Base64 encoded binary data Enum variants with custom string representations \"Black boxing\" JSON data that Rust should not attempt to parse","title":"String serialization"},{"location":"guide/json_string/#implementing-jsonstring-for-custom-types","text":"As mentioned above, there are two trait implementations that every struct or enum should implement to be compatible with core serialization logic: impl From<MyType> for JsonString to serialize MyType impl TryFrom<JsonString> for MyType to attempt to deserialize into MyType Note that TryFrom is currently an unstable Rust feature. To enable it add !#[feature(try_from)] to your crate/zome. Based on discussions in the Rust community issue queues/forums, we expect this feature to eventually stabilise and no longer require feature flags to use. The TryFrom trait will need to be added as use std::convert::TryFrom to each module/zome implementing it for a struct/enum.","title":"Implementing JsonString for custom types"},{"location":"guide/json_string/#boilerplate","text":"To defer all the logic to standard serde defaults with some sensible debug logic in the case of an error, there are two utility functions in core, default_to_json and default_try_from_json . The standard minimal boilerplate looks like this: struct MyType {} impl From < MyType > for JsonString { fn from ( my_type : MyType ) -> Self { default_to_json ( my_type ) } } impl TryFrom < JsonString > for MyType { type Error = HolochainError ; fn try_from ( json_string : JsonString ) -> Result < Self , Self :: Error > { default_try_from_json ( json_string ) } }","title":"Boilerplate"},{"location":"guide/json_string/#automatic-derive","text":"The standard boilerplate has been implemented as a derive macro in the holochain_persistence_derive crate. Simply #[derive(DefaultJson)] to add the above boilerplate plus some extra conveniences (e.g. for references) to your type. DefaultJson requires: JsonString is included HolochainError is included MyType implements Serialize , Deserialize and Debug from serde/std use holochain_persistence_api :: json :: JsonString ; use holochain_core_types :: error :: HolochainError ; #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct MyType {}","title":"Automatic derive"},{"location":"guide/json_string/#using-jsonstring-as-the-property-of-a-structenum","text":"Because JsonString cannot automatically be round tripped with Serialize and Deserialize , the following can cause difficulty: #[derive(Serialize, Deserialize)] struct Foo { bar : JsonString , } The compiler will complain about this because anything deriving Serialize recursively must consist only of values that also implement Serialize . There are a few approaches here, each with benefits and tradeoffs. Swap out JsonString with String Use a serde attribute to manually serialize Bar Use a serde attribute to skip Bar Create a \"new type\" or wrapper/conversion struct","title":"Using JsonString as the property of a struct/enum"},{"location":"guide/json_string/#swap-jsonstring-with-string","text":"This approach is quick and dirty. Simply change the type of Bar to String . When prototyping or on deadline, this might be the most attractive option ;) This will likely cause problems upstream and downstream of what you are doing, or may be symptomatic of poorly handled JSON somewhere. This is roughly how Entry used to work, with a String valued SerializedEntry and JsonString valued Entry that could be swapped between using a From implementation. Done correctly we can \"onboard\" values to Foo by simply carefully wrapping and unwrapping the String . Done badly, we reintroduce the possibility for invalid wrap/nest/etc. logic to creep in. This works best when the fields on Foo are private and immutable, exposed only through getter/setter/new style methods that internally convert between JsonString and String . This option is less suitable if we want to double serialize the nested JSON data when serializing Foo . For an example of where we preserve JSON rather than trying to automatically deserialize or wrap it with structs, see the return values from hdk::call (not using structs, but similar ideas). Also consider that somebody reading your code might entirely miss the fact that Foo::bar is JSON data if all they read is the struct definition. It may be worthwhile adding methods to Foo to enforce this: #[derive(Serialize, Deserialize)] pub struct Foo { bar : String , } impl Foo { pub fn new ( bar : JsonString ) -> Foo { Foo { bar : String :: from ( bar )} } pub fn bar ( & self ) -> JsonString { JsonString :: from ( self . bar . clone ()) } } Treat bar as though it was going to be stored as a JsonString right until the last moment. Avoid this: let bar_json = json ! ({ \"bar\" : bar . inner ()}). to_string (); // somwhere later... let foo = Foo { bar : bar_json }; Because then everything that needs to use Foo must consistently implement the manual jsonification logic. This is especially important if Foo and/or bar is to be used across multiple crates. Instead, prefer this: #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Bar { bar : .. } let bar_json = JsonString :: from ( Bar { bar : ..}); let foo = Foo :: new ( bar ); // assuming impl Foo::new from above The result is still a raw String in Foo but the validity and consistency of the JSON data is enforced across all crates by JsonString::from(bar) . It is even possible to internalise the JsonString completely within the Foo methods using Into<JsonString> . This is covered in more detail below.","title":"Swap JsonString with String"},{"location":"guide/json_string/#using-serde-attributes","text":"Serde allows us to set serialization logic at the field level for structs. The best example of this is handling of AppEntryValue in core. As all zome data is treated as JSON, assumed to line up with internal structs in the HDK but potentially opaque string primitives (see above) we simply alias AppEntryValue to JsonString . The Entry enum needs to be serialized for many reasons in different contexts, including for system entries that zome logic never handles directly. It looks something like this (at the time of writing): #[derive(Clone, Debug, Serialize, Deserialize, DefaultJson)] pub enum Entry { #[serde(serialize_with = \"serialize_app_entry\" )] #[serde(deserialize_with = \"deserialize_app_entry\" )] App ( AppEntryType , AppEntryValue ), Dna ( Dna ), AgentId ( AgentId ), Delete ( Delete ), LinkAdd ( LinkAdd ), LinkRemove ( LinkRemove ), LinkList ( LinkList ), ChainHeader ( ChainHeader ), ChainMigrate ( ChainMigrate ), } Note that Entry : Derives Serialize and Deserialize and even DefaultJson ! Contains AppEntryValue in a tuple, which is a JsonString Uses some serde serialization attributes This works because the serialization attributes tell serde how to handle the JsonString in this context . This is a double edged sword. We have explicit control over the serialization so we can never accidentally wrap/nest/etc. JSON data in an invalid way. We also only define the serialization for this type in this one place. If AppEntryValue was used in some other struct/enum, we would have to manually remember to use the same or compatible serialize/deserialize callbacks. This approach also gives a lot of control over the final JSON structure. We can avoid stutters and reams of redundant data in the final output. This can mitigate the verbosity and awkwardness of compiler-driven JSON structures when sending data to other languages (see above). The serde documentation explains in great (technical) detail how to implement custom serialization and deserialization logic for many different data types: https://serde.rs/field-attrs.html For reference, the callbacks used in Entry above look like this: pub type AppEntryValue = JsonString ; fn serialize_app_entry < S > ( app_entry_type : & AppEntryType , app_entry_value : & AppEntryValue , serializer : S , ) -> Result < S :: Ok , S :: Error > where S : Serializer , { let mut state = serializer . serialize_tuple ( 2 ) ? ; state . serialize_element ( & app_entry_type . to_string ()) ? ; state . serialize_element ( & app_entry_value . to_string ()) ? ; state . end () } fn deserialize_app_entry < 'de , D > ( deserializer : D ) -> Result < ( AppEntryType , AppEntryValue ), D :: Error > where D : Deserializer < 'de > , { #[derive(Deserialize)] struct SerializedAppEntry ( String , String ); let serialized_app_entry = SerializedAppEntry :: deserialize ( deserializer ) ? ; Ok (( AppEntryType :: from ( serialized_app_entry . 0 ), AppEntryValue :: from ( serialized_app_entry . 1 ), )) } Obviously this is a lot of boilerplate for one tuple, and is really only the tip of the iceberg for how complex custom serde implementations can get. Use this for surgical implementations along critical path type safety/ergonomics.","title":"Using serde attributes"},{"location":"guide/json_string/#skip-the-attribute","text":"Serde also allows for attributes to be completely skipped during serialization. In the context of a JsonString this is unlikely to be the desired behaviour. If we are serializing the outer struct we probably want the inner JSON data to also be serialized, but not necessarily, or perhaps we don't need it and so can live without it. This option has very clear tradeoffs. We lose the JSON data when the outer struct is serialized but also don't have to worry about how it might be represented. This option is very handy during development/prototyping/debugging when you want to sketch out a larger idea without immediately tackling serde logic. Simply add the #[serde(skip)] attribute to your struct. #[derive(Serialize, Deserialize)] struct Foo { #[serde(skip)] bar : JsonString , }","title":"Skip the attribute"},{"location":"guide/json_string/#wrapconvert-to-a-new-type-or-struct","text":"If it is possible to create a struct that better represents the data, or a new type to hold it, then that struct can implement to/try_from JsonString . This is very similar to the first option where we put a String into Foo but it provides semantics, information for the compiler and somewhere to hook into() for our code. // Bar as a new type #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Bar ( String ) #[derive(Serialize, Deserialize)] struct Foo { bar : Bar , } impl Foo { fn new ( bar : Bar ) -> Foo { Foo { bar } } fn bar ( & self ) -> Bar { self . bar . clone () } } // somewhere else... let json = JsonString :: from (..); let bar = Bar :: from ( json ); let foo = Foo :: new ( bar ); // or... let json = JsonString :: from (..); let foo = Foo :: new ( json . into ()); The biggest drawback to this approach is the potential for stutter. With lots of nested types we give the compiler more power but also can incidentally bloat the JSON output a lot. Many ad-hoc/once-off types can also become confusing for humans and lead to duplicated/redundant code over time. It is easy to end up with JSON like {\"Foo\":{\"bar\":{\"Bar\":[\"..\"]}}} with a poorly chosen combination of enum variants and tuples. As per all the considerations outlined for using String directly on Foo , avoid using json! or similar to build up the internal String of Bar .","title":"Wrap/convert to a new type or struct"},{"location":"guide/json_string/#hiding-jsonstring-with-intoltjsonstringgt","text":"It is possible in function signatures to simply leave an argument open to anything that can be converted to JsonString . This is exactly like using Into<String> but for JSON data. An even looser option is to only require TryInto<JsonString> but this makes little or no difference to us in practise. An example of this is the store_as_json used to pass native Rust typed data across the WASM boundary. This is used internally by the define_zome! macro for all zome funtions: pub fn store_as_json < J : TryInto < JsonString >> ( stack : & mut WasmStack , jsonable : J , ) -> Result < SinglePageAllocation , RibosomeErrorCode > { let j : JsonString = jsonable . try_into () . map_err ( | _ | RibosomeErrorCode :: ArgumentDeserializationFailed ) ? ; let json_bytes = j . into_bytes (); let json_bytes_len = json_bytes . len () as u32 ; if json_bytes_len > U16_MAX { return Err ( RibosomeErrorCode :: OutOfMemory ); } write_in_wasm_memory ( stack , & json_bytes , json_bytes_len as u16 ) } The relevant into() or try_into() method is called internally by the function accepting Into<JsonString> , meaning the caller needs to know almost nothing about how the serialization is done. Additionally, the caller could do its own custom serialization, passing a String through, which would be wrapped as-is into a JsonString . Unfortunately this doesn't work as well for structs because of the way trait bounds work (or don't work) without complex boxing etc. See above for simple strategies to cope with nested/wrapped serialization in nested native data structures. This approach can be combined with the \"quick and dirty\" Foo with private String internals to create a Foo that can store anything that round trips through JsonString : struct Foo { bar : String , } impl Foo { fn new < J : Into < JsonString >> ( bar : J ) -> Foo { Foo { bar : String :: from ( JsonString :: from ( bar )) } } fn bar < T : TryFrom < JsonString >> ( & self ) -> Result < T , HolochainError > { Ok ( JsonString :: from ( self . bar . clone ()). try_into () ? ) } } // somewhere later.. // we can build MyBar ad-hoc to send to Foo as long as it implements JsonString // we could create MyOtherBar in the same way and send to Foo in the same way #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct MyBar { .. } let my_bar = MyBar :: new (..); // auto stores as String via. JsonString internally let foo = Foo :: new ( my_bar ); // note we must provide the MyBar type at restore time because we destroyed // that type info during the serialization process let restored_bar : MyBar = foo . bar () ? ; This is how the ContentAddressableStorage trait used to work. It would \"magically\" restore the correct Content from storage based on an Address and type alone, provided the compiler had the type info available at compile time. We had to sacrifice this neat trick due to incompatible constraints from the type system elsewhereon the CAS, but it should work well in most scenarios :)","title":"Hiding JsonString with Into&lt;JsonString&gt;"},{"location":"guide/keys/","text":"Keys \u00b6","title":"(E) Keys"},{"location":"guide/keys/#keys","text":"","title":"Keys"},{"location":"guide/lifecycle_of_an_entry/","text":"Lifecycle of an Entry \u00b6 Commit \u00b6 New entry Validate commit Commit to source chain Entries must be committed to the local source chain before they can be broadcast to the DHT. Every entry must pass a ValidateCommit lifecycle function check before it can be committed. If ValidateCommit is not implemented for the zome committing the entry then this is treated as a pass and the entry will be committed.","title":"Lifecycle of an Entry"},{"location":"guide/lifecycle_of_an_entry/#lifecycle-of-an-entry","text":"","title":"Lifecycle of an Entry"},{"location":"guide/lifecycle_of_an_entry/#commit","text":"New entry Validate commit Commit to source chain Entries must be committed to the local source chain before they can be broadcast to the DHT. Every entry must pass a ValidateCommit lifecycle function check before it can be committed. If ValidateCommit is not implemented for the zome committing the entry then this is treated as a pass and the entry will be committed.","title":"Commit"},{"location":"guide/live_hc_apps/","text":"Going Live with Holochain Apps \u00b6","title":"Going Live with Holochain Apps"},{"location":"guide/live_hc_apps/#going-live-with-holochain-apps","text":"","title":"Going Live with Holochain Apps"},{"location":"guide/managing_the_conductor/","text":"Managing the Conductor \u00b6 Conductor is a class that is exported from holochain-nodejs , and can be imported into your code. It is mostly used internally in the library, but can be useful in some of your own use cases. Import Example \u00b6 const { Conductor } = require ( '@holochain/holochain-nodejs' ) Simple Use \u00b6 Conductor.run(conductorConfig, runner) => Promise \u00b6 Spin up a Conductor with a Conductor configuration . When you're done with it, call stop , a function injected into the closure. Name conductorConfig Type string or object Description should be a TOML configuration string, as described here or an equivalent JavaScript object constructed manually, or setup using the Config helper functions described here . Name runner Type function Description runner is a closure: (stop, conductor) => { (code to run) } - stop is a function that shuts down the Conductor and must be called in the closure body - conductor is a Conductor instance, from which one can make Instances and thus Zome calls. Example \u00b6 // ... Conductor . run ( Config . conductor ([ instanceAlice , instanceBob , instanceCarol , ]), ( stop , conductor ) => { doStuffWith ( conductor ) stop () }) Manually Instantiating a Conductor \u00b6 constructor(conductorConfig) => Conductor \u00b6 Instantiate a Conductor with a full Conductor configuration . Name conductorConfig Type string or object Description should be a TOML configuration string, as described here or an equivalent JavaScript object constructed manually, or setup using the Config helper functions described here . Example \u00b6 // config var can be defined using the Config helper functions const conductor = new Conductor ( config ) Manually Starting and Stopping a Conductor \u00b6 conductor.start() => null \u00b6 Start running all instances. No Zome functions can be called within an instance if the instance is not started, so this must be called beforehand. Example \u00b6 conductor . start () conductor.stop() => Promise \u00b6 Stop all running instances configured for the conductor. This function should be called after all desired Zome calls have been made, otherwise the conductor instances will continue running as processes in the background. Returns a Promise that you can optionally wait on to ensure that internal cleanup is complete. Example \u00b6 conductor . stop ()","title":"Manually Manage the Conductor"},{"location":"guide/managing_the_conductor/#managing-the-conductor","text":"Conductor is a class that is exported from holochain-nodejs , and can be imported into your code. It is mostly used internally in the library, but can be useful in some of your own use cases.","title":"Managing the Conductor"},{"location":"guide/managing_the_conductor/#import-example","text":"const { Conductor } = require ( '@holochain/holochain-nodejs' )","title":"Import Example"},{"location":"guide/managing_the_conductor/#simple-use","text":"","title":"Simple Use"},{"location":"guide/managing_the_conductor/#conductorrunconductorconfig-runner-promise","text":"Spin up a Conductor with a Conductor configuration . When you're done with it, call stop , a function injected into the closure. Name conductorConfig Type string or object Description should be a TOML configuration string, as described here or an equivalent JavaScript object constructed manually, or setup using the Config helper functions described here . Name runner Type function Description runner is a closure: (stop, conductor) => { (code to run) } - stop is a function that shuts down the Conductor and must be called in the closure body - conductor is a Conductor instance, from which one can make Instances and thus Zome calls.","title":"Conductor.run(conductorConfig, runner) =&gt; Promise"},{"location":"guide/managing_the_conductor/#example","text":"// ... Conductor . run ( Config . conductor ([ instanceAlice , instanceBob , instanceCarol , ]), ( stop , conductor ) => { doStuffWith ( conductor ) stop () })","title":"Example"},{"location":"guide/managing_the_conductor/#manually-instantiating-a-conductor","text":"","title":"Manually Instantiating a Conductor"},{"location":"guide/managing_the_conductor/#constructorconductorconfig-conductor","text":"Instantiate a Conductor with a full Conductor configuration . Name conductorConfig Type string or object Description should be a TOML configuration string, as described here or an equivalent JavaScript object constructed manually, or setup using the Config helper functions described here .","title":"constructor(conductorConfig) =&gt; Conductor"},{"location":"guide/managing_the_conductor/#example_1","text":"// config var can be defined using the Config helper functions const conductor = new Conductor ( config )","title":"Example"},{"location":"guide/managing_the_conductor/#manually-starting-and-stopping-a-conductor","text":"","title":"Manually Starting and Stopping a Conductor"},{"location":"guide/managing_the_conductor/#conductorstart-null","text":"Start running all instances. No Zome functions can be called within an instance if the instance is not started, so this must be called beforehand.","title":"conductor.start() =&gt; null"},{"location":"guide/managing_the_conductor/#example_2","text":"conductor . start ()","title":"Example"},{"location":"guide/managing_the_conductor/#conductorstop-promise","text":"Stop all running instances configured for the conductor. This function should be called after all desired Zome calls have been made, otherwise the conductor instances will continue running as processes in the background. Returns a Promise that you can optionally wait on to ensure that internal cleanup is complete.","title":"conductor.stop() =&gt; Promise"},{"location":"guide/managing_the_conductor/#example_3","text":"conductor . stop ()","title":"Example"},{"location":"guide/naming_conventions/","text":"Naming things \u00b6 There are only two hard things in Computer Science: cache invalidation and naming things. Rust naming conventions \u00b6 If in doubt refer to the Rust conventions. https://doc.rust-lang.org/1.0.0/style/style/naming/README.html Holochain naming conventions \u00b6 There are gaps where the Rust conventions are either silent or following them would make things too ambiguous. Actions & reducers \u00b6 Action is VerbNoun or Verb if there is no available noun and matches the underlying function e.g. GetEntry ActionResponse is ActionName e.g. Action::QueryEntry results in ActionResponse::GetEntry reducer name is reduce_action_name e.g. reduce_get_entry Actors & protocols \u00b6 Actor Protocol is VerbNoun or Verb if there is no available noun and matches the underlying function e.g. PutEntry or Setup Result of a Protocol is VerbNounResult or VerbResult e.g. PutEntryResult or SetupResult Method names \u00b6 method names that access something directly \"for free\" are the name of the thing being accessed, e.g. entry() method names that have side effects or an expensive lookup are verb_noun() e.g. put_entry() Short names \u00b6 avoid micro names like t , e , h when table , entry , header is clearer. avoid shorthand names like table when table_actor is clearer. in the long run the legibility and unambiguity saves orders of magnitude more time than the typing costs.","title":"Naming conventions"},{"location":"guide/naming_conventions/#naming-things","text":"There are only two hard things in Computer Science: cache invalidation and naming things.","title":"Naming things"},{"location":"guide/naming_conventions/#rust-naming-conventions","text":"If in doubt refer to the Rust conventions. https://doc.rust-lang.org/1.0.0/style/style/naming/README.html","title":"Rust naming conventions"},{"location":"guide/naming_conventions/#holochain-naming-conventions","text":"There are gaps where the Rust conventions are either silent or following them would make things too ambiguous.","title":"Holochain naming conventions"},{"location":"guide/naming_conventions/#actions-reducers","text":"Action is VerbNoun or Verb if there is no available noun and matches the underlying function e.g. GetEntry ActionResponse is ActionName e.g. Action::QueryEntry results in ActionResponse::GetEntry reducer name is reduce_action_name e.g. reduce_get_entry","title":"Actions &amp; reducers"},{"location":"guide/naming_conventions/#actors-protocols","text":"Actor Protocol is VerbNoun or Verb if there is no available noun and matches the underlying function e.g. PutEntry or Setup Result of a Protocol is VerbNounResult or VerbResult e.g. PutEntryResult or SetupResult","title":"Actors &amp; protocols"},{"location":"guide/naming_conventions/#method-names","text":"method names that access something directly \"for free\" are the name of the thing being accessed, e.g. entry() method names that have side effects or an expensive lookup are verb_noun() e.g. put_entry()","title":"Method names"},{"location":"guide/naming_conventions/#short-names","text":"avoid micro names like t , e , h when table , entry , header is clearer. avoid shorthand names like table when table_actor is clearer. in the long run the legibility and unambiguity saves orders of magnitude more time than the typing costs.","title":"Short names"},{"location":"guide/new_project/","text":"Create A New Project \u00b6 The command line tools discussed in the last article can be used to easily create a new folder on your computer, that contains all the initial folders and files needed for a Holochain application. You will typically want to create a new project folder for a Holochain application this way. This one approach will suit the creation of a new Holochain app or implementing an existing app with Holochain instead. In your terminal, change directories to one where you wish to initialize a new Holochain app. The command will create a new folder within the current directory for your app. Come up with a name for your application, or at least for your project folder. Copy or type the command below into your terminal, except replace your_app_name with the name you came up with. Press Enter to execute the command. Again, make sure you have followed the quick start guide . $ nix-shell https://holochain.love # snip [ nix-shell:~ ] $ hc init your_app_name Created new Holochain project at: \"your_app_name\" hc specifies that you wish to use the Holochain command line tools. init specifies to use the command for initializing a new project folder. your_app_name is an argument you supply as the app, and folder name. This has created a new folder in which you have the beginnings of a Holochain app. Check out the next article to see what the contents of a DNA source code folder looks like.","title":"Create A New Project"},{"location":"guide/new_project/#create-a-new-project","text":"The command line tools discussed in the last article can be used to easily create a new folder on your computer, that contains all the initial folders and files needed for a Holochain application. You will typically want to create a new project folder for a Holochain application this way. This one approach will suit the creation of a new Holochain app or implementing an existing app with Holochain instead. In your terminal, change directories to one where you wish to initialize a new Holochain app. The command will create a new folder within the current directory for your app. Come up with a name for your application, or at least for your project folder. Copy or type the command below into your terminal, except replace your_app_name with the name you came up with. Press Enter to execute the command. Again, make sure you have followed the quick start guide . $ nix-shell https://holochain.love # snip [ nix-shell:~ ] $ hc init your_app_name Created new Holochain project at: \"your_app_name\" hc specifies that you wish to use the Holochain command line tools. init specifies to use the command for initializing a new project folder. your_app_name is an argument you supply as the app, and folder name. This has created a new folder in which you have the beginnings of a Holochain app. Check out the next article to see what the contents of a DNA source code folder looks like.","title":"Create A New Project"},{"location":"guide/nodejs_calling_zome_functions/","text":"Calling Zome Functions \u00b6 dnaInstance.call(zomeName, functionName, callParams) => object \u00b6 A DnaInstance can use the Conductor in which it's running to make calls to the custom functions defined in its Zomes. This is necessary in order to be able to test them. It calls synchronously and returns the result that the Zome function provides. An error could also be thrown, or returned. Note that Holochain has to serialize the actual arguments for the function call into JSON strings, which the Conductor will handle for you automatically. It also parses the result from a JSON string into an object. This function will only succeed if conductor.start() has been called for the Conductor in which the DnaInstance is running. Name zomeName Type string Description The name of the Zome within that instance being called into Name functionName Type string Description The name of the custom function in the Zome to call Name callParams Type object Description An object which will get stringified to a JSON string, before being passed into the Zome function. The keys of this object must match one-to-one with the names of the arguments expected by the Zome function, or an error will occur. Example \u00b6 // ... scenario . runTape ( \"test something\" , ( t , runner ) => { const alice = runner . alice // scenario.run and scenario.runTape both inject instances const callResult = alice . call ( 'people' , 'create_person' , { name : 'Franklin' }) }) Note that there are some cases where, for the purposes of testing, you may wish to wait for the results of calling a function in one instance, in terms of chain actions like commits and linking, to propogate to the other instances. For this, extra ways of performing calls have been added as utilities. Check them out in handling asynchronous network effects .","title":"Calling Zome Functions"},{"location":"guide/nodejs_calling_zome_functions/#calling-zome-functions","text":"","title":"Calling Zome Functions"},{"location":"guide/nodejs_calling_zome_functions/#dnainstancecallzomename-functionname-callparams-object","text":"A DnaInstance can use the Conductor in which it's running to make calls to the custom functions defined in its Zomes. This is necessary in order to be able to test them. It calls synchronously and returns the result that the Zome function provides. An error could also be thrown, or returned. Note that Holochain has to serialize the actual arguments for the function call into JSON strings, which the Conductor will handle for you automatically. It also parses the result from a JSON string into an object. This function will only succeed if conductor.start() has been called for the Conductor in which the DnaInstance is running. Name zomeName Type string Description The name of the Zome within that instance being called into Name functionName Type string Description The name of the custom function in the Zome to call Name callParams Type object Description An object which will get stringified to a JSON string, before being passed into the Zome function. The keys of this object must match one-to-one with the names of the arguments expected by the Zome function, or an error will occur.","title":"dnaInstance.call(zomeName, functionName, callParams) =&gt; object"},{"location":"guide/nodejs_calling_zome_functions/#example","text":"// ... scenario . runTape ( \"test something\" , ( t , runner ) => { const alice = runner . alice // scenario.run and scenario.runTape both inject instances const callResult = alice . call ( 'people' , 'create_person' , { name : 'Franklin' }) }) Note that there are some cases where, for the purposes of testing, you may wish to wait for the results of calling a function in one instance, in terms of chain actions like commits and linking, to propogate to the other instances. For this, extra ways of performing calls have been added as utilities. Check them out in handling asynchronous network effects .","title":"Example"},{"location":"guide/nodejs_dna_instances/","text":"DNA Instances \u00b6 DnaInstance is a class that is exported from holochain-nodejs and can be imported into your code. This class is used externally and instances of it are built automatically for you to use, so you typically should not have to construct a DnaInstance yourself. A DnaInstance represents a running version of a DNA package by a particular agent. This means that the agent has a source chain for this DNA. In addition to these basic properties on a DnaInstance that are covered below, the following articles cover how to make function calls into the Zomes . Import Example \u00b6 const { DnaInstance } = require ( '@holochain/holochain-nodejs' ) Instantiate A DnaInstance \u00b6 constructor(instanceId, conductor) => DnaInstance \u00b6 Instantiate a DnaInstance based on an instanceId, and the conductor where an instance with that id is running. Calling this manually is not typically necessary, since the Scenario testing returns these natively. A DnaInstance can make calls via that Conductor into Zome functions. Name instanceId Type string Description The instance id of the DnaInstance as specified in the configuration of conductor . Note that when using the Config.instance helper, the instance ID defaults to the agent name (as specified in Config.agent ) if not explicitly passed as a third argument. Name conductor Type Conductor Description A valid, and running Conductor instance Example \u00b6 const aliceInstance = new DnaInstance ( 'alice' , conductor ) DnaInstance Attributes \u00b6 dnaInstance.agentId \u00b6 The agentId for an instance. Example \u00b6 console . log ( alice . agentId ) // alice-----------------------------------------------------------------------------AAAIuDJb4M dnaInstance.dnaAddress \u00b6 The address of the DNA for an instance. Example \u00b6 console . log ( alice . dnaAddress ) // QmYiUmMEq1WQmSSjbM7pcLCy1GkdkfbwH5cxugGmeNZPE3","title":"DNA Instances"},{"location":"guide/nodejs_dna_instances/#dna-instances","text":"DnaInstance is a class that is exported from holochain-nodejs and can be imported into your code. This class is used externally and instances of it are built automatically for you to use, so you typically should not have to construct a DnaInstance yourself. A DnaInstance represents a running version of a DNA package by a particular agent. This means that the agent has a source chain for this DNA. In addition to these basic properties on a DnaInstance that are covered below, the following articles cover how to make function calls into the Zomes .","title":"DNA Instances"},{"location":"guide/nodejs_dna_instances/#import-example","text":"const { DnaInstance } = require ( '@holochain/holochain-nodejs' )","title":"Import Example"},{"location":"guide/nodejs_dna_instances/#instantiate-a-dnainstance","text":"","title":"Instantiate A DnaInstance"},{"location":"guide/nodejs_dna_instances/#constructorinstanceid-conductor-dnainstance","text":"Instantiate a DnaInstance based on an instanceId, and the conductor where an instance with that id is running. Calling this manually is not typically necessary, since the Scenario testing returns these natively. A DnaInstance can make calls via that Conductor into Zome functions. Name instanceId Type string Description The instance id of the DnaInstance as specified in the configuration of conductor . Note that when using the Config.instance helper, the instance ID defaults to the agent name (as specified in Config.agent ) if not explicitly passed as a third argument. Name conductor Type Conductor Description A valid, and running Conductor instance","title":"constructor(instanceId, conductor) =&gt; DnaInstance"},{"location":"guide/nodejs_dna_instances/#example","text":"const aliceInstance = new DnaInstance ( 'alice' , conductor )","title":"Example"},{"location":"guide/nodejs_dna_instances/#dnainstance-attributes","text":"","title":"DnaInstance Attributes"},{"location":"guide/nodejs_dna_instances/#dnainstanceagentid","text":"The agentId for an instance.","title":"dnaInstance.agentId"},{"location":"guide/nodejs_dna_instances/#example_1","text":"console . log ( alice . agentId ) // alice-----------------------------------------------------------------------------AAAIuDJb4M","title":"Example"},{"location":"guide/nodejs_dna_instances/#dnainstancednaaddress","text":"The address of the DNA for an instance.","title":"dnaInstance.dnaAddress"},{"location":"guide/nodejs_dna_instances/#example_2","text":"console . log ( alice . dnaAddress ) // QmYiUmMEq1WQmSSjbM7pcLCy1GkdkfbwH5cxugGmeNZPE3","title":"Example"},{"location":"guide/nodejs_instances/","text":"Instances \u00b6","title":"Instances"},{"location":"guide/nodejs_instances/#instances","text":"","title":"Instances"},{"location":"guide/other_test_harnesses/","text":"Other Test Harnesses \u00b6 Only tape is currently supported as a fully integrated test harness, but you can also run tests with more manual control using scenario.run . Using run allows you to manage the test yourself, only providing you with the basic help of starting and stopping a fresh Conductor instance. The example does still use tape to show how it compares to using runTape , but it could use any test harness, like Jest or Mocha. In fact, runTape simply calls run internally. scenario.run(runner) => null \u00b6 Each invocation of scenario.run does the following: Starts a fresh Conductor based on the configuration used to construct scenario Injects the values needed for the test into a closure you provide Name runner Type function Description runner is a closure: (stop, runner) => { (code to run) } . - stop is a function that shuts down the Conductor and must be called in the closure body - runner is an object containing an interface into each Instance specified in the config. The Instances are keyed by \"name\", as taken from the optional third parameter of Config.instance , which itself defaults to what was given in Config.agent . Example \u00b6 This example does also use tape as an illustration, but each test harness would have its own particular way const tape = require ( 'tape' ) // Create a scenario object in the same fashion as in other examples const scenario = new Scenario ([ instanceAlice , instanceBob ]) // scenario.run only manages the Conductor for us now, but we have to manage the test itself scenario . run (( stop , runner ) => { const alice = runner . alice const bob = runner . bob tape ( \"test something\" , t => { const result = alice . call ( 'zome' , 'function' , { params : 'go here' }) t . equal ( result , 'expected value' ) // the following two steps were not necessary when using runTape: t . end () // end the test stop () // use this injected function to stop the conductor }) }) // This example uses destructuring to show a clean and simple way to get the Instances scenario . run (( stop , { alice , bob }) => { tape ( \"test something else\" , t => { // write more tests in the same fashion t . equal ( 2 + 2 , 4 ) t . end () stop () // but don't forget to stop the conductor when it's done! }) })","title":"Other Test Harnesses"},{"location":"guide/other_test_harnesses/#other-test-harnesses","text":"Only tape is currently supported as a fully integrated test harness, but you can also run tests with more manual control using scenario.run . Using run allows you to manage the test yourself, only providing you with the basic help of starting and stopping a fresh Conductor instance. The example does still use tape to show how it compares to using runTape , but it could use any test harness, like Jest or Mocha. In fact, runTape simply calls run internally.","title":"Other Test Harnesses"},{"location":"guide/other_test_harnesses/#scenariorunrunner-null","text":"Each invocation of scenario.run does the following: Starts a fresh Conductor based on the configuration used to construct scenario Injects the values needed for the test into a closure you provide Name runner Type function Description runner is a closure: (stop, runner) => { (code to run) } . - stop is a function that shuts down the Conductor and must be called in the closure body - runner is an object containing an interface into each Instance specified in the config. The Instances are keyed by \"name\", as taken from the optional third parameter of Config.instance , which itself defaults to what was given in Config.agent .","title":"scenario.run(runner) =&gt; null"},{"location":"guide/other_test_harnesses/#example","text":"This example does also use tape as an illustration, but each test harness would have its own particular way const tape = require ( 'tape' ) // Create a scenario object in the same fashion as in other examples const scenario = new Scenario ([ instanceAlice , instanceBob ]) // scenario.run only manages the Conductor for us now, but we have to manage the test itself scenario . run (( stop , runner ) => { const alice = runner . alice const bob = runner . bob tape ( \"test something\" , t => { const result = alice . call ( 'zome' , 'function' , { params : 'go here' }) t . equal ( result , 'expected value' ) // the following two steps were not necessary when using runTape: t . end () // end the test stop () // use this injected function to stop the conductor }) }) // This example uses destructuring to show a clean and simple way to get the Instances scenario . run (( stop , { alice , bob }) => { tape ( \"test something else\" , t => { // write more tests in the same fashion t . equal ( 2 + 2 , 4 ) t . end () stop () // but don't forget to stop the conductor when it's done! }) })","title":"Example"},{"location":"guide/overview/","text":"Overview \u00b6 Depending on whether your interest is specific, or general, you may wish to read it front to back, or skip to specific sections of the book. If the summary of a section describes you, then it's a section for you! Planning a dApp \u00b6 Readers: You can't put the horse before the cart. First things first: understanding the landscape of decentralized apps. What are the mechanics of a functioning decentralized app? You have an idea of what you want to build, and you need to get a grasp on what it's going to take. You need to know what your blind spots are, and what are the common pitfalls. You need to know which of your assumptions to hold on to, and which to let go. Writing: General Audience Building Holochain Apps \u00b6 Readers: Whether you're a seasoned developer, or just starting out, you're in it to write code. You've either got an app project (or five) on the go, or you're in it to experiment and test the limits. You need to know what you need to know. You want to talk Holochain's language. You're an intrepid explorer of technical documentation. Writing: Technical, Explanatory & How-to Going Live with Holochain Apps \u00b6 Readers: You're involved in the conception, development, or design of a Holochain app, and you've got to know how to get your app out into the world, into the hands of the people who need it. You've got questions like \"How do updates to the app work?\", \"How do I track performance of the app?\", \"What are best practices for security?\" Writing: General Audience Extending the Holochain Platform \u00b6 Readers: You want to look at Holochain itself, not what you can build with it, but to see what you can tweak, or contribute. You've got ideas for Holochain and got skills to pull them off. You're reading the Holochain source code, or source documentation. Maybe you want to enable app development in a whole other language not available yet, or maybe to run Holochain on a device or platform not supported yet. You can make sense of terse technical language, and direct yourself well. Writing: Technical, Explanatory & How-to","title":"Overview"},{"location":"guide/overview/#overview","text":"Depending on whether your interest is specific, or general, you may wish to read it front to back, or skip to specific sections of the book. If the summary of a section describes you, then it's a section for you!","title":"Overview"},{"location":"guide/overview/#planning-a-dapp","text":"Readers: You can't put the horse before the cart. First things first: understanding the landscape of decentralized apps. What are the mechanics of a functioning decentralized app? You have an idea of what you want to build, and you need to get a grasp on what it's going to take. You need to know what your blind spots are, and what are the common pitfalls. You need to know which of your assumptions to hold on to, and which to let go. Writing: General Audience","title":"Planning a dApp"},{"location":"guide/overview/#building-holochain-apps","text":"Readers: Whether you're a seasoned developer, or just starting out, you're in it to write code. You've either got an app project (or five) on the go, or you're in it to experiment and test the limits. You need to know what you need to know. You want to talk Holochain's language. You're an intrepid explorer of technical documentation. Writing: Technical, Explanatory & How-to","title":"Building Holochain Apps"},{"location":"guide/overview/#going-live-with-holochain-apps","text":"Readers: You're involved in the conception, development, or design of a Holochain app, and you've got to know how to get your app out into the world, into the hands of the people who need it. You've got questions like \"How do updates to the app work?\", \"How do I track performance of the app?\", \"What are best practices for security?\" Writing: General Audience","title":"Going Live with Holochain Apps"},{"location":"guide/overview/#extending-the-holochain-platform","text":"Readers: You want to look at Holochain itself, not what you can build with it, but to see what you can tweak, or contribute. You've got ideas for Holochain and got skills to pull them off. You're reading the Holochain source code, or source documentation. Maybe you want to enable app development in a whole other language not available yet, or maybe to run Holochain on a device or platform not supported yet. You can make sense of terse technical language, and direct yourself well. Writing: Technical, Explanatory & How-to","title":"Extending the Holochain Platform"},{"location":"guide/packaging/","text":"Building Holochain Apps: Packaging \u00b6 The hc package command will automate the process of compiling your Zome code, encoding it, and inserting into the .dna.json file. In order to get these benefits, you just need to make sure that you have the right compilation tools installed on the machine you are using the command line tools from, and that you have the proper configuration files in your Zome folders. hc package works with two special files called .hcignore files and .hcbuild files .","title":"Building Holochain Apps: Packaging"},{"location":"guide/packaging/#building-holochain-apps-packaging","text":"The hc package command will automate the process of compiling your Zome code, encoding it, and inserting into the .dna.json file. In order to get these benefits, you just need to make sure that you have the right compilation tools installed on the machine you are using the command line tools from, and that you have the proper configuration files in your Zome folders. hc package works with two special files called .hcignore files and .hcbuild files .","title":"Building Holochain Apps: Packaging"},{"location":"guide/planning_a_dapp/","text":"Planning a dApp \u00b6 What is a dApp? \u00b6 A dApp is a distributed application. This means that the data associated with the application is stored by each user rather than in a central database. Basic expectations for dApps \u00b6 Generally speaking, you need to know the following in order to build a Holochain dApp: how to install Holochain how to use the command line tools how to configure your application with a \"DNA\" file how to write your application code in a language that compiles to WebAssembly how to think through building a distributed application how to build a user interface for your app how to test your application code This article will help you plan a dApp by providing practical considerations about the specifics of distributed applications in general, and Holochain dApps in particular. It has been remarked that holochain dApps require us to make a mental shift, first from applications whose data is centrally organized, and also from blockchain-based, data-centric dApps. Here we will provide a basic overview of concepts from cryptography that are central to holochains. Then, we will consider the consequences of Holochain's cryptographic architecture for data permissioning, access, and security. Because app data storage is distributed amongst user-participants, one must expect that data encryption and permissions are important for protecting privacy in accordance with the jurisdictions in which the app is operating. Remember that, as user-participants leave the application, they take their data with them. They also retain copies of other data that they held to support the DHT. One must also re-think the dApp's business model such that it does not rely on a central authority's ability to whitelist access to a given resource. Cryptography in Holochain dApps \u00b6 Distributed systems rely more heavily on cryptographic patterns and techniques than centralized systems. The basic concepts below explain how data integrity, ownership, and security are achieved natively with holochain's architecture. They are time-worn, relatively intuitive ideas that are critical for planning a holochain dApp. Hashes \u00b6 Hashes ensure the reliability of information by representing any given piece of data with a unique, consistent string of random looking characters. This makes changes to data visible because one can see that a hash has changed without needing to inspect the data itself. However, it is impossible to get the original data from a hash -- its purpose is to prove that the data to which it corresponds has not been altered. The same data consistently gives the same hash, and different data always give a completely different hash. These features imply that one can use small, portable hashes to verify data. One could also use a database containing data and their hashes as a table of contents, indexing (though not reading) data associated with a given hash. In the context of Holochain hashes are frequently used to look up content, both in our internal implementations as well as on the DHT. Therefore we frequently refer to the hash of some item (i.e. an entry on the chain) as its Address . Signatures \u00b6 Signatures provide an additional type of data verification, answering the question \"who created this data?\" Signatures look like hashes. They are unique, reliable, and like hashes, cannot be used to retrieve the data to which they correspond. Signatures also come with a pair of keys. One is public, and the other private. The private key designates a unique author (or device), and the public key lets anyone verify a signature made by one specific private key. This key infrastructure addresses the problem of single points of failure associated with centralized systems by making authors responsible for securing their unique private key. Encryption \u00b6 What if one needs to restrict access in addition to verifying data? Two types of encryption are possible. Symmetric encryption has one key for reading and writing data. Asymmetric encryption has two keys, where one creates messages and the other reads them. Encryption is a two way process, so the right key enables one to decrypt an encrypted message. With this added benefit come the drawbacks of the size of encrypted messages (at least as large as the original data) and broken encryption stripping the author of control of the original data. Key Management \u00b6 Once you are using cryptographic functions, either signing or encrypting, managing and securing the keys that unlock these functions, becomes a huge challenge to maintaining the overall system's integrity because there have to be safe ways to generate and store keys and also verify the provenance of the keys being used. In Holochain we have designed for a Distributed Public Key Infrastructure (DPKI) in which we leverage the distributed nature of Holochain itself to help manage all the aspects of this challenge. Data access paradigms \u00b6 The following are five data access paradigms. Note that in real-world scenarios it is common to mix these options by combining separate dApps. In instances when many separate dApps are needed to share data, Holochain supports bridging between dApps. Bridges between two networks with different data privacy models specify who can use the bridge, what data crosses the bridge, and tasks that might run in response to the bridge (e.g. notifications) The default model for Holochain data is public data shared on a public network, and every Holochain dApp has its own network and data, and creates networks for user-participants as soon as they join a dApp. The dApp code sets sharing and verification rules. Public, shared data on a public network \u00b6 Public data works like Bittorrent: anybody can join a network anybody can request any data they want from the network any data is available as long as at least one person is sharing it if some data is not shared by enough people, a new random person on the network must share it there is no \"local only\" data As stated above, an additional requirement for Holochain dApps is that new data must have a digital signature. Public, shared data on a private network \u00b6 The functionality is the same as a public network, but private networks use cryptography for access control to the network itself. Each device on the network must open a P2P connection to another device before it can send any data. The devices that are already on the private network send a challenge to any new device before sending any more data. The new device must sign the challenge with the network private key. The network public key is set in the dApp configuration, available to Holochain. Holochain can then refuse any connection with a bad challenge signature. Data within the network is public and shared. Every device on the network has \u201clogged in\u201d with a signed challenge, so has full access. Encrypted data on a public or private network \u00b6 Encryption relies on dApp developers encrypting and decrypting data within the dApp software. Holochain exposes a set of industry standard encryption algorithms (e.g. AES) to each dApp that cover both symmetric and asymmetric encryption options, in addition to hashing and signing tools. This option is very flexible for dApp developers but security becomes more subtle. Much like the private network, any one user-participant losing a key can become a serious security problem. Note that encryption can pose problems for Holochain's native validation method. Private, local only data \u00b6 Any data in a dApp can be validated and stored by its author without being shared to the network. Private, local data can provide a useful database for portable user preferences and notes and avoids the complexity of encryption and key-based security. Private data is hashed in the same way as public data, and the hash is public. Accordingly, one could tell that private data exists without being able to access it or take advantage of this with dApps that feature the eventual reveal of previously authored, private data -- think a distributed guessing game, like \"rock, paper, scissors\" or a digital classroom that operates with signatures disconnected from real-world identity and uses this method to prevent cheating. Hybrid model with servers \u00b6 Holochain supports many different environments and interfaces so that Holochain is easy to integrate with existing infrastructure. Any connected system with an API can push data through a dApp, as when one's phone sends a summary of private calendar data to a Holochain dApp. Any data in a dApp immediately becomes transparent, auditable and portable. The version of Holochain in active development covers the following integrations: Command line tools Web server Android Qt/QML Unity 3D games engine Security - best practices \u00b6 A great way to begin offsetting the governance crises now typical of distributed systems (i.e. DAO hack) is to think in terms of protecting and enabling the community of user-participants in addition to cryptography. In essence, one must consider how to prevent undesired access to the DHT. If membranes are not properly built in the dApps' DNA, having access to the source code also means having access to the entire network's entries via the DHT. Developers must treat the code, or at least the DNA taken as a whole, as if it's a key to the data. Note, too, that one can easily fork a Holochain dApp without disrupting its activity, making it possible to retain the benefits of open-source code without some of the risks. Membranes \u00b6 Security efforts begin with the specification of membranes, lest the code itself become a target. Though holochains rely on the cryptography above to create trust in data's provenance and immutability, trust is a distinctly human affair at the level of creating membranes. Different applications will require different levels of security, and Holochain is uniquely suited to accommodate a high degree of flexibility. DNA could define a closed list of participants, Proof of Service, or social triangulation requirements, for example. Sybil attacks are attacks launched through the creation of many nodes explicitly for the purpose of the attack. These nodes are identifiable by having no history. Blockchains prevent Sybil Attacks with Proof of Work. PoW is an implied membrane since it constrains who can verify blocks. In that case, the clearing node must have done \"work\" to maintain the network. Holochain requires membranes to identify and filter out Sybil nodes so that an attacker cannot use them to overrun the DHT. Immune system \u00b6 Holochain relies on random users across the network validating every piece of data. This keeps the verification rules reliable and unbiased. This is called an \"immune system\" for validating content. Note that when using encrypted data, it is not possible to verify contents without the appropriate key. Encryption is a choice that should be made carefully, as it undermines Holochain's native immune system. Scenarios to consider \u00b6 p2p platforms supply chains and open value networks social networks collaboration apps","title":"Planning a dApp"},{"location":"guide/planning_a_dapp/#planning-a-dapp","text":"","title":"Planning a dApp"},{"location":"guide/planning_a_dapp/#what-is-a-dapp","text":"A dApp is a distributed application. This means that the data associated with the application is stored by each user rather than in a central database.","title":"What is a dApp?"},{"location":"guide/planning_a_dapp/#basic-expectations-for-dapps","text":"Generally speaking, you need to know the following in order to build a Holochain dApp: how to install Holochain how to use the command line tools how to configure your application with a \"DNA\" file how to write your application code in a language that compiles to WebAssembly how to think through building a distributed application how to build a user interface for your app how to test your application code This article will help you plan a dApp by providing practical considerations about the specifics of distributed applications in general, and Holochain dApps in particular. It has been remarked that holochain dApps require us to make a mental shift, first from applications whose data is centrally organized, and also from blockchain-based, data-centric dApps. Here we will provide a basic overview of concepts from cryptography that are central to holochains. Then, we will consider the consequences of Holochain's cryptographic architecture for data permissioning, access, and security. Because app data storage is distributed amongst user-participants, one must expect that data encryption and permissions are important for protecting privacy in accordance with the jurisdictions in which the app is operating. Remember that, as user-participants leave the application, they take their data with them. They also retain copies of other data that they held to support the DHT. One must also re-think the dApp's business model such that it does not rely on a central authority's ability to whitelist access to a given resource.","title":"Basic expectations for dApps"},{"location":"guide/planning_a_dapp/#cryptography-in-holochain-dapps","text":"Distributed systems rely more heavily on cryptographic patterns and techniques than centralized systems. The basic concepts below explain how data integrity, ownership, and security are achieved natively with holochain's architecture. They are time-worn, relatively intuitive ideas that are critical for planning a holochain dApp.","title":"Cryptography in Holochain dApps"},{"location":"guide/planning_a_dapp/#hashes","text":"Hashes ensure the reliability of information by representing any given piece of data with a unique, consistent string of random looking characters. This makes changes to data visible because one can see that a hash has changed without needing to inspect the data itself. However, it is impossible to get the original data from a hash -- its purpose is to prove that the data to which it corresponds has not been altered. The same data consistently gives the same hash, and different data always give a completely different hash. These features imply that one can use small, portable hashes to verify data. One could also use a database containing data and their hashes as a table of contents, indexing (though not reading) data associated with a given hash. In the context of Holochain hashes are frequently used to look up content, both in our internal implementations as well as on the DHT. Therefore we frequently refer to the hash of some item (i.e. an entry on the chain) as its Address .","title":"Hashes"},{"location":"guide/planning_a_dapp/#signatures","text":"Signatures provide an additional type of data verification, answering the question \"who created this data?\" Signatures look like hashes. They are unique, reliable, and like hashes, cannot be used to retrieve the data to which they correspond. Signatures also come with a pair of keys. One is public, and the other private. The private key designates a unique author (or device), and the public key lets anyone verify a signature made by one specific private key. This key infrastructure addresses the problem of single points of failure associated with centralized systems by making authors responsible for securing their unique private key.","title":"Signatures"},{"location":"guide/planning_a_dapp/#encryption","text":"What if one needs to restrict access in addition to verifying data? Two types of encryption are possible. Symmetric encryption has one key for reading and writing data. Asymmetric encryption has two keys, where one creates messages and the other reads them. Encryption is a two way process, so the right key enables one to decrypt an encrypted message. With this added benefit come the drawbacks of the size of encrypted messages (at least as large as the original data) and broken encryption stripping the author of control of the original data.","title":"Encryption"},{"location":"guide/planning_a_dapp/#key-management","text":"Once you are using cryptographic functions, either signing or encrypting, managing and securing the keys that unlock these functions, becomes a huge challenge to maintaining the overall system's integrity because there have to be safe ways to generate and store keys and also verify the provenance of the keys being used. In Holochain we have designed for a Distributed Public Key Infrastructure (DPKI) in which we leverage the distributed nature of Holochain itself to help manage all the aspects of this challenge.","title":"Key Management"},{"location":"guide/planning_a_dapp/#data-access-paradigms","text":"The following are five data access paradigms. Note that in real-world scenarios it is common to mix these options by combining separate dApps. In instances when many separate dApps are needed to share data, Holochain supports bridging between dApps. Bridges between two networks with different data privacy models specify who can use the bridge, what data crosses the bridge, and tasks that might run in response to the bridge (e.g. notifications) The default model for Holochain data is public data shared on a public network, and every Holochain dApp has its own network and data, and creates networks for user-participants as soon as they join a dApp. The dApp code sets sharing and verification rules.","title":"Data access paradigms"},{"location":"guide/planning_a_dapp/#public-shared-data-on-a-public-network","text":"Public data works like Bittorrent: anybody can join a network anybody can request any data they want from the network any data is available as long as at least one person is sharing it if some data is not shared by enough people, a new random person on the network must share it there is no \"local only\" data As stated above, an additional requirement for Holochain dApps is that new data must have a digital signature.","title":"Public, shared data on a public network"},{"location":"guide/planning_a_dapp/#public-shared-data-on-a-private-network","text":"The functionality is the same as a public network, but private networks use cryptography for access control to the network itself. Each device on the network must open a P2P connection to another device before it can send any data. The devices that are already on the private network send a challenge to any new device before sending any more data. The new device must sign the challenge with the network private key. The network public key is set in the dApp configuration, available to Holochain. Holochain can then refuse any connection with a bad challenge signature. Data within the network is public and shared. Every device on the network has \u201clogged in\u201d with a signed challenge, so has full access.","title":"Public, shared data on a private network"},{"location":"guide/planning_a_dapp/#encrypted-data-on-a-public-or-private-network","text":"Encryption relies on dApp developers encrypting and decrypting data within the dApp software. Holochain exposes a set of industry standard encryption algorithms (e.g. AES) to each dApp that cover both symmetric and asymmetric encryption options, in addition to hashing and signing tools. This option is very flexible for dApp developers but security becomes more subtle. Much like the private network, any one user-participant losing a key can become a serious security problem. Note that encryption can pose problems for Holochain's native validation method.","title":"Encrypted data on a public or private network"},{"location":"guide/planning_a_dapp/#private-local-only-data","text":"Any data in a dApp can be validated and stored by its author without being shared to the network. Private, local data can provide a useful database for portable user preferences and notes and avoids the complexity of encryption and key-based security. Private data is hashed in the same way as public data, and the hash is public. Accordingly, one could tell that private data exists without being able to access it or take advantage of this with dApps that feature the eventual reveal of previously authored, private data -- think a distributed guessing game, like \"rock, paper, scissors\" or a digital classroom that operates with signatures disconnected from real-world identity and uses this method to prevent cheating.","title":"Private, local only data"},{"location":"guide/planning_a_dapp/#hybrid-model-with-servers","text":"Holochain supports many different environments and interfaces so that Holochain is easy to integrate with existing infrastructure. Any connected system with an API can push data through a dApp, as when one's phone sends a summary of private calendar data to a Holochain dApp. Any data in a dApp immediately becomes transparent, auditable and portable. The version of Holochain in active development covers the following integrations: Command line tools Web server Android Qt/QML Unity 3D games engine","title":"Hybrid model with servers"},{"location":"guide/planning_a_dapp/#security-best-practices","text":"A great way to begin offsetting the governance crises now typical of distributed systems (i.e. DAO hack) is to think in terms of protecting and enabling the community of user-participants in addition to cryptography. In essence, one must consider how to prevent undesired access to the DHT. If membranes are not properly built in the dApps' DNA, having access to the source code also means having access to the entire network's entries via the DHT. Developers must treat the code, or at least the DNA taken as a whole, as if it's a key to the data. Note, too, that one can easily fork a Holochain dApp without disrupting its activity, making it possible to retain the benefits of open-source code without some of the risks.","title":"Security - best practices"},{"location":"guide/planning_a_dapp/#membranes","text":"Security efforts begin with the specification of membranes, lest the code itself become a target. Though holochains rely on the cryptography above to create trust in data's provenance and immutability, trust is a distinctly human affair at the level of creating membranes. Different applications will require different levels of security, and Holochain is uniquely suited to accommodate a high degree of flexibility. DNA could define a closed list of participants, Proof of Service, or social triangulation requirements, for example. Sybil attacks are attacks launched through the creation of many nodes explicitly for the purpose of the attack. These nodes are identifiable by having no history. Blockchains prevent Sybil Attacks with Proof of Work. PoW is an implied membrane since it constrains who can verify blocks. In that case, the clearing node must have done \"work\" to maintain the network. Holochain requires membranes to identify and filter out Sybil nodes so that an attacker cannot use them to overrun the DHT.","title":"Membranes"},{"location":"guide/planning_a_dapp/#immune-system","text":"Holochain relies on random users across the network validating every piece of data. This keeps the verification rules reliable and unbiased. This is called an \"immune system\" for validating content. Note that when using encrypted data, it is not possible to verify contents without the appropriate key. Encryption is a choice that should be made carefully, as it undermines Holochain's native immune system.","title":"Immune system"},{"location":"guide/planning_a_dapp/#scenarios-to-consider","text":"p2p platforms supply chains and open value networks social networks collaboration apps","title":"Scenarios to consider"},{"location":"guide/production_conductor/","text":"Production Conductor \u00b6 In addition to the zero config development Conductor using hc run , there is a highly configurable sophisticated Conductor for running DNA instances long term. This Conductor will play an important role in making the use of Holochain truly easy for end users, because it supports all the functionality that those users are likely to want, in terms of managing their Holochain apps, or hApps, just on a low level. On that note, a graphical user interface that exposes all the functionality of this Conductor to users is under development. For now, use of this Conductor must happen mostly manually, and by tech-savvy users or developers. This Conductor is simply a command line tool called holochain . Its only function is to boot a Conductor based on a configuration file, and optionally, the ability to write changes back to that file. Within that Conductor many DNA instances can be run for one or more agents, multiple types of interfaces to the APIs can be exposed, UI file bundles can be served, and logs from all of that can be accessed. The first step to using holochain is of course installing it. Instructions for installation can be found in its README . If you wish to attempt any of the things you read in this chapter while going through it, you will need to have installed the executable. Like Holochain core, this particular Conductor is written in Rust. View it on GitHub here . To understand how to configure the holochain Conductor, check out the next article .","title":"Production Conductor"},{"location":"guide/production_conductor/#production-conductor","text":"In addition to the zero config development Conductor using hc run , there is a highly configurable sophisticated Conductor for running DNA instances long term. This Conductor will play an important role in making the use of Holochain truly easy for end users, because it supports all the functionality that those users are likely to want, in terms of managing their Holochain apps, or hApps, just on a low level. On that note, a graphical user interface that exposes all the functionality of this Conductor to users is under development. For now, use of this Conductor must happen mostly manually, and by tech-savvy users or developers. This Conductor is simply a command line tool called holochain . Its only function is to boot a Conductor based on a configuration file, and optionally, the ability to write changes back to that file. Within that Conductor many DNA instances can be run for one or more agents, multiple types of interfaces to the APIs can be exposed, UI file bundles can be served, and logs from all of that can be accessed. The first step to using holochain is of course installing it. Instructions for installation can be found in its README . If you wish to attempt any of the things you read in this chapter while going through it, you will need to have installed the executable. Like Holochain core, this particular Conductor is written in Rust. View it on GitHub here . To understand how to configure the holochain Conductor, check out the next article .","title":"Production Conductor"},{"location":"guide/project_source_folders/","text":"Project Source Folders \u00b6 The source code folder for a Holochain DNA project looks something like this, where the ellipses (...) indicate a folder - test - ... - zomes - ... - .gitignore - .hcignore - app.json test contains some starter code for writing tests. zomes will contain sub-folders, each of which represents a \"Zome\", which can be thought of as a submodule of the source code of your DNA. .gitignore contains useful defaults for ignoring files when using GIT version control. .hcignore is utilized by the packaging commands of the hc command line tools app.json is the top level configuration of your DNA. Carry on to the next article to see about making changes to the configuration of a new project.","title":"Project Source Folders"},{"location":"guide/project_source_folders/#project-source-folders","text":"The source code folder for a Holochain DNA project looks something like this, where the ellipses (...) indicate a folder - test - ... - zomes - ... - .gitignore - .hcignore - app.json test contains some starter code for writing tests. zomes will contain sub-folders, each of which represents a \"Zome\", which can be thought of as a submodule of the source code of your DNA. .gitignore contains useful defaults for ignoring files when using GIT version control. .hcignore is utilized by the packaging commands of the hc command line tools app.json is the top level configuration of your DNA. Carry on to the next article to see about making changes to the configuration of a new project.","title":"Project Source Folders"},{"location":"guide/running_tests/","text":"Running Tests \u00b6 By default, when you use hc init to create a new project folder , it creates a sub-directory called test . The files in that folder are equipped for testing your project. The contents of the folder represent a simple nodejs package (in that they have a index.js file and a package.json file). Tools to help with testing are also built right into the development command line tools . hc test \u00b6 Once you have a project folder initiated, you can run hc test to execute your tests. This combines the following steps: 1. Packaging your files into a DNA file, located at dist/your.dna.json . This step will fail if your packaging step fails. 2. Installing build and testing dependencies, if they're not installed ( npm install ) 3. Executing the test file found at test/index.js ( node test/index.js ) The tests can of course be called manually using nodejs, but you will find that using the convenience of the hc test command makes the process much smoother, since it includes the packaging step for when you change the DNA source files. hc test also has some configurable options. If you want to run it without repackaging the DNA, run it with hc test --skip-package If your tests are in a different folder than test , run it with hc test --dir tests where tests is the name of the folder. If the file you wish to actually execute is somewhere besides test/index.js then run it with hc test --testfile test/test.js where test/test.js is the path of the file.","title":"Running Tests"},{"location":"guide/running_tests/#running-tests","text":"By default, when you use hc init to create a new project folder , it creates a sub-directory called test . The files in that folder are equipped for testing your project. The contents of the folder represent a simple nodejs package (in that they have a index.js file and a package.json file). Tools to help with testing are also built right into the development command line tools .","title":"Running Tests"},{"location":"guide/running_tests/#hc-test","text":"Once you have a project folder initiated, you can run hc test to execute your tests. This combines the following steps: 1. Packaging your files into a DNA file, located at dist/your.dna.json . This step will fail if your packaging step fails. 2. Installing build and testing dependencies, if they're not installed ( npm install ) 3. Executing the test file found at test/index.js ( node test/index.js ) The tests can of course be called manually using nodejs, but you will find that using the convenience of the hc test command makes the process much smoother, since it includes the packaging step for when you change the DNA source files. hc test also has some configurable options. If you want to run it without repackaging the DNA, run it with hc test --skip-package If your tests are in a different folder than test , run it with hc test --dir tests where tests is the name of the folder. If the file you wish to actually execute is somewhere besides test/index.js then run it with hc test --testfile test/test.js where test/test.js is the path of the file.","title":"hc test"},{"location":"guide/scenario_testing/","text":"Scenario Testing \u00b6 Scenario is a class that is exported from holochain-nodejs and can be imported into your code. It can be used to run tests individually for a single node, or to orchestrate multi-node tests, which is why it is called Scenario . It does all the work of starting and stopping conductors and integrating with various test harnesses. Import Example \u00b6 const { Scenario } = require ( '@holochain/holochain-nodejs' )","title":"Scenario Testing"},{"location":"guide/scenario_testing/#scenario-testing","text":"Scenario is a class that is exported from holochain-nodejs and can be imported into your code. It can be used to run tests individually for a single node, or to orchestrate multi-node tests, which is why it is called Scenario . It does all the work of starting and stopping conductors and integrating with various test harnesses.","title":"Scenario Testing"},{"location":"guide/scenario_testing/#import-example","text":"const { Scenario } = require ( '@holochain/holochain-nodejs' )","title":"Import Example"},{"location":"guide/scenario_testing_running_tape/","text":"Run Tests With Tape \u00b6 scenario.runTape(description, runner) => null \u00b6 Each invocation of scenario.runTape does the following: Starts a fresh Conductor based on the configuration used to construct scenario Starts a new tape test Injects the values needed for the test into a closure you provide Automatically ends the test and stops the conductor when the closure is done running It will error if you have not called Scenario.setTape first. Name description Type string Description Will be used to initialize the tape test, and should describe that which is being tested in this scenario Name runner Type function Description runner is a closure: (t, runner) => { (code to run) } . When this function ends, the test is automatically ended, and the inner Conductor is stopped. - t is the object that tape tests use - runner is an object containing an interface into each Instance specified in the config. The Instances are keyed by \"name\", as taken from the optional third parameter of Config.instance , which itself defaults to what was given in Config.agent . Example \u00b6 scenario . runTape ( \"test something\" , ( t , runner ) => { const alice = runner . alice const bob = runner . bob // fire zome function calls from both agents const result1 = alice . call ( 'zome' , 'function' , { params : 'go here' }) const result2 = bob . call ( 'zome' , 'function' , { params : 'go here' }) // make some tape assertions t . ok ( result1 ) t . equal ( result2 , 'expected value' ) }) // Run another test in a freshly created Conductor // This example uses destructuring to show a clean and simple way to get the Instances scenario . runTape ( \"test something else\" , ( t , { alice , bob }) => { // write more tests in the same fashion })","title":"Running With Tape"},{"location":"guide/scenario_testing_running_tape/#run-tests-with-tape","text":"","title":"Run Tests With Tape"},{"location":"guide/scenario_testing_running_tape/#scenarioruntapedescription-runner-null","text":"Each invocation of scenario.runTape does the following: Starts a fresh Conductor based on the configuration used to construct scenario Starts a new tape test Injects the values needed for the test into a closure you provide Automatically ends the test and stops the conductor when the closure is done running It will error if you have not called Scenario.setTape first. Name description Type string Description Will be used to initialize the tape test, and should describe that which is being tested in this scenario Name runner Type function Description runner is a closure: (t, runner) => { (code to run) } . When this function ends, the test is automatically ended, and the inner Conductor is stopped. - t is the object that tape tests use - runner is an object containing an interface into each Instance specified in the config. The Instances are keyed by \"name\", as taken from the optional third parameter of Config.instance , which itself defaults to what was given in Config.agent .","title":"scenario.runTape(description, runner) =&gt; null"},{"location":"guide/scenario_testing_running_tape/#example","text":"scenario . runTape ( \"test something\" , ( t , runner ) => { const alice = runner . alice const bob = runner . bob // fire zome function calls from both agents const result1 = alice . call ( 'zome' , 'function' , { params : 'go here' }) const result2 = bob . call ( 'zome' , 'function' , { params : 'go here' }) // make some tape assertions t . ok ( result1 ) t . equal ( result2 , 'expected value' ) }) // Run another test in a freshly created Conductor // This example uses destructuring to show a clean and simple way to get the Instances scenario . runTape ( \"test something else\" , ( t , { alice , bob }) => { // write more tests in the same fashion })","title":"Example"},{"location":"guide/scenario_testing_setup/","text":"Scenario Testing Setup \u00b6 constructor(instancesArray, conductorOptions) => Scenario \u00b6 Instantiate a Scenario with an array of instance configurations and some optional configuration overrides. Note that this function has the same signature as Config.conductor . Name instancesArray Type array Description Pass in an array of instance configuration objects generated by Config.instance to have them within the final configuration to be instantiated by the Conductor Name conductorOptions Optional Type object Description conductorOptions.debugLog boolean Enables debug logging. The logger produces nice, colorful output of the internal workings of Holochain. Default { debugLog: false } Example \u00b6 const dna = Config . dna ( \"path/to/happ.dna.json\" ) const instanceAlice = Config . instance ( Config . agent ( \"alice\" ), dna ) const instanceBob = Config . instance ( Config . agent ( \"bob\" ), dna ) const scenario = new Scenario ([ instanceAlice ]) With conductorOptions Example \u00b6 const dna = Config . dna ( \"path/to/happ.dna.json\" ) const instanceAlice = Config . instance ( Config . agent ( \"alice\" ), dna ) const instanceBob = Config . instance ( Config . agent ( \"bob\" ), dna ) const scenario = new Scenario ([ instanceAlice ], { debugLog : true }) Inject Tape Version \u00b6 Scenario.setTape(tape) => null \u00b6 setTape should be called prior to usage of the Scenario class, if you intend to use tape as your testing framework. It sets a reference internally to the version of tape, since there are many variations, that you wish to use. It is used in conjunction with Scenario.runTape . Name tape Type object Description A reference to the imported tape package you wish to use as your test framework. Example \u00b6 const { Config , Scenario } = require ( '@holochain/holochain-nodejs' ) Scenario . setTape ( require ( 'tape' )) Full Multiple Instances Example \u00b6 The following example shows the simplest, most convenient way to start writing scenario tests with this module. We'll set up an environment for running tests against two instances of one DNA, using the tape test harness: const { Config , Scenario } = require ( '@holochain/holochain-nodejs' ) Scenario . setTape ( require ( 'tape' )) // specify two agents... const agentAlice = Config . agent ( \"alice\" ) const agentBob = Config . agent ( \"bob\" ) // ...and one DNA... const dna = Config . dna ( \"path/to/happ.dna.json\" ) // ...then make instances out of them... const instanceAlice = Config . instance ( agentAlice , dna ) const instanceBob = Config . instance ( agentBob , dna ) // Now we can construct a `scenario` object which lets us run as many scenario tests as we want involving the two instances we set up: const scenario = new Scenario ([ instanceAlice , instanceBob ])","title":"Setup"},{"location":"guide/scenario_testing_setup/#scenario-testing-setup","text":"","title":"Scenario Testing Setup"},{"location":"guide/scenario_testing_setup/#constructorinstancesarray-conductoroptions-scenario","text":"Instantiate a Scenario with an array of instance configurations and some optional configuration overrides. Note that this function has the same signature as Config.conductor . Name instancesArray Type array Description Pass in an array of instance configuration objects generated by Config.instance to have them within the final configuration to be instantiated by the Conductor Name conductorOptions Optional Type object Description conductorOptions.debugLog boolean Enables debug logging. The logger produces nice, colorful output of the internal workings of Holochain. Default { debugLog: false }","title":"constructor(instancesArray, conductorOptions) =&gt; Scenario"},{"location":"guide/scenario_testing_setup/#example","text":"const dna = Config . dna ( \"path/to/happ.dna.json\" ) const instanceAlice = Config . instance ( Config . agent ( \"alice\" ), dna ) const instanceBob = Config . instance ( Config . agent ( \"bob\" ), dna ) const scenario = new Scenario ([ instanceAlice ])","title":"Example"},{"location":"guide/scenario_testing_setup/#with-conductoroptions-example","text":"const dna = Config . dna ( \"path/to/happ.dna.json\" ) const instanceAlice = Config . instance ( Config . agent ( \"alice\" ), dna ) const instanceBob = Config . instance ( Config . agent ( \"bob\" ), dna ) const scenario = new Scenario ([ instanceAlice ], { debugLog : true })","title":"With conductorOptions Example"},{"location":"guide/scenario_testing_setup/#inject-tape-version","text":"","title":"Inject Tape Version"},{"location":"guide/scenario_testing_setup/#scenariosettapetape-null","text":"setTape should be called prior to usage of the Scenario class, if you intend to use tape as your testing framework. It sets a reference internally to the version of tape, since there are many variations, that you wish to use. It is used in conjunction with Scenario.runTape . Name tape Type object Description A reference to the imported tape package you wish to use as your test framework.","title":"Scenario.setTape(tape) =&gt; null"},{"location":"guide/scenario_testing_setup/#example_1","text":"const { Config , Scenario } = require ( '@holochain/holochain-nodejs' ) Scenario . setTape ( require ( 'tape' ))","title":"Example"},{"location":"guide/scenario_testing_setup/#full-multiple-instances-example","text":"The following example shows the simplest, most convenient way to start writing scenario tests with this module. We'll set up an environment for running tests against two instances of one DNA, using the tape test harness: const { Config , Scenario } = require ( '@holochain/holochain-nodejs' ) Scenario . setTape ( require ( 'tape' )) // specify two agents... const agentAlice = Config . agent ( \"alice\" ) const agentBob = Config . agent ( \"bob\" ) // ...and one DNA... const dna = Config . dna ( \"path/to/happ.dna.json\" ) // ...then make instances out of them... const instanceAlice = Config . instance ( agentAlice , dna ) const instanceBob = Config . instance ( agentBob , dna ) // Now we can construct a `scenario` object which lets us run as many scenario tests as we want involving the two instances we set up: const scenario = new Scenario ([ instanceAlice , instanceBob ])","title":"Full Multiple Instances Example"},{"location":"guide/source_chain/","text":"Source Chain \u00b6","title":"(E) Source Chain"},{"location":"guide/source_chain/#source-chain","text":"","title":"Source Chain"},{"location":"guide/state_actions/","text":"State & Actions \u00b6 Holochain uses a hybrid global/local state model. In our bio mimicry terms the global state is for \"short term memory\" and local state wraps references to \"long term memory\". The global state is implemented as Redux style reducers. Any module can dispatch an action to the global state. The action will be \"reduced\" to a new state tree value by the modules responsible for each branch of the state tree. The response values from a reduction must be polled directly from the state tree in a thread using a \"sensor\" closure in an observer. Actions are stateless/immutable data structures that are dispatched by modules to communicate a request to do something potentially state changing. Everything in the system should be either stateless or change state only in response to an incoming action. The global state is called \"short term memory\" because it is highly dynamic, readily inspectable, and volatile. It does not survive indefinitely and is best thought of as a cache of recent history. Local state is implemented using actors to co-ordinate memory and threads in Rust for external, persistent state. The classic example is a database connection to the database that stores entries and headers. The db actor receives read/write messages, and a reference to the sender is stored in the global state. Actions \u00b6 The action module defines actions and action wrappers: ActionWrapper : struct contains a unique ID for the action and the Action Action : enum of specific data to a given action, e.g. Action::Commit Processing an incoming action is a 3 step process: Implement reduce to resolve and dispatch to a handler Resolve the action to an appropriate handler Implement handler logic Reduce \u00b6 The reduce implementation is essentially copypasta. It handles resolving and dispatching to a handler with a new state clone. The handler resolution and dispatch logic should be split to facilitate clean unit testing. pub fn reduce ( old_state : Arc < FooState > , action_wrapper : & ActionWrapper , action_channel : & Sender < ActionWrapper > , observer_channel : & Sender < Observer > , ) -> Arc < AgentState > { let handler = resolve_action_handler ( action_wrapper ); match handler { Some ( f ) => { let mut new_state : FooState = ( * old_state ). clone (); f ( & mut new_state , & action_wrapper , action_channel , observer_channel ); Arc :: new ( new_state ) } None => old_state , } } Resolve an appropriate handler \u00b6 The action handler should map signals to action handlers. fn resolve_action_handler ( action_wrapper : & ActionWrapper , ) -> Option < fn ( & mut AgentState , & ActionWrapper , & Sender < ActionWrapper > , & Sender < Observer > ) > { match action_wrapper . action () { Action :: Commit ( _ , _ ) => Some ( handle_commit ), Action :: Query ( _ ) => Some ( handle_get ), _ => None , } } Implement the handlers \u00b6 Each handler should respond to one action signal and mutate the relevant state. The standard pattern is to maintain a HashMap of incoming action wrappers against the result of their action from the perspective of the current module. Each action wrapper has a unique id internally so there will be no key collisions. fn handle_foo ( state : & mut FooState , action_wrapper : & ActionWrapper , _action_channel : & Sender < ActionWrapper > , _observer_channel : & Sender < Observer > , ) { let action = action_wrapper . action (); let bar = unwrap_to ! ( action => Action :: Bar ); // do something with bar... let result = bar . do_something (); state . actions . insert ( action_wrapper . clone (), ActionResponse :: Bar ( result . clone ())); } WARNING: Actions are reduced in a simple loop. Holochain will hang if you dispatch and block on a new action while an outer action reduction is also blocking, waiting for a response. Global state \u00b6 instance::Instance has a state::State which is the one global state. Each stateful module has a state.rs module containing sub-state slices. See src/agent/state.rs and src/nucleus/state.rs and how they are put together in src/state.rs . State is read from the instance through relevant getter methods: instance . state (). nucleus (). dna () and mutated by dispatching an action: let entry = Entry :: App ( ... ); let action_wrapper = ActionWrapper :: new ( & Action :: Commit ( entry )); instance . dispatch ( action_wrapper ); Instance calls reduce on the state with the next action to consume: pub fn consume_next_action ( & mut self ) { if self . pending_actions . len () > 0 { let action = self . pending_actions . pop_front (). unwrap (); self . state = self . state . clone (). reduce ( & action ); } } The main reducer creates a new State object and calls the sub-reducers: pub fn reduce ( & mut self , action_wrapper : & ActionWrapper ) -> Self { let mut new_state = State { nucleus : :: nucleus :: reduce ( ... ), agent : :: agent :: reduce ( ... ) } new_state . history . insert ( action_wrapper ); new_state } Each incoming action wrapper is logged in the main state history to facilitate testing and \"time travel\" debugging. Sub-module state slices are included in state::State as counted references. The sub-module reducer must choose to either: If mutations happen, return a cloned, mutated state slice with a new reference If no mutations happen, return the reference to the original state slice The reduce copypasta above demonstrates this as the possible return values. Redux in Rust code was used as a reference from this repository . Local state \u00b6 Coming Soon.","title":"State & Actions"},{"location":"guide/state_actions/#state-actions","text":"Holochain uses a hybrid global/local state model. In our bio mimicry terms the global state is for \"short term memory\" and local state wraps references to \"long term memory\". The global state is implemented as Redux style reducers. Any module can dispatch an action to the global state. The action will be \"reduced\" to a new state tree value by the modules responsible for each branch of the state tree. The response values from a reduction must be polled directly from the state tree in a thread using a \"sensor\" closure in an observer. Actions are stateless/immutable data structures that are dispatched by modules to communicate a request to do something potentially state changing. Everything in the system should be either stateless or change state only in response to an incoming action. The global state is called \"short term memory\" because it is highly dynamic, readily inspectable, and volatile. It does not survive indefinitely and is best thought of as a cache of recent history. Local state is implemented using actors to co-ordinate memory and threads in Rust for external, persistent state. The classic example is a database connection to the database that stores entries and headers. The db actor receives read/write messages, and a reference to the sender is stored in the global state.","title":"State &amp; Actions"},{"location":"guide/state_actions/#actions","text":"The action module defines actions and action wrappers: ActionWrapper : struct contains a unique ID for the action and the Action Action : enum of specific data to a given action, e.g. Action::Commit Processing an incoming action is a 3 step process: Implement reduce to resolve and dispatch to a handler Resolve the action to an appropriate handler Implement handler logic","title":"Actions"},{"location":"guide/state_actions/#reduce","text":"The reduce implementation is essentially copypasta. It handles resolving and dispatching to a handler with a new state clone. The handler resolution and dispatch logic should be split to facilitate clean unit testing. pub fn reduce ( old_state : Arc < FooState > , action_wrapper : & ActionWrapper , action_channel : & Sender < ActionWrapper > , observer_channel : & Sender < Observer > , ) -> Arc < AgentState > { let handler = resolve_action_handler ( action_wrapper ); match handler { Some ( f ) => { let mut new_state : FooState = ( * old_state ). clone (); f ( & mut new_state , & action_wrapper , action_channel , observer_channel ); Arc :: new ( new_state ) } None => old_state , } }","title":"Reduce"},{"location":"guide/state_actions/#resolve-an-appropriate-handler","text":"The action handler should map signals to action handlers. fn resolve_action_handler ( action_wrapper : & ActionWrapper , ) -> Option < fn ( & mut AgentState , & ActionWrapper , & Sender < ActionWrapper > , & Sender < Observer > ) > { match action_wrapper . action () { Action :: Commit ( _ , _ ) => Some ( handle_commit ), Action :: Query ( _ ) => Some ( handle_get ), _ => None , } }","title":"Resolve an appropriate handler"},{"location":"guide/state_actions/#implement-the-handlers","text":"Each handler should respond to one action signal and mutate the relevant state. The standard pattern is to maintain a HashMap of incoming action wrappers against the result of their action from the perspective of the current module. Each action wrapper has a unique id internally so there will be no key collisions. fn handle_foo ( state : & mut FooState , action_wrapper : & ActionWrapper , _action_channel : & Sender < ActionWrapper > , _observer_channel : & Sender < Observer > , ) { let action = action_wrapper . action (); let bar = unwrap_to ! ( action => Action :: Bar ); // do something with bar... let result = bar . do_something (); state . actions . insert ( action_wrapper . clone (), ActionResponse :: Bar ( result . clone ())); } WARNING: Actions are reduced in a simple loop. Holochain will hang if you dispatch and block on a new action while an outer action reduction is also blocking, waiting for a response.","title":"Implement the handlers"},{"location":"guide/state_actions/#global-state","text":"instance::Instance has a state::State which is the one global state. Each stateful module has a state.rs module containing sub-state slices. See src/agent/state.rs and src/nucleus/state.rs and how they are put together in src/state.rs . State is read from the instance through relevant getter methods: instance . state (). nucleus (). dna () and mutated by dispatching an action: let entry = Entry :: App ( ... ); let action_wrapper = ActionWrapper :: new ( & Action :: Commit ( entry )); instance . dispatch ( action_wrapper ); Instance calls reduce on the state with the next action to consume: pub fn consume_next_action ( & mut self ) { if self . pending_actions . len () > 0 { let action = self . pending_actions . pop_front (). unwrap (); self . state = self . state . clone (). reduce ( & action ); } } The main reducer creates a new State object and calls the sub-reducers: pub fn reduce ( & mut self , action_wrapper : & ActionWrapper ) -> Self { let mut new_state = State { nucleus : :: nucleus :: reduce ( ... ), agent : :: agent :: reduce ( ... ) } new_state . history . insert ( action_wrapper ); new_state } Each incoming action wrapper is logged in the main state history to facilitate testing and \"time travel\" debugging. Sub-module state slices are included in state::State as counted references. The sub-module reducer must choose to either: If mutations happen, return a cloned, mutated state slice with a new reference If no mutations happen, return the reference to the original state slice The reduce copypasta above demonstrates this as the possible return values. Redux in Rust code was used as a reference from this repository .","title":"Global state"},{"location":"guide/state_actions/#local-state","text":"Coming Soon.","title":"Local state"},{"location":"guide/testing_checking_results/","text":"(E) Checking Results \u00b6","title":"(E) Checking Results"},{"location":"guide/testing_checking_results/#e-checking-results","text":"","title":"(E) Checking Results"},{"location":"guide/testing_configuration/","text":"Configuration \u00b6 Config is an object with helper functions for configuration that is exported from holochain-nodejs and can be imported into your code. The functions can be combined to produce a valid configuration object to instantiate a Conductor instance with. Import Example \u00b6 const { Config } = require ( '@holochain/holochain-nodejs' ) Agent \u00b6 Config.agent(agentName) => object \u00b6 Takes an agent name and creates a simple configuration object for that agent Name agentName Type string Description An identifying string for this agent Example \u00b6 const agentConfig = Config . agent ( 'alice' ) console . log ( agentConfig ) /* { name: 'alice' } */ DNA \u00b6 Config.dna(dnaPath, [dnaName]) => object \u00b6 Takes a path to a valid DNA package, and optionally a name and creates a simple configuration object for that DNA Name dnaPath Type string Description The path to a .dna.json file containing a valid DNA configuration Name dnaName Optional Type string Description The path to a .dna.json file containing a valid DNA configuration Default The same as the given dnaPath Example \u00b6 const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) console . log ( dnaConfig ) /* { path: 'path/to/your.dna.json', name: 'path/to/your.dna.json' } */ Instances \u00b6 Config.instance(agentConfig, dnaConfig, [name]) => object \u00b6 Takes an agent config object and a dna confid object, and optionally a unique name, and returns a full configuration object for a DNA instance. Name agentConfig Type object Description A config object with a name property, as produced by Config.agent Name dnaConfig Type object Description A config object with a name and path property, as produced by Config.dna Name name Optional Type string Description The name acts like the instance ID, and in fact will be used as such when calling Zome functions Default The same as the name property of the given agentConfig ( agentConfig.name ) Example \u00b6 const agentConfig = Config . agent ( 'alice' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig = Config . instance ( agentConfig , dnaConfig ) console . log ( dnaConfig ) /* { agent: { name: 'alice' }, dna: { path: 'path/to/your.dna.json', name: 'path/to/your.dna.json' }, name: 'alice' } */ Bridges \u00b6 Config.bridge(handle, callerInstanceConfig, calleeInstanceConfig) => object \u00b6 Takes three arguments: the bridge handle, the caller, and the callee (both instances) Name handle Type string Description The desired bridge handle, which is used by the \"caller\" DNA to refer to the \"callee\" DNA. See the bridging section of the docs for more detail. Name callerInstanceConfig Type object Description A config object as produced by Config.instance , which specifies the instance which will be making calls over the bridge Name calleeInstanceConfig Type object Description A config object as produced by Config.instance , which specifies the instance which will be receiving calls over the bridge Example \u00b6 const agentConfig1 = Config . agent ( 'alice' ) const agentConfig2 = Config . agent ( 'bob' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig1 = Config . instance ( agentConfig1 , dnaConfig ) const instanceConfig2 = Config . instance ( agentConfig2 , dnaConfig ) const bridgeConfig = Config . bridge ( 'bridge-handle' , instanceConfig1 , instanceConfig2 ) console . log ( bridgeConfig ) /* { handle: 'bridge-handle', caller_id: 'alice', callee_id: 'bob' } */ DPKI \u00b6 Config.dpki(instanceConfig, initParams) => object \u00b6 Takes two arguments: an instance object, as specified by Config.instance , and an object which gets passed into the init_params conductor config object. Name instanceConfig Type object Description A config object with a name property, as produced by Config.instance Name initParams Type object Description A config object which will be passed directly through to the conductor config (as dpki.init_params ) Full Conductor Configuration \u00b6 Config.conductor(conductorOptions) => object \u00b6 Config.conductor(instancesArray, [conductorOptions]) => object \u00b6 There are two ways to construct a valid Conductor configuration from these Config helpers. Using the first way, you put all the config data into a single required object. Using the second \"shorthand\" style, you specify an array of Config.instance data, along with an optional object of extra options. The second way can be more convenient when you are just trying to set up a collection of instances with nothing extra options. Consumes an array of configured instances and produces an object which is a fully valid Conductor configuration. It can be passed into the Conductor constructor, which is covered in the next articles. This function is mostly useful in conjunction with manually instantiating a Conductor . Name conductorOptions Optional Type object Description conductorOptions.instances array Pass in an array of instance configuration objects generated by Config.instance to have them within the final configuration to be instantiated by the Conductor. Note: If using the two-argument \"shorthand\" style of Config.conductor , the first instancesArray argument will override this property. Description conductorOptions.bridges array Pass in an array of instance configuration objects generated by Config.bridges to have them within the final configuration to be instantiated by the Conductor Description conductorOptions.debugLog boolean Enables debug logging. The logger produces nice, colorful output of the internal workings of Holochain. Default { debugLog: false } Name instancesArray Type array Description When using the two-argument \"shorthand\" style of Config.conductor , you can specify the list of instances as the first argument, rather than folding it into the conductorOptions object. Example \u00b6 const agentConfig = Config . agent ( 'alice' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig = Config . instance ( agentConfig , dnaConfig ) const conductorConfig = Config . conductor ({ instances : [ instanceConfig ] }) Or, equivalently, using the shorthand style: const conductorConfig = Config . conductor ([ instanceConfig ]) Example With conductorOptions \u00b6 const agentConfig = Config . agent ( 'alice' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig = Config . instance ( agentConfig , dnaConfig ) const conductorConfig = Config . conductor ({ instances : [ instanceConfig ], debugLog : true }) Or, equivalently, using the shorthand style: const conductorConfig = Config . conductor ([ instanceConfig ], { debugLog : true }) Multiple Instances Example, with Bridges \u00b6 const { Config } = require ( '@holochain/holochain-nodejs' ) // specify two agents... const aliceName = \"alice\" const bobName = \"bob\" const agentAlice = Config . agent ( aliceName ) const agentBob = Config . agent ( bobName ) // ...and one DNA... const dnaPath = \"path/to/happ.dna.json\" const dna = Config . dna ( dnaPath ) // ...then make instances out of them... const instanceAlice = Config . instance ( agentAlice , dna ) const instanceBob = Config . instance ( agentBob , dna ) const bridgeForward = Config . bridge ( 'bridge-forward' , instanceAlice , instanceBob ) const bridgeBackward = Config . bridge ( 'bridge-backward' , instanceAlice , instanceBob ) // ...and finally throw them all together const config = Config . conductor ({ instances : [ instanceAlice , instanceBob ], bridges : [ bridgeForward , bridgeBackward ] })","title":"Configuration"},{"location":"guide/testing_configuration/#configuration","text":"Config is an object with helper functions for configuration that is exported from holochain-nodejs and can be imported into your code. The functions can be combined to produce a valid configuration object to instantiate a Conductor instance with.","title":"Configuration"},{"location":"guide/testing_configuration/#import-example","text":"const { Config } = require ( '@holochain/holochain-nodejs' )","title":"Import Example"},{"location":"guide/testing_configuration/#agent","text":"","title":"Agent"},{"location":"guide/testing_configuration/#configagentagentname-object","text":"Takes an agent name and creates a simple configuration object for that agent Name agentName Type string Description An identifying string for this agent","title":"Config.agent(agentName) =&gt; object"},{"location":"guide/testing_configuration/#example","text":"const agentConfig = Config . agent ( 'alice' ) console . log ( agentConfig ) /* { name: 'alice' } */","title":"Example"},{"location":"guide/testing_configuration/#dna","text":"","title":"DNA"},{"location":"guide/testing_configuration/#configdnadnapath-dnaname-object","text":"Takes a path to a valid DNA package, and optionally a name and creates a simple configuration object for that DNA Name dnaPath Type string Description The path to a .dna.json file containing a valid DNA configuration Name dnaName Optional Type string Description The path to a .dna.json file containing a valid DNA configuration Default The same as the given dnaPath","title":"Config.dna(dnaPath, [dnaName]) =&gt; object"},{"location":"guide/testing_configuration/#example_1","text":"const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) console . log ( dnaConfig ) /* { path: 'path/to/your.dna.json', name: 'path/to/your.dna.json' } */","title":"Example"},{"location":"guide/testing_configuration/#instances","text":"","title":"Instances"},{"location":"guide/testing_configuration/#configinstanceagentconfig-dnaconfig-name-object","text":"Takes an agent config object and a dna confid object, and optionally a unique name, and returns a full configuration object for a DNA instance. Name agentConfig Type object Description A config object with a name property, as produced by Config.agent Name dnaConfig Type object Description A config object with a name and path property, as produced by Config.dna Name name Optional Type string Description The name acts like the instance ID, and in fact will be used as such when calling Zome functions Default The same as the name property of the given agentConfig ( agentConfig.name )","title":"Config.instance(agentConfig, dnaConfig, [name]) =&gt; object"},{"location":"guide/testing_configuration/#example_2","text":"const agentConfig = Config . agent ( 'alice' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig = Config . instance ( agentConfig , dnaConfig ) console . log ( dnaConfig ) /* { agent: { name: 'alice' }, dna: { path: 'path/to/your.dna.json', name: 'path/to/your.dna.json' }, name: 'alice' } */","title":"Example"},{"location":"guide/testing_configuration/#bridges","text":"","title":"Bridges"},{"location":"guide/testing_configuration/#configbridgehandle-callerinstanceconfig-calleeinstanceconfig-object","text":"Takes three arguments: the bridge handle, the caller, and the callee (both instances) Name handle Type string Description The desired bridge handle, which is used by the \"caller\" DNA to refer to the \"callee\" DNA. See the bridging section of the docs for more detail. Name callerInstanceConfig Type object Description A config object as produced by Config.instance , which specifies the instance which will be making calls over the bridge Name calleeInstanceConfig Type object Description A config object as produced by Config.instance , which specifies the instance which will be receiving calls over the bridge","title":"Config.bridge(handle, callerInstanceConfig, calleeInstanceConfig) =&gt; object"},{"location":"guide/testing_configuration/#example_3","text":"const agentConfig1 = Config . agent ( 'alice' ) const agentConfig2 = Config . agent ( 'bob' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig1 = Config . instance ( agentConfig1 , dnaConfig ) const instanceConfig2 = Config . instance ( agentConfig2 , dnaConfig ) const bridgeConfig = Config . bridge ( 'bridge-handle' , instanceConfig1 , instanceConfig2 ) console . log ( bridgeConfig ) /* { handle: 'bridge-handle', caller_id: 'alice', callee_id: 'bob' } */","title":"Example"},{"location":"guide/testing_configuration/#dpki","text":"","title":"DPKI"},{"location":"guide/testing_configuration/#configdpkiinstanceconfig-initparams-object","text":"Takes two arguments: an instance object, as specified by Config.instance , and an object which gets passed into the init_params conductor config object. Name instanceConfig Type object Description A config object with a name property, as produced by Config.instance Name initParams Type object Description A config object which will be passed directly through to the conductor config (as dpki.init_params )","title":"Config.dpki(instanceConfig, initParams) =&gt; object"},{"location":"guide/testing_configuration/#full-conductor-configuration","text":"","title":"Full Conductor Configuration"},{"location":"guide/testing_configuration/#configconductorconductoroptions-object","text":"","title":"Config.conductor(conductorOptions) =&gt; object"},{"location":"guide/testing_configuration/#configconductorinstancesarray-conductoroptions-object","text":"There are two ways to construct a valid Conductor configuration from these Config helpers. Using the first way, you put all the config data into a single required object. Using the second \"shorthand\" style, you specify an array of Config.instance data, along with an optional object of extra options. The second way can be more convenient when you are just trying to set up a collection of instances with nothing extra options. Consumes an array of configured instances and produces an object which is a fully valid Conductor configuration. It can be passed into the Conductor constructor, which is covered in the next articles. This function is mostly useful in conjunction with manually instantiating a Conductor . Name conductorOptions Optional Type object Description conductorOptions.instances array Pass in an array of instance configuration objects generated by Config.instance to have them within the final configuration to be instantiated by the Conductor. Note: If using the two-argument \"shorthand\" style of Config.conductor , the first instancesArray argument will override this property. Description conductorOptions.bridges array Pass in an array of instance configuration objects generated by Config.bridges to have them within the final configuration to be instantiated by the Conductor Description conductorOptions.debugLog boolean Enables debug logging. The logger produces nice, colorful output of the internal workings of Holochain. Default { debugLog: false } Name instancesArray Type array Description When using the two-argument \"shorthand\" style of Config.conductor , you can specify the list of instances as the first argument, rather than folding it into the conductorOptions object.","title":"Config.conductor(instancesArray, [conductorOptions]) =&gt; object"},{"location":"guide/testing_configuration/#example_4","text":"const agentConfig = Config . agent ( 'alice' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig = Config . instance ( agentConfig , dnaConfig ) const conductorConfig = Config . conductor ({ instances : [ instanceConfig ] }) Or, equivalently, using the shorthand style: const conductorConfig = Config . conductor ([ instanceConfig ])","title":"Example"},{"location":"guide/testing_configuration/#example-with-conductoroptions","text":"const agentConfig = Config . agent ( 'alice' ) const dnaConfig = Config . dna ( 'path/to/your.dna.json' ) const instanceConfig = Config . instance ( agentConfig , dnaConfig ) const conductorConfig = Config . conductor ({ instances : [ instanceConfig ], debugLog : true }) Or, equivalently, using the shorthand style: const conductorConfig = Config . conductor ([ instanceConfig ], { debugLog : true })","title":"Example With conductorOptions"},{"location":"guide/testing_configuration/#multiple-instances-example-with-bridges","text":"const { Config } = require ( '@holochain/holochain-nodejs' ) // specify two agents... const aliceName = \"alice\" const bobName = \"bob\" const agentAlice = Config . agent ( aliceName ) const agentBob = Config . agent ( bobName ) // ...and one DNA... const dnaPath = \"path/to/happ.dna.json\" const dna = Config . dna ( dnaPath ) // ...then make instances out of them... const instanceAlice = Config . instance ( agentAlice , dna ) const instanceBob = Config . instance ( agentBob , dna ) const bridgeForward = Config . bridge ( 'bridge-forward' , instanceAlice , instanceBob ) const bridgeBackward = Config . bridge ( 'bridge-backward' , instanceAlice , instanceBob ) // ...and finally throw them all together const config = Config . conductor ({ instances : [ instanceAlice , instanceBob ], bridges : [ bridgeForward , bridgeBackward ] })","title":"Multiple Instances Example, with Bridges"},{"location":"guide/wasm/","text":"WASM \u00b6","title":"WASM"},{"location":"guide/wasm/#wasm","text":"","title":"WASM"},{"location":"guide/webassembly/","text":"WebAssembly \u00b6","title":"WebAssembly"},{"location":"guide/webassembly/#webassembly","text":"","title":"WebAssembly"},{"location":"guide/welcome/","text":"Welcome \u00b6 Status \u00b6 This documentation is a work in progress. Articles which have content in them appear normally in the chapter navigation. Articles which are incomplete appear with a (E) next to their name, indicating it is an empty unwritten article. Hi there! You've discovered the comprehensive Holochain guidebook. Holochain is an open source software library that provides a way for businesses, communities, and other groups to build and run applications which are hosted and validated by the \"users\" themselves. Doing so provides a superior level of agency and autonomy over heavy reliance on the so-called \"cloud\" and other third parties. These applications are known more widely as peer-to-peer, decentralized applications, or dApps. To distinguish between dApps built on Blockchains and those built on Holochain, we preemptively call the latter \"hApps\". A more detailed comparison between Blockchain dApps and Holochain hApps is available here . Holochain provides a cross-platform framework for the development and execution of these applications. By running these kinds of applications, \"users\" cease to merely \"use\". They become \"user-participants\" who are also responsible for hosting and validating the network's data. Applications can be developed utilizing any of the major operating systems, and run on virtually any device. The many benefits and opportunities associated with peer-to-peer dApps (e.g. offloaded server costs, elimination of single points of failure, and flexible governance structures) are made available, and often amplified through the Holochain hApp architecture, on desktops, laptops, and Android (arm64) devices. This book provides an in-depth explanation of Holochain's functions, from data validation to data propagation, so that you can get to work straightaway on developing applications that will serve your business, community, or otherwise.","title":"Welcome"},{"location":"guide/welcome/#welcome","text":"","title":"Welcome"},{"location":"guide/welcome/#status","text":"This documentation is a work in progress. Articles which have content in them appear normally in the chapter navigation. Articles which are incomplete appear with a (E) next to their name, indicating it is an empty unwritten article. Hi there! You've discovered the comprehensive Holochain guidebook. Holochain is an open source software library that provides a way for businesses, communities, and other groups to build and run applications which are hosted and validated by the \"users\" themselves. Doing so provides a superior level of agency and autonomy over heavy reliance on the so-called \"cloud\" and other third parties. These applications are known more widely as peer-to-peer, decentralized applications, or dApps. To distinguish between dApps built on Blockchains and those built on Holochain, we preemptively call the latter \"hApps\". A more detailed comparison between Blockchain dApps and Holochain hApps is available here . Holochain provides a cross-platform framework for the development and execution of these applications. By running these kinds of applications, \"users\" cease to merely \"use\". They become \"user-participants\" who are also responsible for hosting and validating the network's data. Applications can be developed utilizing any of the major operating systems, and run on virtually any device. The many benefits and opportunities associated with peer-to-peer dApps (e.g. offloaded server costs, elimination of single points of failure, and flexible governance structures) are made available, and often amplified through the Holochain hApp architecture, on desktops, laptops, and Android (arm64) devices. This book provides an in-depth explanation of Holochain's functions, from data validation to data propagation, so that you can get to work straightaway on developing applications that will serve your business, community, or otherwise.","title":"Status"},{"location":"guide/writing_development_kit/","text":"Writing a Development Kit \u00b6 The end goal of a Development Kit is to simplify the experience of writing Zomes that compile to WASM for Holochain apps. At the time of writing, there is currently one active Developer Kit being written, for the Rust language. While it is possible to look at the Rust language HDK as a reference, this article is a more general guide that outlines what it takes to build a Development Kit. If you are interested in supporting developers to write Zomes in an unsupported language, you will want to first of all check whether that language can be compiled to WebAssembly, as that is a requirement. Why Development Kits \u00b6 Development Kits are important because the WASM interface between Zomes and Holochain is constrained to singular 64 bit integers. The WASM spec allows for multiple function arguments and defines integers as neither signed nor unsigned, but Holochain only supports a single u64 input and output for all zome functions. WASM implements a single linear memory of bytes accessible by offset and length. Holochain sends and receives allocated bytes of memory to zomes by treating the 64 bit integer as two 32 bit integers (high bits as offset and low bits as length). If no bytes of memory are allocated (i.e. the 32 bit length is 0) the high bits map to an internal enum. This enum is contextual to the zome but typically represents errors: pub enum RibosomeErrorCode { Unspecified = 1 << 32 , ArgumentDeserializationFailed = 2 << 32 , OutOfMemory = 3 << 32 , ReceivedWrongActionResult = 4 << 32 , CallbackFailed = 5 << 32 , RecursiveCallForbidden = 6 << 32 , ResponseSerializationFailed = 7 << 32 , NotAnAllocation = 8 << 32 , ZeroSizedAllocation = 9 << 32 , UnknownEntryType = 10 << 32 , } Each development kit should abstract memory handling in some contextually idiomatic way. The Rust Development Kit WASM Solution \u00b6 The standard development kit implements a simple memory stack. The WasmAllocation struct represents a pair of offset/length u32 values. The WasmStack struct is a single \"top\" u32 value that tracks the current end of linear memory that can be written to (either allocation or deallocation). Use of these structs is optional inside zome WASM, Holochain core will always write/read according to the input/output position represented by the u64 arg/return values. Reads and write methods are provided for both primitive Rust UTF-8 strings and JsonString structs. Write new data to WasmStack as stack.write_string(String) and stack.write_json(Into<JsonString>) . If the allocation is successful a WasmAllocation will be returned else an AllocationError will result. Allocation to the stack can be handled manually as stack.allocate(allocation) and the next allocation can be built with stack.next_allocation(length) . Allocation on the stack will fail if the offset of the new allocation does not match the current stack top value. To read a previous write call let s = allocation.read_to_string() and standard let foo: Result<Foo, HolochainError> = JsonString::try_from(s) for JSON deserialization. To write a deallocation call stack.deallocate(allocation) . Deallocation does not clear out WASM memory, it simply moves the top of the stack back to the start of the passed allocation ready to be overwritten by the next allocation. Deallocation will fail if the allocation offset + length does not equal the current stack top. Holochain compatible encodings of allocations for the return value of zome functions can be generated with allocation.as_ribosome_encoding() . The development kit: Implements the simple stack/allocation structs and methods Manages a static stack for consistent writing Exposes convenience functions for the Holochain API to handle relevant allocation/deallocations Maps u64 values to/from encoded error values and u32 offset/length values for memory allocations For more details review the unit/integration test suites in hdk-rust and wasm_utils . Crafting the API \u00b6 Using its WASM interpreter, Holochain exposes its callable Zome API functions by making them available as \"imports\" in Zome WASM modules. Per the memory discussion above, each of the Zome API functions have the same explicit function signature, but different implicit function signatures. The native functions have each been given a prefix so that Development Kit wrappers can expose a regular function name. Here is a complete list: hc_debug hc_call hc_sign hc_verify_signature hc_commit_entry hc_update_entry hc_update_agent hc_remove_entry hc_get_entry hc_link_entries hc_query hc_send hc_start_bundle hc_close_bundle There is a special additional one called hc_init_globals which we will discuss further. The Development Kit should implement and export one function per each native function from the list. The function should be called the same as its native form, but without the prefix. E.g. hc_update_agent should be called update_agent or updateAgent . That function should internally call the native function and handle the additional complexity around that. In order to call these \"external\" functions, you will need to import them and provide their signature, but in a WASM import compatible way. In Rust, for example, this is simply: extern { fn hc_commit_entry ( encoded_allocation_of_input : RibosomeEncodingBits ) -> RibosomeEncodingBits ; } TODO: define or link to meaningful function signatures Working with WASM Memory \u00b6 The goal of the Development Kit is to expose a meaningful and easy to use version of the API functions, with meaningful arguments and return values. There is a bit of flexibility around how this is done, as coding languages differ. However, the internal process will be similar in nature. Here it is, generalized: 1. declare, or use a passed, single page 64 KiB memory stack 2. join whatever inputs are given into a single serializable structure 3. serialize the given data structure as an array of bytes 4. determine byte array length 5. ensure it is not oversized for the stack 6. allocate the memory 7. write the byte array to memory 8. create an allocation pointer for the memory a. use a 16 bit integer for the pointers offset b. use a 16 bit integer for the pointers length 9. join the pointers into a single 32 bit integer a. high bits are offset b. low bits are length 10. call the native function with that 32 bit integer and assign the result to another 32 bit integer a. e.g. encoded_alloc_of_result = hc_commit_entry(encoded_alloc_of_input) 11. deconstruct that 32 bit integer into two variables a. use a 16 bit integer for the pointers offset b. use a 16 bit integer for the pointers length 12. read string data from memory at the offset address 13. deallocate the memory 14. deserialize the string to JSON if JSON is expected That looks like a lot of steps, but most of this code can be shared for the various functions throughout the Development Kit, leaving implementations to be as little as 5 lines long. Basically, the process inverts at the point of the native function call. WASM Single Page Stack \u00b6 TODO App Globals \u00b6 When writing Zome code, it is common to need to reference aspects of the context it runs in, such as the active user/agent, or the DNA address of the app. Holochain exposes certain values through to the Zome, though it does so natively by way of the hc_init_globals function mentioned. Taking care to expose these values as constants will simplify the developer experience. This is done by calling hc_init_globals with an input value of 0. The result of calling the function is a 32 bit integer which represents the memory location of a serialized JSON object containing all the app global values. Fetch the result from memory, and deserialize the result back into an object. If appropriate, set those values as exports for the Development Kit. For example, in Rust, values become accessible in Zomes using hdk::DNA_NAME . It's recommended to use all capital letters for the export of the constants, but as they are returned as keys on an object from hc_init_globals they are in lower case. The object has the following values: - dna_name - dna_address - agent_id_str - agent_address - agent_initial_hash - agent_latest_hash - public_token See the API global variables page for details on what these are. Publish It and Get In Touch \u00b6 If you've made it through the process so far, good work. The community is an important part of the success of any project, and Holochain is no different. If you're really proud of your work, get in touch with the development team on the chat server , mention you're working on it, and request help if necessary. This book could be updated to include links to other HDKs. Whether you would like to, or you'd like the team to, the HDK could be published to the primary package manager in use for the language, to be used by developers around the world. For example, RubyGems for Ruby or npm for nodejs.","title":"Writing a Development Kit (HDK'"},{"location":"guide/writing_development_kit/#writing-a-development-kit","text":"The end goal of a Development Kit is to simplify the experience of writing Zomes that compile to WASM for Holochain apps. At the time of writing, there is currently one active Developer Kit being written, for the Rust language. While it is possible to look at the Rust language HDK as a reference, this article is a more general guide that outlines what it takes to build a Development Kit. If you are interested in supporting developers to write Zomes in an unsupported language, you will want to first of all check whether that language can be compiled to WebAssembly, as that is a requirement.","title":"Writing a Development Kit"},{"location":"guide/writing_development_kit/#why-development-kits","text":"Development Kits are important because the WASM interface between Zomes and Holochain is constrained to singular 64 bit integers. The WASM spec allows for multiple function arguments and defines integers as neither signed nor unsigned, but Holochain only supports a single u64 input and output for all zome functions. WASM implements a single linear memory of bytes accessible by offset and length. Holochain sends and receives allocated bytes of memory to zomes by treating the 64 bit integer as two 32 bit integers (high bits as offset and low bits as length). If no bytes of memory are allocated (i.e. the 32 bit length is 0) the high bits map to an internal enum. This enum is contextual to the zome but typically represents errors: pub enum RibosomeErrorCode { Unspecified = 1 << 32 , ArgumentDeserializationFailed = 2 << 32 , OutOfMemory = 3 << 32 , ReceivedWrongActionResult = 4 << 32 , CallbackFailed = 5 << 32 , RecursiveCallForbidden = 6 << 32 , ResponseSerializationFailed = 7 << 32 , NotAnAllocation = 8 << 32 , ZeroSizedAllocation = 9 << 32 , UnknownEntryType = 10 << 32 , } Each development kit should abstract memory handling in some contextually idiomatic way.","title":"Why Development Kits"},{"location":"guide/writing_development_kit/#the-rust-development-kit-wasm-solution","text":"The standard development kit implements a simple memory stack. The WasmAllocation struct represents a pair of offset/length u32 values. The WasmStack struct is a single \"top\" u32 value that tracks the current end of linear memory that can be written to (either allocation or deallocation). Use of these structs is optional inside zome WASM, Holochain core will always write/read according to the input/output position represented by the u64 arg/return values. Reads and write methods are provided for both primitive Rust UTF-8 strings and JsonString structs. Write new data to WasmStack as stack.write_string(String) and stack.write_json(Into<JsonString>) . If the allocation is successful a WasmAllocation will be returned else an AllocationError will result. Allocation to the stack can be handled manually as stack.allocate(allocation) and the next allocation can be built with stack.next_allocation(length) . Allocation on the stack will fail if the offset of the new allocation does not match the current stack top value. To read a previous write call let s = allocation.read_to_string() and standard let foo: Result<Foo, HolochainError> = JsonString::try_from(s) for JSON deserialization. To write a deallocation call stack.deallocate(allocation) . Deallocation does not clear out WASM memory, it simply moves the top of the stack back to the start of the passed allocation ready to be overwritten by the next allocation. Deallocation will fail if the allocation offset + length does not equal the current stack top. Holochain compatible encodings of allocations for the return value of zome functions can be generated with allocation.as_ribosome_encoding() . The development kit: Implements the simple stack/allocation structs and methods Manages a static stack for consistent writing Exposes convenience functions for the Holochain API to handle relevant allocation/deallocations Maps u64 values to/from encoded error values and u32 offset/length values for memory allocations For more details review the unit/integration test suites in hdk-rust and wasm_utils .","title":"The Rust Development Kit WASM Solution"},{"location":"guide/writing_development_kit/#crafting-the-api","text":"Using its WASM interpreter, Holochain exposes its callable Zome API functions by making them available as \"imports\" in Zome WASM modules. Per the memory discussion above, each of the Zome API functions have the same explicit function signature, but different implicit function signatures. The native functions have each been given a prefix so that Development Kit wrappers can expose a regular function name. Here is a complete list: hc_debug hc_call hc_sign hc_verify_signature hc_commit_entry hc_update_entry hc_update_agent hc_remove_entry hc_get_entry hc_link_entries hc_query hc_send hc_start_bundle hc_close_bundle There is a special additional one called hc_init_globals which we will discuss further. The Development Kit should implement and export one function per each native function from the list. The function should be called the same as its native form, but without the prefix. E.g. hc_update_agent should be called update_agent or updateAgent . That function should internally call the native function and handle the additional complexity around that. In order to call these \"external\" functions, you will need to import them and provide their signature, but in a WASM import compatible way. In Rust, for example, this is simply: extern { fn hc_commit_entry ( encoded_allocation_of_input : RibosomeEncodingBits ) -> RibosomeEncodingBits ; } TODO: define or link to meaningful function signatures","title":"Crafting the API"},{"location":"guide/writing_development_kit/#working-with-wasm-memory","text":"The goal of the Development Kit is to expose a meaningful and easy to use version of the API functions, with meaningful arguments and return values. There is a bit of flexibility around how this is done, as coding languages differ. However, the internal process will be similar in nature. Here it is, generalized: 1. declare, or use a passed, single page 64 KiB memory stack 2. join whatever inputs are given into a single serializable structure 3. serialize the given data structure as an array of bytes 4. determine byte array length 5. ensure it is not oversized for the stack 6. allocate the memory 7. write the byte array to memory 8. create an allocation pointer for the memory a. use a 16 bit integer for the pointers offset b. use a 16 bit integer for the pointers length 9. join the pointers into a single 32 bit integer a. high bits are offset b. low bits are length 10. call the native function with that 32 bit integer and assign the result to another 32 bit integer a. e.g. encoded_alloc_of_result = hc_commit_entry(encoded_alloc_of_input) 11. deconstruct that 32 bit integer into two variables a. use a 16 bit integer for the pointers offset b. use a 16 bit integer for the pointers length 12. read string data from memory at the offset address 13. deallocate the memory 14. deserialize the string to JSON if JSON is expected That looks like a lot of steps, but most of this code can be shared for the various functions throughout the Development Kit, leaving implementations to be as little as 5 lines long. Basically, the process inverts at the point of the native function call.","title":"Working with WASM Memory"},{"location":"guide/writing_development_kit/#wasm-single-page-stack","text":"TODO","title":"WASM Single Page Stack"},{"location":"guide/writing_development_kit/#app-globals","text":"When writing Zome code, it is common to need to reference aspects of the context it runs in, such as the active user/agent, or the DNA address of the app. Holochain exposes certain values through to the Zome, though it does so natively by way of the hc_init_globals function mentioned. Taking care to expose these values as constants will simplify the developer experience. This is done by calling hc_init_globals with an input value of 0. The result of calling the function is a 32 bit integer which represents the memory location of a serialized JSON object containing all the app global values. Fetch the result from memory, and deserialize the result back into an object. If appropriate, set those values as exports for the Development Kit. For example, in Rust, values become accessible in Zomes using hdk::DNA_NAME . It's recommended to use all capital letters for the export of the constants, but as they are returned as keys on an object from hc_init_globals they are in lower case. The object has the following values: - dna_name - dna_address - agent_id_str - agent_address - agent_initial_hash - agent_latest_hash - public_token See the API global variables page for details on what these are.","title":"App Globals"},{"location":"guide/writing_development_kit/#publish-it-and-get-in-touch","text":"If you've made it through the process so far, good work. The community is an important part of the success of any project, and Holochain is no different. If you're really proud of your work, get in touch with the development team on the chat server , mention you're working on it, and request help if necessary. This book could be updated to include links to other HDKs. Whether you would like to, or you'd like the team to, the HDK could be published to the primary package manager in use for the language, to be used by developers around the world. For example, RubyGems for Ruby or npm for nodejs.","title":"Publish It and Get In Touch"},{"location":"guide/zome/","text":"Zome \u00b6","title":"(E) Zome"},{"location":"guide/zome/#zome","text":"","title":"Zome"},{"location":"guide/links/get_links/","text":"Get Links \u00b6 Get Links allows the zome developer to query links from the DHT. The call accepts an options parameter to customize the query behavior. The parameters of the get_links are : base : address of the entry on which to query for links LinkType Link Match : a match enum which is either a regex or an exact match specifier on link_type Tag Link Match : a match enum which is either a regex or an exact match specifier on link's tag Options : a struct (see below) that you can use to specify different options to apply when executing the query. Options \u00b6 Timeout : The timeout variable on the options specifies how long the query process should wait befor a response before it timesout LinksStatusRequest : This is a variable in which you can specify 3 modes, All , Live , Delete . This allows you to query the links based on crud_status in which All will return everything will Live will only return live links and Delete as such. Headers : boolean value which if set to true indicates that the link headers should also be returned.``` Link Results \u00b6 A successful get_links call returns a set of links [(base,link_type,tag,target)] as well as meta data associated with it which is the headers and crud_status if requested in the Options .","title":"Get Links"},{"location":"guide/links/get_links/#get-links","text":"Get Links allows the zome developer to query links from the DHT. The call accepts an options parameter to customize the query behavior. The parameters of the get_links are : base : address of the entry on which to query for links LinkType Link Match : a match enum which is either a regex or an exact match specifier on link_type Tag Link Match : a match enum which is either a regex or an exact match specifier on link's tag Options : a struct (see below) that you can use to specify different options to apply when executing the query.","title":"Get Links"},{"location":"guide/links/get_links/#options","text":"Timeout : The timeout variable on the options specifies how long the query process should wait befor a response before it timesout LinksStatusRequest : This is a variable in which you can specify 3 modes, All , Live , Delete . This allows you to query the links based on crud_status in which All will return everything will Live will only return live links and Delete as such. Headers : boolean value which if set to true indicates that the link headers should also be returned.```","title":"Options"},{"location":"guide/links/get_links/#link-results","text":"A successful get_links call returns a set of links [(base,link_type,tag,target)] as well as meta data associated with it which is the headers and crud_status if requested in the Options .","title":"Link Results"},{"location":"guide/links/links_entries/","text":"Links Entries \u00b6 A link consists of 4 parts : Base - the address of the entry on which the links will be stored in the DHT. Link Type - a String that corresponds to a value that we would use to give the link a type Tag - an arbitrary string which can be added to link when it is created. This s accessible to the validation callback and can be used when retrieving links to filter by regex. Target - that address of the entry to be linked to from the base The process of linking in holochain is done through the link_entries function in the HDK, this will allow the zome developer to connect different kinds of data. All data that is linked is stored in our EAV storage (see holochain-persistance-api for more details on how this works). The EAV is the backbone of our storage mechanism when it comes to our linking process and addition and retrival of links is done using it. Validation \u00b6 System Validation They are two layers of validation when it comes to linking. One layer happens at the system level and this executes every time a link is added and uses validation rules which are defined by the system. The primary validation is to ensure that the base address exists. Since this corresponds to real data on the DHT we have to make sure that the hash we are giving it is correct and corresponds to something. Zome Validation Links also allow for the Zome Developer to define what rules to be checked before a link will be added to the DHT. The Zome Developer can define these rules by utilizing the LinkValidationData parameter in the link validation section of the zome. The Link Validation Data exposes the link data through an enum as well as lifecycle and package data.","title":"Links Entries"},{"location":"guide/links/links_entries/#links-entries","text":"A link consists of 4 parts : Base - the address of the entry on which the links will be stored in the DHT. Link Type - a String that corresponds to a value that we would use to give the link a type Tag - an arbitrary string which can be added to link when it is created. This s accessible to the validation callback and can be used when retrieving links to filter by regex. Target - that address of the entry to be linked to from the base The process of linking in holochain is done through the link_entries function in the HDK, this will allow the zome developer to connect different kinds of data. All data that is linked is stored in our EAV storage (see holochain-persistance-api for more details on how this works). The EAV is the backbone of our storage mechanism when it comes to our linking process and addition and retrival of links is done using it.","title":"Links Entries"},{"location":"guide/links/links_entries/#validation","text":"System Validation They are two layers of validation when it comes to linking. One layer happens at the system level and this executes every time a link is added and uses validation rules which are defined by the system. The primary validation is to ensure that the base address exists. Since this corresponds to real data on the DHT we have to make sure that the hash we are giving it is correct and corresponds to something. Zome Validation Links also allow for the Zome Developer to define what rules to be checked before a link will be added to the DHT. The Zome Developer can define these rules by utilizing the LinkValidationData parameter in the link validation section of the zome. The Link Validation Data exposes the link data through an enum as well as lifecycle and package data.","title":"Validation"},{"location":"guide/links/remove_link/","text":"Remove Link \u00b6 Recommended : read link_entries documentation before reading this documentation. Remove Link is the process of marking a link as deleted in the DHT. For this process to happen, the remove_link first has to get all like hashes that share the same base,tag,link type and target. The remove link process will then mark all of these links as deleted in the process. The reason for this is that links that share the same base, tag, link_type and target do not necesarrily produce the same link hash. In order for the delete to work all corresponding link hashes would have to be marked as deleted in the DHT Validation \u00b6 System Validation A system validation also takes place when a remove_link is executed. Before a remove_link is executed, we have to make sure that the link base address exists. This takes place in the system validation, after the system validation is complete, zome validation runs, see below. Zome Validation A Zome Validation always takes place on each remove_link that is executed after it passes the system validation. These are rules that can be defined by the zome. Thus, the Zome Developer can choose approve link deletion using their defined criteria.","title":"Remove Link"},{"location":"guide/links/remove_link/#remove-link","text":"Recommended : read link_entries documentation before reading this documentation. Remove Link is the process of marking a link as deleted in the DHT. For this process to happen, the remove_link first has to get all like hashes that share the same base,tag,link type and target. The remove link process will then mark all of these links as deleted in the process. The reason for this is that links that share the same base, tag, link_type and target do not necesarrily produce the same link hash. In order for the delete to work all corresponding link hashes would have to be marked as deleted in the DHT","title":"Remove Link"},{"location":"guide/links/remove_link/#validation","text":"System Validation A system validation also takes place when a remove_link is executed. Before a remove_link is executed, we have to make sure that the link base address exists. This takes place in the system validation, after the system validation is complete, zome validation runs, see below. Zome Validation A Zome Validation always takes place on each remove_link that is executed after it passes the system validation. These are rules that can be defined by the zome. Thus, the Zome Developer can choose approve link deletion using their defined criteria.","title":"Validation"},{"location":"guide/meta/version/","text":"Version \u00b6 You are able to call hdk::version() on the hdk and it is able to tell you which version of the hdk you are using. This information is based on the last tag that was applied to the holochain release and corresponds with our releases. The basis of this information comes from a git describe command.","title":"Version"},{"location":"guide/meta/version/#version","text":"You are able to call hdk::version() on the hdk and it is able to tell you which version of the hdk you are using. This information is based on the last tag that was applied to the holochain release and corresponds with our releases. The basis of this information comes from a git describe command.","title":"Version"},{"location":"guide/state/actions/","text":"@TODO @see https://github.com/holochain/holochain-rust/issues/176","title":"State actions"},{"location":"guide/state/actors/","text":"Internal actors \u00b6 Actors are discussed in two contexts: Each Holochain agent as an actor in a networking context Riker actors as an implemenation detail in the Holochain core lib This article is about the latter. Actor model \u00b6 The actor model is a relatively safe approach to co-ordinating concurrency. At a high level: An actor is the \"primitive\", like objects are the primitive of the OO paradigm Actors are stateful but this state is never exposed to the rest of the system Actors manage their internal state Actors maintain a message queue or \"inbox\" Messages can be received concurrently but must be processed sequentially in FIFO order The messages have a preset format Actors update their internal state in response to messages Actors can send messages to each other Messages are always processed at most once Actors can \"supervise\" each other to create a fault tolerent system A supervisor can restart or stop a failed actor, or escalate the decision to another supervisor The guarantees provided by the message queue allow actors to use stateful logic that would not be safe otherwise in a concurrent context. For example, we can implement logic that reads/writes to the file system without locks or other co-ordination. Then put an actor in front of this logic and only interact with the file system through the relevant actor. Riker \u00b6 Riker is an actor library for Rust. The actor implementation in Riker has a few key concepts: protocol: a set of valid messages that can be sent (e.g. an enum) actor system: manages and co-ordinates all actors actor: anything implementing the Actor trait to create new actor instances and handle receiving messages actor instance: an instance of the actor struct that has internal state and is tracked by the actor system actor ref(erence): an ActorRef that can tell messages to the actor instance it references via. the actor system The actor reference is a \"killer feature\" of Riker for us. known size at compile, safe as properties of structs/enums small size, almost free to clone safe to share across threads and copy, no Arc reference counting, no locks, etc. safe to drop (the actor system maintains a URI style lookup) known type, no onerous generic trait handling no onerous lifetimes","title":"State actors"},{"location":"guide/state/actors/#internal-actors","text":"Actors are discussed in two contexts: Each Holochain agent as an actor in a networking context Riker actors as an implemenation detail in the Holochain core lib This article is about the latter.","title":"Internal actors"},{"location":"guide/state/actors/#actor-model","text":"The actor model is a relatively safe approach to co-ordinating concurrency. At a high level: An actor is the \"primitive\", like objects are the primitive of the OO paradigm Actors are stateful but this state is never exposed to the rest of the system Actors manage their internal state Actors maintain a message queue or \"inbox\" Messages can be received concurrently but must be processed sequentially in FIFO order The messages have a preset format Actors update their internal state in response to messages Actors can send messages to each other Messages are always processed at most once Actors can \"supervise\" each other to create a fault tolerent system A supervisor can restart or stop a failed actor, or escalate the decision to another supervisor The guarantees provided by the message queue allow actors to use stateful logic that would not be safe otherwise in a concurrent context. For example, we can implement logic that reads/writes to the file system without locks or other co-ordination. Then put an actor in front of this logic and only interact with the file system through the relevant actor.","title":"Actor model"},{"location":"guide/state/actors/#riker","text":"Riker is an actor library for Rust. The actor implementation in Riker has a few key concepts: protocol: a set of valid messages that can be sent (e.g. an enum) actor system: manages and co-ordinates all actors actor: anything implementing the Actor trait to create new actor instances and handle receiving messages actor instance: an instance of the actor struct that has internal state and is tracked by the actor system actor ref(erence): an ActorRef that can tell messages to the actor instance it references via. the actor system The actor reference is a \"killer feature\" of Riker for us. known size at compile, safe as properties of structs/enums small size, almost free to clone safe to share across threads and copy, no Arc reference counting, no locks, etc. safe to drop (the actor system maintains a URI style lookup) known type, no onerous generic trait handling no onerous lifetimes","title":"Riker"},{"location":"guide/zome/adding_a_zome/","text":"Adding a Zome \u00b6 After creating a new project , in the resulting folder there is an empty folder called 'zomes'. The name of this folder is important and should not be changed. It will serve as the root folder for the one or more Zomes in a project. Every Zome should have its own folder within the 'zomes' root folder, and the name of those folders is also important, it should be the name of the Zome. It shouldn't have any spaces, and it should be a valid folder name. While you could go about creating a new Zome manually through your file system, it will be far faster to use the Holochain command line tools to generate one, with all the basic files you need included. To do this, navigate in a command line to the root directory of your hApp project. In the command line, run hc generate zomes/your_zome_name hc specifies that you wish to use the Holochain command line tools. generate specifies to use the command for initializing a new Zome. zomes/your_zome_name is an argument you supply as the path to, and the name of the Zome to generate. The output should be as follows cargo init --lib --vcs none Created library package Generated new rust Zome at \"zomes/your_zome_name\" Note that in the case of a Rust Zome, which is the only language for a Zome we can generate at the moment, it will rely internally on Rust related commands ( cargo init ), meaning that Rust (and its package manager, cargo) must already ALSO be installed for this command to work successfully. This has created a new folder ( zomes/your_zome_name ) in which you have the beginnings of a Zome. What's in a Zome? \u00b6 A Rust based Zome folder looks something like this: - code - src - lib.rs - .hcbuild - Cargo.toml - zome.json code is a folder that should always exist in a Zome, and should contain either pre-compiled WASM, or the source code and instructions to generate WASM. Everything within code is contextual to the language the Zome is written in, in the case above, a Rust \"crate\". Files within code will be explained in detail below. zome.json is the top level configuration of your Zome. Rust crate Zomes \u00b6 As mentioned above, the files within code are contextual to the language the Zome is written in, and in this case, that's Rust. As developers tend to do, Rust developers gave their own unique name to Rust projects: \"crates\". There are two types of Rust crates: library and binary . Since Zome code is getting compiled to WebAssembly, not standard binary executables, Zome crates use the library style, which is why we see under code/src a lib.rs file. The most minimalistic library crate would look like this: src lib.rs Cargo.toml Notice that the Zome we generated has one extra file, .hcbuild . This is the only Holochain specific file in the code folder. The rest is standard Rust. The .hcbuild file is discussed in another chapter. In general, the generated files have been modified from their defaults to offer basic boilerplate needed to get started writing Zome code. src/lib.rs is the default entry point to the code of a library crate. It can be the one and only Rust file of a Zome, or it can use standard Rust imports from other Rust files and their exports, taking full advantage of the Rust module system natively. Cargo.toml is Rust's equivalent to nodejs' package.json or Ruby's Gemfile : a configuration and dependency specification at the same time. Note that with the Cargo dependency system, Zome developers can take advantage of pre-existing Rust crates in their code, with one condition: that those dependencies are compatible when compiling to WebAssembly. This will be gone into in more detail elsewhere.","title":"Adding a Zome"},{"location":"guide/zome/adding_a_zome/#adding-a-zome","text":"After creating a new project , in the resulting folder there is an empty folder called 'zomes'. The name of this folder is important and should not be changed. It will serve as the root folder for the one or more Zomes in a project. Every Zome should have its own folder within the 'zomes' root folder, and the name of those folders is also important, it should be the name of the Zome. It shouldn't have any spaces, and it should be a valid folder name. While you could go about creating a new Zome manually through your file system, it will be far faster to use the Holochain command line tools to generate one, with all the basic files you need included. To do this, navigate in a command line to the root directory of your hApp project. In the command line, run hc generate zomes/your_zome_name hc specifies that you wish to use the Holochain command line tools. generate specifies to use the command for initializing a new Zome. zomes/your_zome_name is an argument you supply as the path to, and the name of the Zome to generate. The output should be as follows cargo init --lib --vcs none Created library package Generated new rust Zome at \"zomes/your_zome_name\" Note that in the case of a Rust Zome, which is the only language for a Zome we can generate at the moment, it will rely internally on Rust related commands ( cargo init ), meaning that Rust (and its package manager, cargo) must already ALSO be installed for this command to work successfully. This has created a new folder ( zomes/your_zome_name ) in which you have the beginnings of a Zome.","title":"Adding a Zome"},{"location":"guide/zome/adding_a_zome/#whats-in-a-zome","text":"A Rust based Zome folder looks something like this: - code - src - lib.rs - .hcbuild - Cargo.toml - zome.json code is a folder that should always exist in a Zome, and should contain either pre-compiled WASM, or the source code and instructions to generate WASM. Everything within code is contextual to the language the Zome is written in, in the case above, a Rust \"crate\". Files within code will be explained in detail below. zome.json is the top level configuration of your Zome.","title":"What's in a Zome?"},{"location":"guide/zome/adding_a_zome/#rust-crate-zomes","text":"As mentioned above, the files within code are contextual to the language the Zome is written in, and in this case, that's Rust. As developers tend to do, Rust developers gave their own unique name to Rust projects: \"crates\". There are two types of Rust crates: library and binary . Since Zome code is getting compiled to WebAssembly, not standard binary executables, Zome crates use the library style, which is why we see under code/src a lib.rs file. The most minimalistic library crate would look like this: src lib.rs Cargo.toml Notice that the Zome we generated has one extra file, .hcbuild . This is the only Holochain specific file in the code folder. The rest is standard Rust. The .hcbuild file is discussed in another chapter. In general, the generated files have been modified from their defaults to offer basic boilerplate needed to get started writing Zome code. src/lib.rs is the default entry point to the code of a library crate. It can be the one and only Rust file of a Zome, or it can use standard Rust imports from other Rust files and their exports, taking full advantage of the Rust module system natively. Cargo.toml is Rust's equivalent to nodejs' package.json or Ruby's Gemfile : a configuration and dependency specification at the same time. Note that with the Cargo dependency system, Zome developers can take advantage of pre-existing Rust crates in their code, with one condition: that those dependencies are compatible when compiling to WebAssembly. This will be gone into in more detail elsewhere.","title":"Rust crate Zomes"},{"location":"guide/zome/api_functions/","text":"Zome API Functions \u00b6 Overview \u00b6 A Zome API Function is any Holochain core functionality that is exposed as a callable function within Zome code. Compare this to a Zome Callback Function, which is implemented by the Zome code and called by Holochain. So, Zome functions (functions in the Zome code) are called by Holochain, which can optionally call Zome API Functions, and then finally return a value back to Holochain. Holochain \"blocks\" (meaning it pauses further execution in processor threads) -> calls Zome function -> executes WASM logic compiled from Zome language -> Zome logic calls zome API function -> Holochain natively executes Zome API function -> Holochain returns value to Zome function -> Zome function returns some value -> Holochain receives final value of Zome function Each Zome API Function has a canonical name used internally by Holochain. Zome code can be written in any language that compiles to WASM. This means the canonical function name and the function name in the Zome language might be different. The Zome language will closely mirror the canonical names, but naming conventions such as capitalisation of the zome language are also respected. For example, the canonical verify_signature might become verifySignature in AssemblyScript. When a Zome API function is called from within Zome code a corresponding Rust function is called. The Rust function is passed the current Zome runtime and the arguments that the zome API function was called with. The Rust function connects Zome logic to Holochain core functionality and often has side effects. The return value of the Rust function is passed back to the Zome code as the return of the Zome API function. Property \u00b6 Canonical name: property Not Yet Available. Returns an application property, which are defined by the developer in the DNA. It returns values from the DNA file that you set as properties of your application (e.g. Name, Language, Description, Author, etc.). Entry Address \u00b6 Canonical name: entry_address Returns the address that a given entry will hash into. Often used for reconstructing an address for a \"base\" when calling get_links . View it in the Rust HDK Debug \u00b6 Canonical name: debug Debug sends the passed arguments to the log that was given to the Holochain instance and returns None . View it in the Rust HDK Call \u00b6 Canonical name: call Enables making function calls to an exposed function from another app instance via bridging, or simply another Zome within the same instance. View it in the Rust HDK Sign \u00b6 Canonical name: sign Enables the signing of some piece of data, with the private keys associated with the acting agent. View it in the Rust HDK Encrypt \u00b6 Canonical name: encrypt Enables the encryption of some piece of data, with the private keys associated with the acting agent. View it in the Rust HDK Decrypt \u00b6 Canonical name: decrypt Enables the decryption of some piece of data, with the private keys associated with the acting agent. View it in the Rust HDK Verify Signature \u00b6 Canonical name: verify_signature Not yet available. A \"signature\" is a piece of data which claims to be signed by the holder of a private key associated with a public key. This function allows that claim to be verified, when given a \"signature\" and a public key. Commit Entry \u00b6 Canonical name: commit_entry Attempts to commit an entry to your local source chain. The entry will have to pass the defined validation rules for that entry type. If the entry type is defined as public, it will also publish the entry to the DHT. Returns either an address of the committed entry as a string, or an error. View it in the Rust HDK Update Entry \u00b6 Canonical name: update_entry Commit an entry to your local source chain that \"updates\" a previous entry, meaning when getting the previous entry, the updated entry will be returned. update_entry sets the previous entry's status metadata to Modified and adds the updated entry's address in the previous entry's metadata. The updated entry will hold the previous entry's address in its header, which will be used by validation routes. View it in the Rust HDK Update Agent \u00b6 Canonical name: update_agent Not yet available. Remove Entry \u00b6 Canonical name: remove_entry Enables an entry, referred to by its address, to be marked in the chain as 'deleted'. This is done by adding a new entry which indicates the deleted status of the old one. This will changes which types of results that entry would then show up in, according to its new 'deleted' status. It can still be retrieved, but only if specifically asked for. This function also returns the Hash of the deletion entry on completion View it in the Rust HDK Get Entry \u00b6 Canonical name: get_entry Given an entry hash, returns the entry from a chain or DHT if that entry exists. Entry lookup is done in the following order: - The local source chain - The local hash table - The distributed hash table Caller can request additional metadata on the entry such as type or sources (hashes of the agents that committed the entry). View get_entry in the Rust HDK View get_entry_initial in the Rust HDK View get_entry_history in the Rust HDK View get_entry_result in the Rust HDK Get Links \u00b6 Canonical name: get_links Consumes three values, the first of which is the address of an entry, base, the remaining two are Optional types for the link_type and tag . Passing Some(\"string\") will return only links that match the type/tag exactly. Passing None for either of those params will return all links regardless of the type/tag. Returns a list of addresses of other entries which matched as being linked by the given link type. Links are created in the first place using the Zome API function link_entries . Once you have the addresses, there is a good likelihood that you will wish to call get_entry for each of them. View get_links in the Rust HDK View get_links_and_load in the Rust HDK View get_links_result in the Rust HDK View get_links_with_options in the Rust HDK Link Entries \u00b6 Canonical name: link_entries Consumes four values, two of which are the addresses of entries, and two of which are strings that determine which link_type to use and a tag string that should be added to the link. The link_type must exactly match a type defined in an entry! macro. The tag can be any arbitrary string. Later, lists of entries can be looked up by using get_links and optionally filtered based on their type or tag. Entries can only be looked up in the direction from the base , which is the first argument, to the target , which is the second. This function returns a hash for the LinkAdd entry on completion. View it in the Rust HDK Query \u00b6 Canonical name: query Returns a list of addresses of entries from your local source chain, that match a given entry type name, or a vector of names. You can optionally limit the number of results, and you can use \"glob\" patterns such as \"prefix/*\" to specify the entry type names desired. View it in the Rust HDK Send \u00b6 Canonical name: send Sends a node-to-node message to the given agent. This works in conjunction with the receive callback, which is where the response behaviour to receiving a message should be defined. This function returns the result from the receive callback on the other side. View it in the Rust HDK Grant Capability \u00b6 Canonical name: commit_capability_grant Creates a capability grant on the local chain for allowing access to zome functions. View it in the Rust HDK Emit Signal \u00b6 Canonical name: emit_signal Emits a signal that a can be subscribed to by various clients. View it in the Rust HDK Read more about Signals Entry Type Properties \u00b6 Canonical name: entry_type_properties Retrieve the properties that were specified when a given entry was defined. View it in the Rust HDK Start Bundle \u00b6 Canonical name: start_bundle Not yet available. Close Bundle \u00b6 Canonical name: close_bundle Not yet available.","title":"List of API Functions"},{"location":"guide/zome/api_functions/#zome-api-functions","text":"","title":"Zome API Functions"},{"location":"guide/zome/api_functions/#overview","text":"A Zome API Function is any Holochain core functionality that is exposed as a callable function within Zome code. Compare this to a Zome Callback Function, which is implemented by the Zome code and called by Holochain. So, Zome functions (functions in the Zome code) are called by Holochain, which can optionally call Zome API Functions, and then finally return a value back to Holochain. Holochain \"blocks\" (meaning it pauses further execution in processor threads) -> calls Zome function -> executes WASM logic compiled from Zome language -> Zome logic calls zome API function -> Holochain natively executes Zome API function -> Holochain returns value to Zome function -> Zome function returns some value -> Holochain receives final value of Zome function Each Zome API Function has a canonical name used internally by Holochain. Zome code can be written in any language that compiles to WASM. This means the canonical function name and the function name in the Zome language might be different. The Zome language will closely mirror the canonical names, but naming conventions such as capitalisation of the zome language are also respected. For example, the canonical verify_signature might become verifySignature in AssemblyScript. When a Zome API function is called from within Zome code a corresponding Rust function is called. The Rust function is passed the current Zome runtime and the arguments that the zome API function was called with. The Rust function connects Zome logic to Holochain core functionality and often has side effects. The return value of the Rust function is passed back to the Zome code as the return of the Zome API function.","title":"Overview"},{"location":"guide/zome/api_functions/#property","text":"Canonical name: property Not Yet Available. Returns an application property, which are defined by the developer in the DNA. It returns values from the DNA file that you set as properties of your application (e.g. Name, Language, Description, Author, etc.).","title":"Property"},{"location":"guide/zome/api_functions/#entry-address","text":"Canonical name: entry_address Returns the address that a given entry will hash into. Often used for reconstructing an address for a \"base\" when calling get_links . View it in the Rust HDK","title":"Entry Address"},{"location":"guide/zome/api_functions/#debug","text":"Canonical name: debug Debug sends the passed arguments to the log that was given to the Holochain instance and returns None . View it in the Rust HDK","title":"Debug"},{"location":"guide/zome/api_functions/#call","text":"Canonical name: call Enables making function calls to an exposed function from another app instance via bridging, or simply another Zome within the same instance. View it in the Rust HDK","title":"Call"},{"location":"guide/zome/api_functions/#sign","text":"Canonical name: sign Enables the signing of some piece of data, with the private keys associated with the acting agent. View it in the Rust HDK","title":"Sign"},{"location":"guide/zome/api_functions/#encrypt","text":"Canonical name: encrypt Enables the encryption of some piece of data, with the private keys associated with the acting agent. View it in the Rust HDK","title":"Encrypt"},{"location":"guide/zome/api_functions/#decrypt","text":"Canonical name: decrypt Enables the decryption of some piece of data, with the private keys associated with the acting agent. View it in the Rust HDK","title":"Decrypt"},{"location":"guide/zome/api_functions/#verify-signature","text":"Canonical name: verify_signature Not yet available. A \"signature\" is a piece of data which claims to be signed by the holder of a private key associated with a public key. This function allows that claim to be verified, when given a \"signature\" and a public key.","title":"Verify Signature"},{"location":"guide/zome/api_functions/#commit-entry","text":"Canonical name: commit_entry Attempts to commit an entry to your local source chain. The entry will have to pass the defined validation rules for that entry type. If the entry type is defined as public, it will also publish the entry to the DHT. Returns either an address of the committed entry as a string, or an error. View it in the Rust HDK","title":"Commit Entry"},{"location":"guide/zome/api_functions/#update-entry","text":"Canonical name: update_entry Commit an entry to your local source chain that \"updates\" a previous entry, meaning when getting the previous entry, the updated entry will be returned. update_entry sets the previous entry's status metadata to Modified and adds the updated entry's address in the previous entry's metadata. The updated entry will hold the previous entry's address in its header, which will be used by validation routes. View it in the Rust HDK","title":"Update Entry"},{"location":"guide/zome/api_functions/#update-agent","text":"Canonical name: update_agent Not yet available.","title":"Update Agent"},{"location":"guide/zome/api_functions/#remove-entry","text":"Canonical name: remove_entry Enables an entry, referred to by its address, to be marked in the chain as 'deleted'. This is done by adding a new entry which indicates the deleted status of the old one. This will changes which types of results that entry would then show up in, according to its new 'deleted' status. It can still be retrieved, but only if specifically asked for. This function also returns the Hash of the deletion entry on completion View it in the Rust HDK","title":"Remove Entry"},{"location":"guide/zome/api_functions/#get-entry","text":"Canonical name: get_entry Given an entry hash, returns the entry from a chain or DHT if that entry exists. Entry lookup is done in the following order: - The local source chain - The local hash table - The distributed hash table Caller can request additional metadata on the entry such as type or sources (hashes of the agents that committed the entry). View get_entry in the Rust HDK View get_entry_initial in the Rust HDK View get_entry_history in the Rust HDK View get_entry_result in the Rust HDK","title":"Get Entry"},{"location":"guide/zome/api_functions/#get-links","text":"Canonical name: get_links Consumes three values, the first of which is the address of an entry, base, the remaining two are Optional types for the link_type and tag . Passing Some(\"string\") will return only links that match the type/tag exactly. Passing None for either of those params will return all links regardless of the type/tag. Returns a list of addresses of other entries which matched as being linked by the given link type. Links are created in the first place using the Zome API function link_entries . Once you have the addresses, there is a good likelihood that you will wish to call get_entry for each of them. View get_links in the Rust HDK View get_links_and_load in the Rust HDK View get_links_result in the Rust HDK View get_links_with_options in the Rust HDK","title":"Get Links"},{"location":"guide/zome/api_functions/#link-entries","text":"Canonical name: link_entries Consumes four values, two of which are the addresses of entries, and two of which are strings that determine which link_type to use and a tag string that should be added to the link. The link_type must exactly match a type defined in an entry! macro. The tag can be any arbitrary string. Later, lists of entries can be looked up by using get_links and optionally filtered based on their type or tag. Entries can only be looked up in the direction from the base , which is the first argument, to the target , which is the second. This function returns a hash for the LinkAdd entry on completion. View it in the Rust HDK","title":"Link Entries"},{"location":"guide/zome/api_functions/#query","text":"Canonical name: query Returns a list of addresses of entries from your local source chain, that match a given entry type name, or a vector of names. You can optionally limit the number of results, and you can use \"glob\" patterns such as \"prefix/*\" to specify the entry type names desired. View it in the Rust HDK","title":"Query"},{"location":"guide/zome/api_functions/#send","text":"Canonical name: send Sends a node-to-node message to the given agent. This works in conjunction with the receive callback, which is where the response behaviour to receiving a message should be defined. This function returns the result from the receive callback on the other side. View it in the Rust HDK","title":"Send"},{"location":"guide/zome/api_functions/#grant-capability","text":"Canonical name: commit_capability_grant Creates a capability grant on the local chain for allowing access to zome functions. View it in the Rust HDK","title":"Grant Capability"},{"location":"guide/zome/api_functions/#emit-signal","text":"Canonical name: emit_signal Emits a signal that a can be subscribed to by various clients. View it in the Rust HDK Read more about Signals","title":"Emit Signal"},{"location":"guide/zome/api_functions/#entry-type-properties","text":"Canonical name: entry_type_properties Retrieve the properties that were specified when a given entry was defined. View it in the Rust HDK","title":"Entry Type Properties"},{"location":"guide/zome/api_functions/#start-bundle","text":"Canonical name: start_bundle Not yet available.","title":"Start Bundle"},{"location":"guide/zome/api_functions/#close-bundle","text":"Canonical name: close_bundle Not yet available.","title":"Close Bundle"},{"location":"guide/zome/assemblyscript/","text":"Writing in Assemblyscript \u00b6 As mentioned in writing in Rust Assemblyscript is a language based off of Typescript, which is designed to compile to WebAssembly. It is hoped that Assemblyscript will soon be mature enough, and have the necessary features, to be able to have an HDK for it. Work on an HDK for Assemblyscript commenced mid 2018, but hit roadblocks. Updates on this should appear in a number of places: - the Holochain medium publication - the hdk-assemblyscript repository","title":"Writing in Assemblyscript"},{"location":"guide/zome/assemblyscript/#writing-in-assemblyscript","text":"As mentioned in writing in Rust Assemblyscript is a language based off of Typescript, which is designed to compile to WebAssembly. It is hoped that Assemblyscript will soon be mature enough, and have the necessary features, to be able to have an HDK for it. Work on an HDK for Assemblyscript commenced mid 2018, but hit roadblocks. Updates on this should appear in a number of places: - the Holochain medium publication - the hdk-assemblyscript repository","title":"Writing in Assemblyscript"},{"location":"guide/zome/bundling/","text":"Bundling \u00b6","title":"(E) Bundling"},{"location":"guide/zome/bundling/#bundling","text":"","title":"Bundling"},{"location":"guide/zome/calling_other_zomes/","text":"Calling Other Zomes \u00b6","title":"(E) Calling Other Zomes"},{"location":"guide/zome/calling_other_zomes/#calling-other-zomes","text":"","title":"Calling Other Zomes"},{"location":"guide/zome/capabilities/","text":"Capabilities \u00b6 Overview \u00b6 Holochain uses a modified version of the capabilities security model. Holochain DNA instances will grant revokable, cryptographic capability tokens which are shared as access credentials. Appropriate access credentials must be used to access functions and private data. This enables us to use a single security pattern for: connecting end-user UIs, calls across zomes within a DNA, bridging calls between different DNAs, and providing selective users of a DNA the ability to query private entries on the local chain via send/receive. Each capability grant gets recorded as a private entry on the grantor\u2019s chain. The hash (i.e. address) of that entry is then serves as the capability token usable by the grantee when making zome function call, because the grantor simply verifies the existence of that grant in it's chain. Thus, all zome functions calls include a capability request object which contains: public key of the grantee and signature of the parameters being used to call the function, along with the capability token being used as the access credential. Using Capabilities \u00b6 Public Capabilities \u00b6 You can declare some functions as \"public\" using the special hc_public marker trait in your define_zome! call. Functions in that trait will be added to the public capability grant which gets auto-committed during init. Like this: define_zome! { ... traits: { hc_public [read_post, write_post] } ... } Grant Capabilities \u00b6 You can use the commit_capability_grant HDK function to create a custom capability grant. For example, imaging a blogging use-case where you want to grant friends the ability to call the create_post function in a blog zome. Assuming the function is_my_friend(addr) correctly examines the provenance in CAPABILITY_REQ global which always holds the capability request of the current zome call, then the following code is an example of how you might call hdk::commit_capability_grant : pub fn handle_request_post_grant () -> ZomeApiResult < Option < Address >> { let addr = CAPABILITY_REQ . provenance . source (); if is_my_friend ( addr . clone ()) { let mut functions = BTreeMap :: new (); functions . insert ( \"blog\" . to_string (), vec ! [ \"create_post\" . to_string ()]); Ok ( Some ( hdk :: commit_capability_grant ( \"can_post\" , CapabilityType :: Assigned , Some ( vec ! [ addr ]), functions , ) ? )) } else { Ok ( None ) } } Capabilities in Bridging \u00b6 TBD.","title":"Capabilities"},{"location":"guide/zome/capabilities/#capabilities","text":"","title":"Capabilities"},{"location":"guide/zome/capabilities/#overview","text":"Holochain uses a modified version of the capabilities security model. Holochain DNA instances will grant revokable, cryptographic capability tokens which are shared as access credentials. Appropriate access credentials must be used to access functions and private data. This enables us to use a single security pattern for: connecting end-user UIs, calls across zomes within a DNA, bridging calls between different DNAs, and providing selective users of a DNA the ability to query private entries on the local chain via send/receive. Each capability grant gets recorded as a private entry on the grantor\u2019s chain. The hash (i.e. address) of that entry is then serves as the capability token usable by the grantee when making zome function call, because the grantor simply verifies the existence of that grant in it's chain. Thus, all zome functions calls include a capability request object which contains: public key of the grantee and signature of the parameters being used to call the function, along with the capability token being used as the access credential.","title":"Overview"},{"location":"guide/zome/capabilities/#using-capabilities","text":"","title":"Using Capabilities"},{"location":"guide/zome/capabilities/#public-capabilities","text":"You can declare some functions as \"public\" using the special hc_public marker trait in your define_zome! call. Functions in that trait will be added to the public capability grant which gets auto-committed during init. Like this: define_zome! { ... traits: { hc_public [read_post, write_post] } ... }","title":"Public Capabilities"},{"location":"guide/zome/capabilities/#grant-capabilities","text":"You can use the commit_capability_grant HDK function to create a custom capability grant. For example, imaging a blogging use-case where you want to grant friends the ability to call the create_post function in a blog zome. Assuming the function is_my_friend(addr) correctly examines the provenance in CAPABILITY_REQ global which always holds the capability request of the current zome call, then the following code is an example of how you might call hdk::commit_capability_grant : pub fn handle_request_post_grant () -> ZomeApiResult < Option < Address >> { let addr = CAPABILITY_REQ . provenance . source (); if is_my_friend ( addr . clone ()) { let mut functions = BTreeMap :: new (); functions . insert ( \"blog\" . to_string (), vec ! [ \"create_post\" . to_string ()]); Ok ( Some ( hdk :: commit_capability_grant ( \"can_post\" , CapabilityType :: Assigned , Some ( vec ! [ addr ]), functions , ) ? )) } else { Ok ( None ) } }","title":"Grant Capabilities"},{"location":"guide/zome/capabilities/#capabilities-in-bridging","text":"TBD.","title":"Capabilities in Bridging"},{"location":"guide/zome/compiling_to_webassembly/","text":"Compiling to WebAssembly \u00b6","title":"Compiling to WebAssembly"},{"location":"guide/zome/compiling_to_webassembly/#compiling-to-webassembly","text":"","title":"Compiling to WebAssembly"},{"location":"guide/zome/complete_reference/","text":"Complete Zome API Reference \u00b6","title":"Complete Zome API Reference"},{"location":"guide/zome/complete_reference/#complete-zome-api-reference","text":"","title":"Complete Zome API Reference"},{"location":"guide/zome/crypto/","text":"Crypto Functions \u00b6 Holochain DNA instances are designed to function in the context of a Distributed Public Key Insfrastructure (DPKI) which: manages key creation and revocation manages an agency context that allows grouping and verifying that sets of DNA instances are controlled by the same agent. creates a context for identity verification Holochain assumes that there may be different DPKI implementations but provides a reference implementation we call DeepKey. We assume that the DPKI implementation is itself a Holochain application, and we provide access to a set of generic cryptographic functions. These functions also allow DNA authors to build ad-hoc cryptogrpahic protocols. For each Holochain DNA instance, the conductor maintains a Keystore, which holds \"secrets\" (seeds and keys) needed for cryptographic signing and encrypting. Each of the secrets in the Keystore is associated with a string which is a handle needed when using that secret for some cryptographic operation. Our cryptographic implementation is based on libsodium, and the seeds use their notions of context and index for key derivation paths. This implementation allows DNA developers to securely call cryptographic functions from wasm which will be executed in the conductor's secure memory space when actually doing the cryptographic processing. Here are the available functions: keystore_list() -> returns a list of all the secret identifiers in the keystore keystore_new_random(dst_id) -> creates a new random root seed identified by dst_id keystore_derive_seed(src_id,dst_id,context,index) -> derives a higherarchical deterministic key seed to be identifided by dst_id from the src_id . Uses context and index as part of the derivation path. keystore_derive_key(src_id,dst_id,key_type) -> derives a key (signing or encrypting) to be identified by dst_id from a previously created seed identified by src_id . This function returns the public key of the keypair. keystore_sign(src_id,payload) -> returns a signature of the payload as signed by the key identified by src_id keystore_get_public_key(src_id) -> returns a the public key of a key identified by src_id . Returns an error if you pass an identifier of a seed secret. sign(payload) -> signs the payload using the DNA's instance agent ID public key. This is a convenience function which is equivalent to calling keystore_sign(\"primary_keybundle:sign_key\",payload) sign_one_time(payload_list) -> signs the payloads with a randomly generated key-pair, returning the signatures and the public key of the key-pair after shredding the private-key. verify_signature(provenance, payload) -> verifies that the payload matches the provenance which is a public key/signature pair. encrypt(payload) -> encrypts a message with the agent private key. decrypt(payload) -> decrypts a message with the agent private key Not Yet Implemented: keystore_encrypt(src_id,payload)","title":"Crypto Functions"},{"location":"guide/zome/crypto/#crypto-functions","text":"Holochain DNA instances are designed to function in the context of a Distributed Public Key Insfrastructure (DPKI) which: manages key creation and revocation manages an agency context that allows grouping and verifying that sets of DNA instances are controlled by the same agent. creates a context for identity verification Holochain assumes that there may be different DPKI implementations but provides a reference implementation we call DeepKey. We assume that the DPKI implementation is itself a Holochain application, and we provide access to a set of generic cryptographic functions. These functions also allow DNA authors to build ad-hoc cryptogrpahic protocols. For each Holochain DNA instance, the conductor maintains a Keystore, which holds \"secrets\" (seeds and keys) needed for cryptographic signing and encrypting. Each of the secrets in the Keystore is associated with a string which is a handle needed when using that secret for some cryptographic operation. Our cryptographic implementation is based on libsodium, and the seeds use their notions of context and index for key derivation paths. This implementation allows DNA developers to securely call cryptographic functions from wasm which will be executed in the conductor's secure memory space when actually doing the cryptographic processing. Here are the available functions: keystore_list() -> returns a list of all the secret identifiers in the keystore keystore_new_random(dst_id) -> creates a new random root seed identified by dst_id keystore_derive_seed(src_id,dst_id,context,index) -> derives a higherarchical deterministic key seed to be identifided by dst_id from the src_id . Uses context and index as part of the derivation path. keystore_derive_key(src_id,dst_id,key_type) -> derives a key (signing or encrypting) to be identified by dst_id from a previously created seed identified by src_id . This function returns the public key of the keypair. keystore_sign(src_id,payload) -> returns a signature of the payload as signed by the key identified by src_id keystore_get_public_key(src_id) -> returns a the public key of a key identified by src_id . Returns an error if you pass an identifier of a seed secret. sign(payload) -> signs the payload using the DNA's instance agent ID public key. This is a convenience function which is equivalent to calling keystore_sign(\"primary_keybundle:sign_key\",payload) sign_one_time(payload_list) -> signs the payloads with a randomly generated key-pair, returning the signatures and the public key of the key-pair after shredding the private-key. verify_signature(provenance, payload) -> verifies that the payload matches the provenance which is a public key/signature pair. encrypt(payload) -> encrypts a message with the agent private key. decrypt(payload) -> decrypts a message with the agent private key Not Yet Implemented: keystore_encrypt(src_id,payload)","title":"Crypto Functions"},{"location":"guide/zome/define_zome/","text":"Intro to Zome Definition \u00b6 The adding a zome section explored the file structure of a Zome. Intro to HDK covered Holochain Development Kit libraries for languages that Zomes can be written in. Once a Zome has been generated, and the HDK imported, it is time to start adding definitions to it. There are multiple aspects to defining a Zome. They will each be covered in detail in the following articles. What are the characteristics of a Zome, that need defining? A Zome has - name : the name of the containing folder - description : defined in JSON in the zome.json file within a Zome folder - config : Not Implemented Yet - validating entry types: definition may vary based on the language - a init function: a callback that Holochain expects and requires, defined in the code itself - fn_declarations : a collection of custom functions declarations, - traits : sets of named function groups used for composability - code : the core application logic of a Zome, written in a language that compiles to WASM, which Holochain interprets through that compiled WASM To develop a Zome, you will have to become familiar with these different aspects, the most complex of which are the validating entry types, and the traits and function definition. Implementation details will differ depending on the language that you are developing a Zome in. Building in Rust: define_zome! \u00b6 As discussed in the intro to HDK article, by setting the HDK as a dependency in the Cargo.toml file, and then referencing it in src/lib.rs , Zome code in Rust gains access to a host of features. The first line in the following code snippet (from a src/lib.rs ) is important: #[macro_use] . This imports access to custom Rust macros defined by the HDK. What are Rust macros? Generally speaking, they are code that will actually generate other code, when compiled. They are shortcuts. Anywhere in Rust that you see an expression followed immediately (no space) by an exclamation mark (!) that is the use of a macro. In the case of Zome development, it was discovered that much code could be saved from being written, by encapsulating it in a macro. That is how define_zome! came about. It is a Rust macro imported from the HDK which must be used for every Zome (unless you read the source code for it yourself and write something that behaves the same way!) The following is technically the most minimalistic Zome that could be implemented. It does nothing, but still conforms to the expectations Holochain has for a Zome. #[macro_use] extern crate hdk ; define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] } entries represents the validating entry type definitions. Note that it is an array, because there can be many. What validating entry types are will be explained next . init represents the previously mentioned init callback that Holochain expects from every Zome. Skip here for details. functions is where the functions are defined. Skip here for details. These are the three required properties of define_zome! .","title":"Intro to Zome Definition"},{"location":"guide/zome/define_zome/#intro-to-zome-definition","text":"The adding a zome section explored the file structure of a Zome. Intro to HDK covered Holochain Development Kit libraries for languages that Zomes can be written in. Once a Zome has been generated, and the HDK imported, it is time to start adding definitions to it. There are multiple aspects to defining a Zome. They will each be covered in detail in the following articles. What are the characteristics of a Zome, that need defining? A Zome has - name : the name of the containing folder - description : defined in JSON in the zome.json file within a Zome folder - config : Not Implemented Yet - validating entry types: definition may vary based on the language - a init function: a callback that Holochain expects and requires, defined in the code itself - fn_declarations : a collection of custom functions declarations, - traits : sets of named function groups used for composability - code : the core application logic of a Zome, written in a language that compiles to WASM, which Holochain interprets through that compiled WASM To develop a Zome, you will have to become familiar with these different aspects, the most complex of which are the validating entry types, and the traits and function definition. Implementation details will differ depending on the language that you are developing a Zome in.","title":"Intro to Zome Definition"},{"location":"guide/zome/define_zome/#building-in-rust-define_zome","text":"As discussed in the intro to HDK article, by setting the HDK as a dependency in the Cargo.toml file, and then referencing it in src/lib.rs , Zome code in Rust gains access to a host of features. The first line in the following code snippet (from a src/lib.rs ) is important: #[macro_use] . This imports access to custom Rust macros defined by the HDK. What are Rust macros? Generally speaking, they are code that will actually generate other code, when compiled. They are shortcuts. Anywhere in Rust that you see an expression followed immediately (no space) by an exclamation mark (!) that is the use of a macro. In the case of Zome development, it was discovered that much code could be saved from being written, by encapsulating it in a macro. That is how define_zome! came about. It is a Rust macro imported from the HDK which must be used for every Zome (unless you read the source code for it yourself and write something that behaves the same way!) The following is technically the most minimalistic Zome that could be implemented. It does nothing, but still conforms to the expectations Holochain has for a Zome. #[macro_use] extern crate hdk ; define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] } entries represents the validating entry type definitions. Note that it is an array, because there can be many. What validating entry types are will be explained next . init represents the previously mentioned init callback that Holochain expects from every Zome. Skip here for details. functions is where the functions are defined. Skip here for details. These are the three required properties of define_zome! .","title":"Building in Rust: define_zome!"},{"location":"guide/zome/dna_variables/","text":"API DNA Variables \u00b6 Note: Full reference is available in language-specific API Reference documentation. For the Rust hdk , see here Name Purpose DNA_NAME Name of the Holochain DNA taken from the DNA. DNA_ADDRESS The address of the DNA AGENT_ID_STR The identity string used to initialize this Holochain AGENT_ADDRESS The address (constructed from the public key) of this agent. AGENT_INITIAL_HASH The hash of the first identity entry on the local chain. AGENT_LATEST_HASH The hash of the most recent identity entry that has been committed to the local chain. CAPABILITY_REQ The capability request that was used to run the zome call","title":"API DNA Variables"},{"location":"guide/zome/dna_variables/#api-dna-variables","text":"Note: Full reference is available in language-specific API Reference documentation. For the Rust hdk , see here Name Purpose DNA_NAME Name of the Holochain DNA taken from the DNA. DNA_ADDRESS The address of the DNA AGENT_ID_STR The identity string used to initialize this Holochain AGENT_ADDRESS The address (constructed from the public key) of this agent. AGENT_INITIAL_HASH The hash of the first identity entry on the local chain. AGENT_LATEST_HASH The hash of the most recent identity entry that has been committed to the local chain. CAPABILITY_REQ The capability request that was used to run the zome call","title":"API DNA Variables"},{"location":"guide/zome/emitting_signals/","text":"Emitting Signals \u00b6 Holochain provides a mechanism for clients to subscribe to various signals that are emitted by different parts the system, some of them low level, and other explicitly by zome developers. Low level signals are used by our testing frame work . High level signals produced by zomes can be used by UIs to receive \"Application\" level notifications of events. A signal consists of a name and a content represented as a JsonString . Best practice for creating type safe JsonString s is by having a struct derive from DefaultJson like so: #[derive(Debug, Serialize, Deserialize, DefaultJson)] struct SignalPayload { message : String } let message = \"Hello World\" . to_string (); hdk :: emit_signal ( \"message_received\" , SignalPayload { message }); This will send a signal with the following JSON representation: { signal_type: 'User', name: 'message_received', arguments: '{ \"message\" : \"Hello World\" } ', } User signals (those emitted by hdk::emit_signal ) are sent over all websocket interfaces that include the instance that is emitting a signal. hc-web-client enables UIs to easily listen for signals by registering a callback through onSignal : const { onSignal } = await this . webClientConnect ({ url }) onSignal (( msg ) => { console . log ( msg . signal ) // -> { signal_type: 'User', name: 'test-signal', arguments: '{\"message\":\"test message\"}' } }) Outlook \u00b6 The current implementation of signals is at MVP stage - there is everything needed to not force UIs to poll for new data, given that the DNA uses callbacks (mainly node-2-node messages and receive ) to emit signals as needed. Future additions will be: * Signal signature description in the DNA ADR 13 describes signals as statically defined properties of a DNA which would enable conductor level binding/connecting of signals with slotes (i.e. zome functions) similar to bridges but with looser coupling. * DHT level signals/hooks Local changes like authoring a new entry could already emit a signal and thus trigger a reload/re-render of the UI. But what if a remote agent authored a chat message in a channel I'm following? It would be very handy to have DNA hooks (i.e. callbacks) that get called when a DHT node has validated and starts holding a new entry or link, such that either a node-2-node message can be sent to a set of observers, or introduce the concept of network level signals that do exactly that automatically.","title":"Emitting Signals"},{"location":"guide/zome/emitting_signals/#emitting-signals","text":"Holochain provides a mechanism for clients to subscribe to various signals that are emitted by different parts the system, some of them low level, and other explicitly by zome developers. Low level signals are used by our testing frame work . High level signals produced by zomes can be used by UIs to receive \"Application\" level notifications of events. A signal consists of a name and a content represented as a JsonString . Best practice for creating type safe JsonString s is by having a struct derive from DefaultJson like so: #[derive(Debug, Serialize, Deserialize, DefaultJson)] struct SignalPayload { message : String } let message = \"Hello World\" . to_string (); hdk :: emit_signal ( \"message_received\" , SignalPayload { message }); This will send a signal with the following JSON representation: { signal_type: 'User', name: 'message_received', arguments: '{ \"message\" : \"Hello World\" } ', } User signals (those emitted by hdk::emit_signal ) are sent over all websocket interfaces that include the instance that is emitting a signal. hc-web-client enables UIs to easily listen for signals by registering a callback through onSignal : const { onSignal } = await this . webClientConnect ({ url }) onSignal (( msg ) => { console . log ( msg . signal ) // -> { signal_type: 'User', name: 'test-signal', arguments: '{\"message\":\"test message\"}' } })","title":"Emitting Signals"},{"location":"guide/zome/emitting_signals/#outlook","text":"The current implementation of signals is at MVP stage - there is everything needed to not force UIs to poll for new data, given that the DNA uses callbacks (mainly node-2-node messages and receive ) to emit signals as needed. Future additions will be: * Signal signature description in the DNA ADR 13 describes signals as statically defined properties of a DNA which would enable conductor level binding/connecting of signals with slotes (i.e. zome functions) similar to bridges but with looser coupling. * DHT level signals/hooks Local changes like authoring a new entry could already emit a signal and thus trigger a reload/re-render of the UI. But what if a remote agent authored a chat message in a channel I'm following? It would be very handy to have DNA hooks (i.e. callbacks) that get called when a DHT node has validated and starts holding a new entry or link, such that either a node-2-node message can be sent to a set of observers, or introduce the concept of network level signals that do exactly that automatically.","title":"Outlook"},{"location":"guide/zome/entry_type_definitions/","text":"App Entry Type Definitions \u00b6 An \"entry\" is a data element that an agent authors to their source-chain (stored on their local device), which is then propagated to peers. The entry is backed by a \"chain header\", which is the data element used for verification of the integrity of itself, as well as the entry. Entries are a fundamental, primitive type within Holochain. Entries are an abstraction, they can technically be persisted to a device in a variety of ways, using a variety of databases, which can be as simple as files in the file system. There are types of entries which cannot be written to the chain by users of an application. These are generally called system entries. They include DNA, and initial Agent entries, which are always the first two entries written to a chain. There are a special type of entries called App Entries. These are entries which are created through the active use of an application by a user. They must have an entry type which, rather than being system defined, is defined by the Zome developer. Defining App Entry Types \u00b6 Creating a Zome for a hApp will almost always involve defining app entry types for that Zome. This means looking closely at the data model. What types of data will the Zome be designed to handle? Is it dealing in \"users\", \"transactions\", \"friendships\", \"tasks\", or what? These will be the entry types needing definition in a Zome. Broadly speaking, when defining the entry type, the developer of a Zome is designing the behaviour and generic properties of data of that type. This includes these important aspects: - how that data is shared, or not, with peers - the schema for entries of the type - custom validation behaviour for entries of the type - types of relationships (links) that can exist between entry types An entry type is given a name that is used when an agent is attempting to write an entry to the chain. That's how Holochain knows what to do with the data for the entry that it has been given. An entry type should also be given a basic description so that other people reading it understand the use of the entry type. A third important property is sharing . The primary options for this at this time are 'Private' and 'Public'. Private means entries of this type will stay only the device of the author. Public means entries of this type will be gossiped to other peers sharing copies of the DNA. Public does NOT mean that it will be shared publicly on the internet. Examining a .dna.json file closely, nested within the JSON configuration for a Zome, for an entry type you might see something like the following: \"entry_types\" : [ { \"entry_type_name\" : \"post\" , \"properties\" : \"{\\\"description\\\": \\\"A blog post entry which has an author\\\"}\" , \"sharing\" : \"public\" , \"links_to\" : [] } ] This is a Zome that implements only a single entry type, post . These values are likely not to be modified within a JSON file, but within some code itself, where the entry type is defined. The validation rules for the entry type, will of course be defined within the code as well. Since this can be a complex topic, defining the validation logic has its' own article . Setting up the entry types for a Zome is an often logical starting point when creating a Zome. Building in Rust: Defining an Entry Type \u00b6 Recall that in define_zome! , there was an array called entries . The most minimalistic Zome could look like this: #[macro_use] extern crate hdk ; define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] traits : {} } entries is where we will populate the Zome with entry type definitions. It expects an array of ValidatingEntryType . So how can one be created? Easy: the entry! macro. It can be used to encapsulate everything needed to define an entry type. All of the following must be defined: name entry ! ( name : \"post\" , ... ) This should be a machine-readable name for the entry type. Spaces should not be used. What will the entry type be that will be given when new entries are being created? description entry ! ( ... description : \"A blog post entry which has an author\" , ... ) Historically this was a human-readable explanation of the meaning or role of this entry type. Now the description field can hold a stringified JSON object to hold various properties of this entry type (Possibly including a description but also UI display hints, indexing fields, example data etc. It is totally up to you). These properties can be accessed via hdk::entry_type_properties . sharing use hdk :: holochain_core_types :: dna :: entry_types :: Sharing ; entry ! ( ... sharing : Sharing :: Public , ... ) As mentioned above, sharing refers to whether entries of this type are private to their author, or whether they will be gossiped to other peers to hold copies of. The value must be referenced from an enum in the HDK . Holochain currently supports the first two values in the enum: Public, and Private. native_type extern crate serde ; extern crate serde_json ; #[macro_use] extern crate serde_derive ; #[macro_use] extern crate holochain_persistence_derive ; #[derive(Serialize, Deserialize, Debug, DefaultJson,Clone)] struct Post { content : String , date_created : String , } entry ! ( ... native_type : Post , ... ) Clearly, native_type is where things start to get interesting. It requires the introduction of quite a number of dependencies, first of all. Why is that? It is important to remember that the Rust code of a Zome is compiled into WASM before it can be executed by Holochain. This introduces a certain constraint. How is data passed between Holochain, and the WASM Zome code? Answer: it is stored in the WASM memory as stringified JSON data, and accessed by the WASM code and by Holochain, running the WASM interpreter. JSON was chosen as the interchange format because it is so universal, and almost all languages have serializers and parsers. Rust's is called serde . The three serde related dependencies all relate to the need to serialize to and from JSON within Zomes. Additionally, the HDK offers built-in conversion functions from JSON strings to Entry structs. This comes from the DefaultJson derive . Every struct used as a native_type reference should include all 4 derives, as in the example: #[derive(Serialize, Deserialize, Debug, DefaultJson)] Serialize and Deserialize come from serde_derive , and DefaultJson comes from holochain_persistence_derive . Then there is the struct itself. This is the real type definition, because it defines the schema. It is simply a list of property names, the 'keys', and the types of values expected, which should be set to one of the primitive types of the language. This will tell serde how to parse JSON string inputs into the type. Note that conversion from JSON strings into the struct type can easily fail, in particular if the proper keys are not present on the input. validation_package use hdk :: ValidationPackageDefinition ; entry ! ( ... validation_package : || { ValidationPackageDefinition :: Entry }, ... ) At the moment, what validation_package is will not be covered in great detail. In short, for a peer to perform validation of an entry from another peer, varying degrees of metadata from the original author of the entry might be needed. validation_package refers to the carrier for that extra metadata. Looking at the above code, there is a required import from the HDK needed for use in validation_package , and that's the enum ValidationPackageDefinition . The value of validation_package is a function that takes no arguments. It will be called as a callback by Holochain. The result should be a value from the ValidationPackageDefinition enum, whose values can be seen here . In the example, and as the most basic option, simply use Entry , which means no extra metadata beyond the entry itself is needed. Further reading is here . validation use hdk :: EntryValidationData ; entry ! ( ... validation : | _validation_data : ValidationData < Post >| { Ok (()) } ) validation is the last required property of entry! . Because it is such an important aspect, it has its' own in depth article . It is a callback that Holochain will call during different moments in the lifecycle of an entry, in order to confirm which action to take with the entry, depending on its' validity. It will be called with two arguments, the first representing the struct of the entry itself, and the second a struct holding extra metadata that can be used for validation, including, if it was requested, the validation_package . The callback should return a Rust Result type. This is seen in Ok(()) . The example above is the simplest possible validation function, since it doesn't perform any real logic. While this is ok in theory, great caution should be taken with the validation rules, and further reading is recommended. validation for a ValidatingEntryType should either return Ok(()) or an Err containing the string explaining why validation failed. The validity of an entry is therefore defined by the author of a Zome. First of all, data which doesn't conform to the schema defined by the native_type will fail, but validation allows for further rules to be defined. Note that not only the entry author will call this function to validate the entry during its' creation, but other peers will call this function to validate the entry when it is requested via the network that they hold a copy of it. This is at the heart of how Holochain functions as peer-to-peer data integrity layer. Further reading can be found here . Putting It All Together \u00b6 Taken all together, use of the entry! macro may look something like the following: ... entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , validation_package : || { ValidationPackageDefinition :: Entry }, validation : | validation_data : EntryValidationData < Post >| { Ok (()) } ) This can be embedded directly inside of the entries array of the define_zome! , like so: ... define_zome ! { entries : [ entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , validation_package : || { ValidationPackageDefinition :: Entry }, validation : | _validation_data : EntryValidationData < Post >| { Ok (()) } ) ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] capabilitites : {} } If there is only entry type, this can be fine, but if there are multiple, this can hurt readability of the source code. You can wrap the entry type definition in a function, and call it in define_zome! , like so: ... fn post_definition () -> ValidatingEntryType { entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : Validation < Post >| { Ok (()) } ) } define_zome ! { entries : [ post_definition () ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] capabilitites : {} } Use of this technique can help you write clean, modular code. If you want to look closely at a complete example of the use of `entry!` in a Zome, check out the [API reference](https://developer.holochain.org/api/0.0.26-alpha1/hdk/macro.entry.html), or the [\"app-spec\" example app](https://github.com/holochain/holochain-rust/blob/release-0.0.26-alpha1/app_spec/zomes/blog/code/src/post.rs). Summary \u00b6 This is still a pretty minimal Zome, since it doesn't have any functions yet, and the most basic init behaviour, so read on to learn about how to work with those aspects of define_zome! .","title":"App Entry Type Definitions"},{"location":"guide/zome/entry_type_definitions/#app-entry-type-definitions","text":"An \"entry\" is a data element that an agent authors to their source-chain (stored on their local device), which is then propagated to peers. The entry is backed by a \"chain header\", which is the data element used for verification of the integrity of itself, as well as the entry. Entries are a fundamental, primitive type within Holochain. Entries are an abstraction, they can technically be persisted to a device in a variety of ways, using a variety of databases, which can be as simple as files in the file system. There are types of entries which cannot be written to the chain by users of an application. These are generally called system entries. They include DNA, and initial Agent entries, which are always the first two entries written to a chain. There are a special type of entries called App Entries. These are entries which are created through the active use of an application by a user. They must have an entry type which, rather than being system defined, is defined by the Zome developer.","title":"App Entry Type Definitions"},{"location":"guide/zome/entry_type_definitions/#defining-app-entry-types","text":"Creating a Zome for a hApp will almost always involve defining app entry types for that Zome. This means looking closely at the data model. What types of data will the Zome be designed to handle? Is it dealing in \"users\", \"transactions\", \"friendships\", \"tasks\", or what? These will be the entry types needing definition in a Zome. Broadly speaking, when defining the entry type, the developer of a Zome is designing the behaviour and generic properties of data of that type. This includes these important aspects: - how that data is shared, or not, with peers - the schema for entries of the type - custom validation behaviour for entries of the type - types of relationships (links) that can exist between entry types An entry type is given a name that is used when an agent is attempting to write an entry to the chain. That's how Holochain knows what to do with the data for the entry that it has been given. An entry type should also be given a basic description so that other people reading it understand the use of the entry type. A third important property is sharing . The primary options for this at this time are 'Private' and 'Public'. Private means entries of this type will stay only the device of the author. Public means entries of this type will be gossiped to other peers sharing copies of the DNA. Public does NOT mean that it will be shared publicly on the internet. Examining a .dna.json file closely, nested within the JSON configuration for a Zome, for an entry type you might see something like the following: \"entry_types\" : [ { \"entry_type_name\" : \"post\" , \"properties\" : \"{\\\"description\\\": \\\"A blog post entry which has an author\\\"}\" , \"sharing\" : \"public\" , \"links_to\" : [] } ] This is a Zome that implements only a single entry type, post . These values are likely not to be modified within a JSON file, but within some code itself, where the entry type is defined. The validation rules for the entry type, will of course be defined within the code as well. Since this can be a complex topic, defining the validation logic has its' own article . Setting up the entry types for a Zome is an often logical starting point when creating a Zome.","title":"Defining App Entry Types"},{"location":"guide/zome/entry_type_definitions/#building-in-rust-defining-an-entry-type","text":"Recall that in define_zome! , there was an array called entries . The most minimalistic Zome could look like this: #[macro_use] extern crate hdk ; define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] traits : {} } entries is where we will populate the Zome with entry type definitions. It expects an array of ValidatingEntryType . So how can one be created? Easy: the entry! macro. It can be used to encapsulate everything needed to define an entry type. All of the following must be defined: name entry ! ( name : \"post\" , ... ) This should be a machine-readable name for the entry type. Spaces should not be used. What will the entry type be that will be given when new entries are being created? description entry ! ( ... description : \"A blog post entry which has an author\" , ... ) Historically this was a human-readable explanation of the meaning or role of this entry type. Now the description field can hold a stringified JSON object to hold various properties of this entry type (Possibly including a description but also UI display hints, indexing fields, example data etc. It is totally up to you). These properties can be accessed via hdk::entry_type_properties . sharing use hdk :: holochain_core_types :: dna :: entry_types :: Sharing ; entry ! ( ... sharing : Sharing :: Public , ... ) As mentioned above, sharing refers to whether entries of this type are private to their author, or whether they will be gossiped to other peers to hold copies of. The value must be referenced from an enum in the HDK . Holochain currently supports the first two values in the enum: Public, and Private. native_type extern crate serde ; extern crate serde_json ; #[macro_use] extern crate serde_derive ; #[macro_use] extern crate holochain_persistence_derive ; #[derive(Serialize, Deserialize, Debug, DefaultJson,Clone)] struct Post { content : String , date_created : String , } entry ! ( ... native_type : Post , ... ) Clearly, native_type is where things start to get interesting. It requires the introduction of quite a number of dependencies, first of all. Why is that? It is important to remember that the Rust code of a Zome is compiled into WASM before it can be executed by Holochain. This introduces a certain constraint. How is data passed between Holochain, and the WASM Zome code? Answer: it is stored in the WASM memory as stringified JSON data, and accessed by the WASM code and by Holochain, running the WASM interpreter. JSON was chosen as the interchange format because it is so universal, and almost all languages have serializers and parsers. Rust's is called serde . The three serde related dependencies all relate to the need to serialize to and from JSON within Zomes. Additionally, the HDK offers built-in conversion functions from JSON strings to Entry structs. This comes from the DefaultJson derive . Every struct used as a native_type reference should include all 4 derives, as in the example: #[derive(Serialize, Deserialize, Debug, DefaultJson)] Serialize and Deserialize come from serde_derive , and DefaultJson comes from holochain_persistence_derive . Then there is the struct itself. This is the real type definition, because it defines the schema. It is simply a list of property names, the 'keys', and the types of values expected, which should be set to one of the primitive types of the language. This will tell serde how to parse JSON string inputs into the type. Note that conversion from JSON strings into the struct type can easily fail, in particular if the proper keys are not present on the input. validation_package use hdk :: ValidationPackageDefinition ; entry ! ( ... validation_package : || { ValidationPackageDefinition :: Entry }, ... ) At the moment, what validation_package is will not be covered in great detail. In short, for a peer to perform validation of an entry from another peer, varying degrees of metadata from the original author of the entry might be needed. validation_package refers to the carrier for that extra metadata. Looking at the above code, there is a required import from the HDK needed for use in validation_package , and that's the enum ValidationPackageDefinition . The value of validation_package is a function that takes no arguments. It will be called as a callback by Holochain. The result should be a value from the ValidationPackageDefinition enum, whose values can be seen here . In the example, and as the most basic option, simply use Entry , which means no extra metadata beyond the entry itself is needed. Further reading is here . validation use hdk :: EntryValidationData ; entry ! ( ... validation : | _validation_data : ValidationData < Post >| { Ok (()) } ) validation is the last required property of entry! . Because it is such an important aspect, it has its' own in depth article . It is a callback that Holochain will call during different moments in the lifecycle of an entry, in order to confirm which action to take with the entry, depending on its' validity. It will be called with two arguments, the first representing the struct of the entry itself, and the second a struct holding extra metadata that can be used for validation, including, if it was requested, the validation_package . The callback should return a Rust Result type. This is seen in Ok(()) . The example above is the simplest possible validation function, since it doesn't perform any real logic. While this is ok in theory, great caution should be taken with the validation rules, and further reading is recommended. validation for a ValidatingEntryType should either return Ok(()) or an Err containing the string explaining why validation failed. The validity of an entry is therefore defined by the author of a Zome. First of all, data which doesn't conform to the schema defined by the native_type will fail, but validation allows for further rules to be defined. Note that not only the entry author will call this function to validate the entry during its' creation, but other peers will call this function to validate the entry when it is requested via the network that they hold a copy of it. This is at the heart of how Holochain functions as peer-to-peer data integrity layer. Further reading can be found here .","title":"Building in Rust: Defining an Entry Type"},{"location":"guide/zome/entry_type_definitions/#putting-it-all-together","text":"Taken all together, use of the entry! macro may look something like the following: ... entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , validation_package : || { ValidationPackageDefinition :: Entry }, validation : | validation_data : EntryValidationData < Post >| { Ok (()) } ) This can be embedded directly inside of the entries array of the define_zome! , like so: ... define_zome ! { entries : [ entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , validation_package : || { ValidationPackageDefinition :: Entry }, validation : | _validation_data : EntryValidationData < Post >| { Ok (()) } ) ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] capabilitites : {} } If there is only entry type, this can be fine, but if there are multiple, this can hurt readability of the source code. You can wrap the entry type definition in a function, and call it in define_zome! , like so: ... fn post_definition () -> ValidatingEntryType { entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , validation_package : || { hdk :: ValidationPackageDefinition :: Entry }, validation : | _validation_data : Validation < Post >| { Ok (()) } ) } define_zome ! { entries : [ post_definition () ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] capabilitites : {} } Use of this technique can help you write clean, modular code. If you want to look closely at a complete example of the use of `entry!` in a Zome, check out the [API reference](https://developer.holochain.org/api/0.0.26-alpha1/hdk/macro.entry.html), or the [\"app-spec\" example app](https://github.com/holochain/holochain-rust/blob/release-0.0.26-alpha1/app_spec/zomes/blog/code/src/post.rs).","title":"Putting It All Together"},{"location":"guide/zome/entry_type_definitions/#summary","text":"This is still a pretty minimal Zome, since it doesn't have any functions yet, and the most basic init behaviour, so read on to learn about how to work with those aspects of define_zome! .","title":"Summary"},{"location":"guide/zome/entry_validation/","text":"Entry Validation \u00b6","title":"(E) Entry Validation"},{"location":"guide/zome/entry_validation/#entry-validation","text":"","title":"Entry Validation"},{"location":"guide/zome/implementation/","text":"Zome implementation \u00b6 Zome API functions \u00b6 Each zome API function is implemented under nucleus::ribosome::api . There is a fair bit of boilerplate at the moment, sorry! To co-ordinate the execution of an API function across Rust and WASM we need to define a few related items. Within nucleus::ribosome::api : A variant in the ZomeApiFunction enum The same canonical string in both as_str and from_str A mapping to the API function under as_fn As a new module under nucleus::ribosome::api : A ribosome module implementing the invocation logic as invoke_* A struct to hold/serialize any input args if needed In ::action : An action if the zome API function has side effects Zome API function definition \u00b6 Simply add the name of the new zome API function to the end of the enum. Make sure to add the canonical names carefully. The Rust compiler will guide you through the rest if you miss something. DO add a doc comment summarising what the zome function does and sketching the function signature. DO extend the relevant unit tests. Do NOT add to the start or middle of the enum as that will renumber the other zome functions. Zome API function ribosome module \u00b6 Each zome API function should have its own module under nucleus::ribosome::* . Implement a public function as invoke_<canonical name> . The function must take two arguments, a &mut nucleus::ribosome::Runtime and a &wasmi::RuntimeArgs . This function will be called by the invocation dispatch (see above). Zome API function arguments \u00b6 The wasmi::RuntimeArgs passed to the Zome API function contains only a single u64 value. This is an encoded representation of a single page of memory supported by the memory manager. The 16 high bits are the memory offset and the 16 low bits are the memory length. See the wasm_utils crate for more implementation details. You don't have to work with the memory manager directly, simply pass the runtime and runtime args to nucleus::runtime_args_to_utf8 to get a utf-8 string from memory. You DO have to handle serialization round trips if you want to pass anything other than a single utf-8 string to a zome API function. The simplest way to do this is implement a struct that derives Serialize and Deserialize from serde, then use serde and .into_bytes() co-ordinate the round trip. For an example implementation of a struct with several fields see: nucleus::ribosome::commit::CommitArgs for the input args struct nucleus::ribosome::commit::tests::test_args_bytes serializing the struct as bytes nucleus::ribosome::commit::invoke_commit deserializing the struct from the runtime Zome API function action dispatch \u00b6 If the function has a side effect it must send an action to the state reduction layer. Actions are covered in more detail in the state chapter. In summary, if you want to send an action and wait for a return value: create an outer channel in the scope of your invoke function that will receive the return value call ::instance::dispatch_action_with_observer with: the runtime's channels the action the reducer will dispatch on an observer sensor, which is a closure that polls for the action result and sends to your outer channel block the outer channel until you receive the action result Zome API function return values \u00b6 The zome API function returns a value to wasm representing success or a wasm trap. The success value can only be a single u64 . Traps are a low level wasm concern and are unlikely to be directly useful to a zome API function implementation. See https://github.com/WebAssembly/design/blob/master/Semantics.md#traps To get complex values out of wasm we use the memory manager, much like the input argument serialization (see above). The util function nucleus::runtime_allocate_encode_str takes a string, allocates memory and returns the value that the zome API function must return. To return an error relevant to holochain, return Ok with an HcApiReturnCode error enum variant. For an example implementation returning a complex struct see: agent::state::ActionResponse::GetEntry containing an Entry struct nucleus::ribosome::get::invoke_get match the action result against the correct enum variant serialize the entry using serde return the result of runtime_allocate_encode_str if the action result variant does NOT match then return HcApiReturnCode::ErrorActionResult Zome API function agent action \u00b6 If the zome API function will cause side effects to the agent state then it must implement and dispatch an action. Actions are covered in more detail in the state chapter. In summary, if a new agent action (for example) is needed: extend the action::Action enum this sets the data type, the ActionWrapper provides a unique ID use the canonical name if that makes sense extend an ActionResult enum if the action has a return value implement a reducer for the new action","title":"Implementing Zome API functions"},{"location":"guide/zome/implementation/#zome-implementation","text":"","title":"Zome implementation"},{"location":"guide/zome/implementation/#zome-api-functions","text":"Each zome API function is implemented under nucleus::ribosome::api . There is a fair bit of boilerplate at the moment, sorry! To co-ordinate the execution of an API function across Rust and WASM we need to define a few related items. Within nucleus::ribosome::api : A variant in the ZomeApiFunction enum The same canonical string in both as_str and from_str A mapping to the API function under as_fn As a new module under nucleus::ribosome::api : A ribosome module implementing the invocation logic as invoke_* A struct to hold/serialize any input args if needed In ::action : An action if the zome API function has side effects","title":"Zome API functions"},{"location":"guide/zome/implementation/#zome-api-function-definition","text":"Simply add the name of the new zome API function to the end of the enum. Make sure to add the canonical names carefully. The Rust compiler will guide you through the rest if you miss something. DO add a doc comment summarising what the zome function does and sketching the function signature. DO extend the relevant unit tests. Do NOT add to the start or middle of the enum as that will renumber the other zome functions.","title":"Zome API function definition"},{"location":"guide/zome/implementation/#zome-api-function-ribosome-module","text":"Each zome API function should have its own module under nucleus::ribosome::* . Implement a public function as invoke_<canonical name> . The function must take two arguments, a &mut nucleus::ribosome::Runtime and a &wasmi::RuntimeArgs . This function will be called by the invocation dispatch (see above).","title":"Zome API function ribosome module"},{"location":"guide/zome/implementation/#zome-api-function-arguments","text":"The wasmi::RuntimeArgs passed to the Zome API function contains only a single u64 value. This is an encoded representation of a single page of memory supported by the memory manager. The 16 high bits are the memory offset and the 16 low bits are the memory length. See the wasm_utils crate for more implementation details. You don't have to work with the memory manager directly, simply pass the runtime and runtime args to nucleus::runtime_args_to_utf8 to get a utf-8 string from memory. You DO have to handle serialization round trips if you want to pass anything other than a single utf-8 string to a zome API function. The simplest way to do this is implement a struct that derives Serialize and Deserialize from serde, then use serde and .into_bytes() co-ordinate the round trip. For an example implementation of a struct with several fields see: nucleus::ribosome::commit::CommitArgs for the input args struct nucleus::ribosome::commit::tests::test_args_bytes serializing the struct as bytes nucleus::ribosome::commit::invoke_commit deserializing the struct from the runtime","title":"Zome API function arguments"},{"location":"guide/zome/implementation/#zome-api-function-action-dispatch","text":"If the function has a side effect it must send an action to the state reduction layer. Actions are covered in more detail in the state chapter. In summary, if you want to send an action and wait for a return value: create an outer channel in the scope of your invoke function that will receive the return value call ::instance::dispatch_action_with_observer with: the runtime's channels the action the reducer will dispatch on an observer sensor, which is a closure that polls for the action result and sends to your outer channel block the outer channel until you receive the action result","title":"Zome API function action dispatch"},{"location":"guide/zome/implementation/#zome-api-function-return-values","text":"The zome API function returns a value to wasm representing success or a wasm trap. The success value can only be a single u64 . Traps are a low level wasm concern and are unlikely to be directly useful to a zome API function implementation. See https://github.com/WebAssembly/design/blob/master/Semantics.md#traps To get complex values out of wasm we use the memory manager, much like the input argument serialization (see above). The util function nucleus::runtime_allocate_encode_str takes a string, allocates memory and returns the value that the zome API function must return. To return an error relevant to holochain, return Ok with an HcApiReturnCode error enum variant. For an example implementation returning a complex struct see: agent::state::ActionResponse::GetEntry containing an Entry struct nucleus::ribosome::get::invoke_get match the action result against the correct enum variant serialize the entry using serde return the result of runtime_allocate_encode_str if the action result variant does NOT match then return HcApiReturnCode::ErrorActionResult","title":"Zome API function return values"},{"location":"guide/zome/implementation/#zome-api-function-agent-action","text":"If the zome API function will cause side effects to the agent state then it must implement and dispatch an action. Actions are covered in more detail in the state chapter. In summary, if a new agent action (for example) is needed: extend the action::Action enum this sets the data type, the ActionWrapper provides a unique ID use the canonical name if that makes sense extend an ActionResult enum if the action has a return value implement a reducer for the new action","title":"Zome API function agent action"},{"location":"guide/zome/init/","text":"Init \u00b6 The Initialization Process \u00b6 Recall that every peer will be running instances of DNA on their device. This is how peers join the network for a given DNA. There are some actions that a developer may wish to initiate, upon a new peer joining a network. For this reason, a hook into this moment of the lifecycle is implemented by Holochain. It also provides an opportunity to reject the user from joining, if for some reason there is an issue with the way they are attempting to. This lifecycle stage is called init . It is a callback that Holochain expects every single Zome to implement, because it will call it during initialization. If it does not exist within the WASM code for a Zome it will cause an error and peers will not be able to launch an instance of the DNA. This function also has the opportunity to reject the success of the launch of the instance. If it succeeds, the expected return value is just an integer (in WASM) representing that, but if it fails, a string is expected to be passed, explaining why. When Holochain is attempting to launch an instance of a Zome, it will iterate through all the Zomes one by one, calling init within each. If each succeeds, success. If any one fails, the launch will fail, and the error string will be returned to the peer. Holochain will wait up to 30 seconds for a init response from the Zome, before it will throw a timeout error. Of course, this also indicates that init is a reserved function name and should not be used as the name of any other function that is publicly callable in the Zome. Building in Rust: init \u00b6 How is init used within the Rust HDK? Previously , the general structure of define_zome! has been covered. It includes a Rust function closure called init , which is passed zero arguments. This is the hook that Holochain is expecting. It expects a Rust Result as a return value, which is either Ok(()) or an Err , with the string explaining the error. In the following two examples, nothing interesting will happen in the init functions, they are simply to illustrate how to return success, and how to return failure. More complex capabilities will be possible during init in the future, yet for now, using the first simple example that succeeds is recommended. If init should succeed: define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] traits : {} } If init should fail: define_zome ! { entries : [] init : || { Err ( \"the error string\" . to_string ()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] traits : {} }","title":"Init"},{"location":"guide/zome/init/#init","text":"","title":"Init"},{"location":"guide/zome/init/#the-initialization-process","text":"Recall that every peer will be running instances of DNA on their device. This is how peers join the network for a given DNA. There are some actions that a developer may wish to initiate, upon a new peer joining a network. For this reason, a hook into this moment of the lifecycle is implemented by Holochain. It also provides an opportunity to reject the user from joining, if for some reason there is an issue with the way they are attempting to. This lifecycle stage is called init . It is a callback that Holochain expects every single Zome to implement, because it will call it during initialization. If it does not exist within the WASM code for a Zome it will cause an error and peers will not be able to launch an instance of the DNA. This function also has the opportunity to reject the success of the launch of the instance. If it succeeds, the expected return value is just an integer (in WASM) representing that, but if it fails, a string is expected to be passed, explaining why. When Holochain is attempting to launch an instance of a Zome, it will iterate through all the Zomes one by one, calling init within each. If each succeeds, success. If any one fails, the launch will fail, and the error string will be returned to the peer. Holochain will wait up to 30 seconds for a init response from the Zome, before it will throw a timeout error. Of course, this also indicates that init is a reserved function name and should not be used as the name of any other function that is publicly callable in the Zome.","title":"The Initialization Process"},{"location":"guide/zome/init/#building-in-rust-init","text":"How is init used within the Rust HDK? Previously , the general structure of define_zome! has been covered. It includes a Rust function closure called init , which is passed zero arguments. This is the hook that Holochain is expecting. It expects a Rust Result as a return value, which is either Ok(()) or an Err , with the string explaining the error. In the following two examples, nothing interesting will happen in the init functions, they are simply to illustrate how to return success, and how to return failure. More complex capabilities will be possible during init in the future, yet for now, using the first simple example that succeeds is recommended. If init should succeed: define_zome ! { entries : [] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] traits : {} } If init should fail: define_zome ! { entries : [] init : || { Err ( \"the error string\" . to_string ()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] traits : {} }","title":"Building in Rust: init"},{"location":"guide/zome/intro_to_hdk/","text":"Intro to Language HDKs \u00b6 Within any Zome, there are a number of conventions that must be followed by the WASM code, in order to work with Holochain core. For example parameters are passed using a specific memory allocation scheme that needs to be followed to access the parameters that a Zome function receives. Although it would be possible for Zome authors to code directly to the Holochain core API, it makes more sense to provide a software library for each language to make it as easy as possible to use those standard functions and behaviours. We call such a library a \"Holochain Development Kit\", or HDK. So in order to get familiar with coding for Holochain, it will involve familiarity with an HDK library. An HDK performs many important functions for developers. - It aids with the memory management of the WASM code that your Zomes compile into. WASM has some strict memory limitations, like 64 KiB paged memory. - It creates helper functions that hide tedious semantics that seem to plague compilation of languages to WASM. - It implements a complete type system that's compatible with Holochain. - It addresses hidden functions that Holochain needs from Zome code, but that would be redundant and confusing to make Zome developers write again and again. The HDK for a given language, if it has deep integration into the build tools, such as the Zome generator, should include this helper library by default, which is the case of the Rust HDK. There is an in-depth article on writing an HDK if this sounds interesting to you. The Rust HDK \u00b6 The HDK with priority development and support is implemented in Rust, and is included right in the core repository along with the Holochain core framework. Other HDKs may be implemented in different languages, and exist in separate repositories. This HDK implements all of the above features for developers, so just know that as you develop your Zome, a lot is going on behind the scenes in the HDK to make it all work. The Rust HDK has documentation for each released version, available at developer.holochain.org/api . This documentation will be invaluable during your use of the HDK, because it can show you how the definitions of the various custom Holochain types, and give examples and details on the use of the API functions exposed to the Zomes. Notice that in the Cargo.toml file of a new Zome, the HDK is included. For example, ... [dependencies] ... hdk = { git = \"https://github.com/holochain/holochain-rust\" , branch = \"master\" } ... Setting the version of the HDK \u00b6 Once Holochain stabilizes beyond the 0.0.x version numbers, it will be published to the Rust package manager, crates.io and versioning will be simplified. For now, Cargo installs the HDK specified as a GIT dependency, fetching it from the specified commit, branch, or tag of the repository . If you wanted to lock the HDK at a specific version, you could adjust the HDK dependency like this: hdk = { git = \"https://github.com/holochain/holochain-rust\" , tag = \"v0.0.18-alpha1\" } Use of the HDK in Rust code \u00b6 Notice now that within the src/lib.rs file of a new Zome, the HDK is already imported here too: #[macro_use] extern crate hdk ; ... The #[macro_use] statement on the first line is very important, since it allows the usage of Rust macros (discussed in the Define Zome article ) defined in the HDK to be used in your code, and the macros will be needed. Now, within the Rust code files, exposed constants, functions, and even special macros from the HDK can be used. The very first thing to familiarize with is the define_zome! macro. Read on to learn about it.","title":"Intro to HDK"},{"location":"guide/zome/intro_to_hdk/#intro-to-language-hdks","text":"Within any Zome, there are a number of conventions that must be followed by the WASM code, in order to work with Holochain core. For example parameters are passed using a specific memory allocation scheme that needs to be followed to access the parameters that a Zome function receives. Although it would be possible for Zome authors to code directly to the Holochain core API, it makes more sense to provide a software library for each language to make it as easy as possible to use those standard functions and behaviours. We call such a library a \"Holochain Development Kit\", or HDK. So in order to get familiar with coding for Holochain, it will involve familiarity with an HDK library. An HDK performs many important functions for developers. - It aids with the memory management of the WASM code that your Zomes compile into. WASM has some strict memory limitations, like 64 KiB paged memory. - It creates helper functions that hide tedious semantics that seem to plague compilation of languages to WASM. - It implements a complete type system that's compatible with Holochain. - It addresses hidden functions that Holochain needs from Zome code, but that would be redundant and confusing to make Zome developers write again and again. The HDK for a given language, if it has deep integration into the build tools, such as the Zome generator, should include this helper library by default, which is the case of the Rust HDK. There is an in-depth article on writing an HDK if this sounds interesting to you.","title":"Intro to Language HDKs"},{"location":"guide/zome/intro_to_hdk/#the-rust-hdk","text":"The HDK with priority development and support is implemented in Rust, and is included right in the core repository along with the Holochain core framework. Other HDKs may be implemented in different languages, and exist in separate repositories. This HDK implements all of the above features for developers, so just know that as you develop your Zome, a lot is going on behind the scenes in the HDK to make it all work. The Rust HDK has documentation for each released version, available at developer.holochain.org/api . This documentation will be invaluable during your use of the HDK, because it can show you how the definitions of the various custom Holochain types, and give examples and details on the use of the API functions exposed to the Zomes. Notice that in the Cargo.toml file of a new Zome, the HDK is included. For example, ... [dependencies] ... hdk = { git = \"https://github.com/holochain/holochain-rust\" , branch = \"master\" } ...","title":"The Rust HDK"},{"location":"guide/zome/intro_to_hdk/#setting-the-version-of-the-hdk","text":"Once Holochain stabilizes beyond the 0.0.x version numbers, it will be published to the Rust package manager, crates.io and versioning will be simplified. For now, Cargo installs the HDK specified as a GIT dependency, fetching it from the specified commit, branch, or tag of the repository . If you wanted to lock the HDK at a specific version, you could adjust the HDK dependency like this: hdk = { git = \"https://github.com/holochain/holochain-rust\" , tag = \"v0.0.18-alpha1\" }","title":"Setting the version of the HDK"},{"location":"guide/zome/intro_to_hdk/#use-of-the-hdk-in-rust-code","text":"Notice now that within the src/lib.rs file of a new Zome, the HDK is already imported here too: #[macro_use] extern crate hdk ; ... The #[macro_use] statement on the first line is very important, since it allows the usage of Rust macros (discussed in the Define Zome article ) defined in the HDK to be used in your code, and the macros will be needed. Now, within the Rust code files, exposed constants, functions, and even special macros from the HDK can be used. The very first thing to familiarize with is the define_zome! macro. Read on to learn about it.","title":"Use of the HDK in Rust code"},{"location":"guide/zome/intro_to_webassembly/","text":"Intro to WebAssembly \u00b6 What is WebAssembly exactly? \"WebAssembly is a standard being developed by the W3C group for an efficient, lightweight instruction set. This means we can compile different types of programming languages ranging from C/C++, Go, Rust, and more into a single standard... WebAssembly, or WASM, for short, is memory-safe,platform independent, and maps well to all types of CPU architectures efficiently.\" - source Though initially designed for use by major browsers IE, Chrome, Firefox and Safari, WASM has quickly been taken up as a portable target for execution on native platforms as well. WebAssembly.org describes it as a binary instruction format for a stack-based virtual machine. Despite being a binary format, \"WebAssembly is designed to be pretty-printed in a textual format for debugging, testing, experimenting, optimizing, learning, teaching, and writing programs by hand.\" This textual format is called WAT. Not because it needs to be understood, but so that you can get a glimpse of what WAT looks like, here's a little sample: (module (memory (;0;) 1) (func (export \"public_test_fn\") (param $p0 i64) (result i64) i64.const 6 ) (data (i32.const 0) \"1337.0\" ) (export \"memory\" (memory 0)) ) Once the above code is converted from WAT to binary WASM it is in the format that could be executed by the Holochain WASM interpreter. Often times, for a language that compiles to WASM, you will have a configuration option to generate the (more) human readable WAT version of the code as well, while compiling it to WASM. While the compilation to WASM mostly happens in the background for you as an app developer, having a basic understanding of the role of WebAssembly in this technology stack will no doubt help you along the way.","title":"Intro to WebAssembly"},{"location":"guide/zome/intro_to_webassembly/#intro-to-webassembly","text":"What is WebAssembly exactly? \"WebAssembly is a standard being developed by the W3C group for an efficient, lightweight instruction set. This means we can compile different types of programming languages ranging from C/C++, Go, Rust, and more into a single standard... WebAssembly, or WASM, for short, is memory-safe,platform independent, and maps well to all types of CPU architectures efficiently.\" - source Though initially designed for use by major browsers IE, Chrome, Firefox and Safari, WASM has quickly been taken up as a portable target for execution on native platforms as well. WebAssembly.org describes it as a binary instruction format for a stack-based virtual machine. Despite being a binary format, \"WebAssembly is designed to be pretty-printed in a textual format for debugging, testing, experimenting, optimizing, learning, teaching, and writing programs by hand.\" This textual format is called WAT. Not because it needs to be understood, but so that you can get a glimpse of what WAT looks like, here's a little sample: (module (memory (;0;) 1) (func (export \"public_test_fn\") (param $p0 i64) (result i64) i64.const 6 ) (data (i32.const 0) \"1337.0\" ) (export \"memory\" (memory 0)) ) Once the above code is converted from WAT to binary WASM it is in the format that could be executed by the Holochain WASM interpreter. Often times, for a language that compiles to WASM, you will have a configuration option to generate the (more) human readable WAT version of the code as well, while compiling it to WASM. While the compilation to WASM mostly happens in the background for you as an app developer, having a basic understanding of the role of WebAssembly in this technology stack will no doubt help you along the way.","title":"Intro to WebAssembly"},{"location":"guide/zome/linking/","text":"Linking \u00b6","title":"(E) Linking"},{"location":"guide/zome/linking/#linking","text":"","title":"Linking"},{"location":"guide/zome/node_to_node_messages/","text":"Node to Node Messaging \u00b6","title":"(E) Node to Node Messaging"},{"location":"guide/zome/node_to_node_messages/#node-to-node-messaging","text":"","title":"Node to Node Messaging"},{"location":"guide/zome/read_and_write/","text":"Read & Write Data Operations \u00b6 The Local Source Chain: Headers and Entries \u00b6 At the core of how Holochain is designed is the \"local source chain\". It is the data structure that puts Holochain in the family of \"distributed ledger technology\". It functions as an \"append-only log\", which is a fancy way of saying that data can only ever be written to the log, and never modified in place, nor removed, without breaking the integrity of the data that follows it in the log. As has been found for many other projects, this is a critical design choice for a distributed technology. Note that across the world of crytocurrency and distributed ledgers, these logs are sometimes given different names, despite usually refering to a very similar data structure: feed, log, ledger, and chain are only four such examples. What they share is the append-only nature, as well as the use of cryptographic signatures for securing data integrity. As a consequence of this log-like data structure, Holochain is a departure from traditional relational (SQL) and also non-relational (noSQL) databases. There are no tables, and no foreign keys, at least on the formal level of Holochain. Despite this, one must of course still be able to persist data, and read back persisted data, and all of that happens in relation to the local source chain. With Holochain, there are two aspects that combine to form each \"element\" in this chain-like log: headers, and entries. Headers \u00b6 The fundamental chain-like aspect of Holochain is driven by \"headers\". They are a minimal data structure containing only a handful of values, but are sufficient to supply Holochain with verifiable data integrity. Fundamental to creating the \"chain\", each header contains a reference to the \"last\" header. There are a couple aspects to this: 1. The cryptographic hash (its data \"fingerprint\") for a header is determined 2. The hash of the previously written header is used as a fixed-length input for the next header, thus influencing the hash of that new header (since any change to contents change a hash) 3. The only time this does not apply is for the very first header, for which there is no previous header hash, and a value of 0 is used Other data contained in the headers includes (but isn't limited to): - the public key/address, and the cryptographically signed signature of the author - a timestamp conveying the moment of creation of the header, from the perspective of the author's system clock - the hash of the \"entry\" being created - the \"entry type\" of the \"entry\" being created The meaning and role of entries is covered next. Entries \u00b6 Entries are elements of data which get persisted to the same storage as Headers, yet have a different nature. They represent the substance of the data that is of interest to the end users of Holochain, storing records such as transactions, social media posts, or whatever else app developers come up with. The hash of each entry is stored and referenced in its corresponding header. Every local source chain should have exactly one Entry for every Header. All Entries have a type. Entries can be \"system\" Entries, or \"app\" Entries. System Entries are Entries integral to the proper functioning of Holochain. Importantly, there are system Entries written to each local source chain, at the time of its initialization. System Entries \u00b6 These include firstly, the DNA itself. The DNA is written as the first entry into each local source chain. Importantly, this is the foundation for establishing confidence in the data of everyone's chains: the fact that you all know you started out by running the same code (otherwise, the hash of the initial DNA entry would be different). There will be more discussion on this later. The second Entry to be written to the chain is information about the agent using this source chain. That includes their \"public key\" which is also their address on the network. There are also other system entry types which are automatically created to record the occurrence of certain actions such as the creation or removal of a link, the marking of an existing entry as deleted, or the granting or revoking of a capability token. These will be covered in more detail in later chapters. App Entries \u00b6 Next, there are the \"App\" Entries. The definitions for these entry types, and how to validate them, are in the hands of developers. When end users are attempting actions which should author data, the data will first be checked against the validation rules supplied by the DNA developer. The possibilities for App Entry type definitions are endless. Defining App Entry types is how you setup the schemas, data structures, and data integrity of your DNA. Entry Sharing \u00b6 Not every Entry must be shared with other people running the same DNA. There are two (and may be more later) levels of sharing for App Entries defined: public and private . With public , all Entries of that type will get propagated to the other peers running the same DNA, so that they will hold a copy as well, and make your data more resilient, and so that it is available even when you're offline. With private , the Entry will never leave your device. In order to still be able to verify your local source chain, the HEADER for that Entry will be propagated to other peers, but the Entry itself will not. Data Propagation \u00b6 After any chain-modifying functions are called, Holochain will attempt to notify other peers in the network of those actions. Note that it is not a requirement that the device be online or connected to any other networks or nodes for Holochain to be usable! Holochain will persist the changes locally to be gossiped to connected peers at a later date. As mentioned previously, if the entry is one of a private App Entry type, then only the Header will be published, not the entry itself. Publishing of the data is secondary to being able to author data in the first place, so this will be further elaborated in the entry validation article. Writing Data \u00b6 Broadly speaking, writing data is accomplished by writing Zome source code that makes API calls to Holochain to affect the local source chain. One should only modify chain data by calling Holochain API functions, as there is lots of internal logic that it is instrumental that Holochain perform, for each change. When an API function to alter the chain from Zome source code is invoked, a series of steps is performed internally. It is important to know, at least roughly, what those steps are so that you can use Holochain effectively. Apart from the following three functions, there is only one additional way that data can be written to the local source chain, which is written about in linking . Creating Entries, or \"Committing\" \u00b6 Of course, first and foremost, there is the simple action of writing a new Entry. This is actually known as \"committing\" an Entry. It is known as \"committing\" an Entry in Holochain for precisely the reason that once you write it, you can't \"unwrite\" it, or delete it, without corrupting the integrity of your local source chain. It is there for good, and can only be \"marked\" as updated or deleted by writing additional entries in the future. Because the Entry will be permanent in your local source chain, it must first pass validation, in order to be written in the first place. Validation of Entries is core to the distributed data integrity model of Holochain. Invoking the commit entry API function will not always return \"success\". The call can fail if the Entry fails to pass validation, or something else goes wrong internally while Holochain is attempting to write the Entry to the local source chain. Like in any code, failure tolerant code is far better, and Zomes should be written to handle possible error cases. So in general, the process that Holochain follows while trying to write an Entry during a \"commit\" call is this: Collects data necessary for use in validation (a \"validation package\") Checks whether the new entry is valid by passing it through the validation callback specified in the Zome for its given entry type. If this fails, it will exit here and return either a user defined, or system level error message Creates a new Header for the Entry, and writes it to storage Writes the Entry itself to storage Announces over the network module this new Header and Entry, requesting that peers validate and hold a copy of it. This is known internally as \"publishing\". Returns the \"address\" of the new Entry, which is the crytographic hash of the Entry data combined with its entry type string. This address can be used later to retrieve the Entry. Updating Entries \u00b6 The act of updating an Entry is technically speaking just the same \"committing an Entry\", plus storing a bit of metadata indicating that this new entry is an \"update\" to the entry at an old address which is also given. This has the effect that when an attempt to retrieve an Entry by its address, it will forward the request along to the latest version in a potential series of updates, and return that instead. There is always the option to return the results for an Entry according to whether it's the \"initial\" version, the \"latest\", or getting the entire history too. So to update an Entry, use the address of that entry, and provide the new Entry to replace it. You can use the address of the Entry at any point in its update history as the address to update (as long as it hasn't been marked deleted), and it will still work, technically updating the very latest version of that Entry instead of whatever you pass in. So in general, the process that Holochain follows while trying to update an Entry during an \"update_entry\" call is this: Retrieves the very latest version of the Entry at the given address Collects data necessary for use in validation (a \"validation package\") Checks whether the new entry is valid by passing it through the validation callback specified in the Zome for its given entry type. If this fails, it will exit here and return either a user defined, or system level error message Creates a new Header for the Entry, and writes it to storage Writes the Entry itself to storage Announces over the network module this new Header and Entry, requesting that peers validate and hold a copy of it. This is known internally as \"publishing\". This step also involves updating the metadata for the Entry at the old address such that default requests for it will forward to the new Entry. Returns the \"address\" of the new Entry, which is the crytographic hash of the Entry data combined with its entry type string. This address can be used later to retrieve the Entry. Removing Entries \u00b6 The goal of removing an Entry is so that it will not automatically show up as a result when attempts to retrieve it are made, it is NOT to delete it entirely. Removing an Entry will not prevent someone who wishes to retrieve it from retrieving it, they will just have to pass a special option to do so. This makes sense due to the \"append-only\" nature of Holochain: the original Entry is never gone. To remove an Entry, Holochain actually commits an Entry of a special type: DeletionEntry . So in general, the process that Holochain follows while trying to remove an Entry during an \"remove_entry\" call is this: Retrieves the very latest version of the Entry at the given address Collects data necessary for use in validation (a \"validation package\") Creates a new DeletionEntry , containing the address of the very latest version of the given entry Checks whether the DeletionEntry is valid by passing it through the validation callback specified in the Zome for its given entry type. If this fails, it will exit here and return either a user defined, or system level error message Creates a new Header for the Entry, and writes it to storage Writes the Entry itself to storage Announces over the network module this new Header and Entry, requesting that peers validate and hold a copy of it. This is known internally as \"publishing\". This step also involves updating the metadata for the Entry at the old address such that default requests for it will return no Entry. Returns a null value, as there is no need to retrieve the new DeletionEntry at any point for any reason. Reading Data \u00b6 Reading data is a lot more straightforward than writing data. Data is either read from your own device, if it lives there, or is fetched from peers over network connections. Throughout the writing data section, it was mentioned that Entries and even Headers have \"addresses\". These addresses are linked to the content itself, following a pattern known as \"content addressable storage\" . This means that to determine the address, the content is passed through a cryptographic hash function, which will deterministically give the same hash whenever the same content is given, and can take an input of any length, and return a result of fixed length. The content is then deposited in a location on a computer, and a network, where it can be retrieved given its address/hash. Only Entries may be retrieved by their address, not Headers. Get an Entry \u00b6 Query Local Chain Entries \u00b6 Building in Rust: Read & Write Data \u00b6 hdk::commit_entry \u00b6 hdk::update_entry \u00b6 hdk::remove_entry \u00b6 hdk::get_entry \u00b6 hdk::query \u00b6","title":"Read & Write Data Operations"},{"location":"guide/zome/read_and_write/#read-write-data-operations","text":"","title":"Read &amp; Write Data Operations"},{"location":"guide/zome/read_and_write/#the-local-source-chain-headers-and-entries","text":"At the core of how Holochain is designed is the \"local source chain\". It is the data structure that puts Holochain in the family of \"distributed ledger technology\". It functions as an \"append-only log\", which is a fancy way of saying that data can only ever be written to the log, and never modified in place, nor removed, without breaking the integrity of the data that follows it in the log. As has been found for many other projects, this is a critical design choice for a distributed technology. Note that across the world of crytocurrency and distributed ledgers, these logs are sometimes given different names, despite usually refering to a very similar data structure: feed, log, ledger, and chain are only four such examples. What they share is the append-only nature, as well as the use of cryptographic signatures for securing data integrity. As a consequence of this log-like data structure, Holochain is a departure from traditional relational (SQL) and also non-relational (noSQL) databases. There are no tables, and no foreign keys, at least on the formal level of Holochain. Despite this, one must of course still be able to persist data, and read back persisted data, and all of that happens in relation to the local source chain. With Holochain, there are two aspects that combine to form each \"element\" in this chain-like log: headers, and entries.","title":"The Local Source Chain: Headers and Entries"},{"location":"guide/zome/read_and_write/#headers","text":"The fundamental chain-like aspect of Holochain is driven by \"headers\". They are a minimal data structure containing only a handful of values, but are sufficient to supply Holochain with verifiable data integrity. Fundamental to creating the \"chain\", each header contains a reference to the \"last\" header. There are a couple aspects to this: 1. The cryptographic hash (its data \"fingerprint\") for a header is determined 2. The hash of the previously written header is used as a fixed-length input for the next header, thus influencing the hash of that new header (since any change to contents change a hash) 3. The only time this does not apply is for the very first header, for which there is no previous header hash, and a value of 0 is used Other data contained in the headers includes (but isn't limited to): - the public key/address, and the cryptographically signed signature of the author - a timestamp conveying the moment of creation of the header, from the perspective of the author's system clock - the hash of the \"entry\" being created - the \"entry type\" of the \"entry\" being created The meaning and role of entries is covered next.","title":"Headers"},{"location":"guide/zome/read_and_write/#entries","text":"Entries are elements of data which get persisted to the same storage as Headers, yet have a different nature. They represent the substance of the data that is of interest to the end users of Holochain, storing records such as transactions, social media posts, or whatever else app developers come up with. The hash of each entry is stored and referenced in its corresponding header. Every local source chain should have exactly one Entry for every Header. All Entries have a type. Entries can be \"system\" Entries, or \"app\" Entries. System Entries are Entries integral to the proper functioning of Holochain. Importantly, there are system Entries written to each local source chain, at the time of its initialization.","title":"Entries"},{"location":"guide/zome/read_and_write/#system-entries","text":"These include firstly, the DNA itself. The DNA is written as the first entry into each local source chain. Importantly, this is the foundation for establishing confidence in the data of everyone's chains: the fact that you all know you started out by running the same code (otherwise, the hash of the initial DNA entry would be different). There will be more discussion on this later. The second Entry to be written to the chain is information about the agent using this source chain. That includes their \"public key\" which is also their address on the network. There are also other system entry types which are automatically created to record the occurrence of certain actions such as the creation or removal of a link, the marking of an existing entry as deleted, or the granting or revoking of a capability token. These will be covered in more detail in later chapters.","title":"System Entries"},{"location":"guide/zome/read_and_write/#app-entries","text":"Next, there are the \"App\" Entries. The definitions for these entry types, and how to validate them, are in the hands of developers. When end users are attempting actions which should author data, the data will first be checked against the validation rules supplied by the DNA developer. The possibilities for App Entry type definitions are endless. Defining App Entry types is how you setup the schemas, data structures, and data integrity of your DNA.","title":"App Entries"},{"location":"guide/zome/read_and_write/#entry-sharing","text":"Not every Entry must be shared with other people running the same DNA. There are two (and may be more later) levels of sharing for App Entries defined: public and private . With public , all Entries of that type will get propagated to the other peers running the same DNA, so that they will hold a copy as well, and make your data more resilient, and so that it is available even when you're offline. With private , the Entry will never leave your device. In order to still be able to verify your local source chain, the HEADER for that Entry will be propagated to other peers, but the Entry itself will not.","title":"Entry Sharing"},{"location":"guide/zome/read_and_write/#data-propagation","text":"After any chain-modifying functions are called, Holochain will attempt to notify other peers in the network of those actions. Note that it is not a requirement that the device be online or connected to any other networks or nodes for Holochain to be usable! Holochain will persist the changes locally to be gossiped to connected peers at a later date. As mentioned previously, if the entry is one of a private App Entry type, then only the Header will be published, not the entry itself. Publishing of the data is secondary to being able to author data in the first place, so this will be further elaborated in the entry validation article.","title":"Data Propagation"},{"location":"guide/zome/read_and_write/#writing-data","text":"Broadly speaking, writing data is accomplished by writing Zome source code that makes API calls to Holochain to affect the local source chain. One should only modify chain data by calling Holochain API functions, as there is lots of internal logic that it is instrumental that Holochain perform, for each change. When an API function to alter the chain from Zome source code is invoked, a series of steps is performed internally. It is important to know, at least roughly, what those steps are so that you can use Holochain effectively. Apart from the following three functions, there is only one additional way that data can be written to the local source chain, which is written about in linking .","title":"Writing Data"},{"location":"guide/zome/read_and_write/#creating-entries-or-committing","text":"Of course, first and foremost, there is the simple action of writing a new Entry. This is actually known as \"committing\" an Entry. It is known as \"committing\" an Entry in Holochain for precisely the reason that once you write it, you can't \"unwrite\" it, or delete it, without corrupting the integrity of your local source chain. It is there for good, and can only be \"marked\" as updated or deleted by writing additional entries in the future. Because the Entry will be permanent in your local source chain, it must first pass validation, in order to be written in the first place. Validation of Entries is core to the distributed data integrity model of Holochain. Invoking the commit entry API function will not always return \"success\". The call can fail if the Entry fails to pass validation, or something else goes wrong internally while Holochain is attempting to write the Entry to the local source chain. Like in any code, failure tolerant code is far better, and Zomes should be written to handle possible error cases. So in general, the process that Holochain follows while trying to write an Entry during a \"commit\" call is this: Collects data necessary for use in validation (a \"validation package\") Checks whether the new entry is valid by passing it through the validation callback specified in the Zome for its given entry type. If this fails, it will exit here and return either a user defined, or system level error message Creates a new Header for the Entry, and writes it to storage Writes the Entry itself to storage Announces over the network module this new Header and Entry, requesting that peers validate and hold a copy of it. This is known internally as \"publishing\". Returns the \"address\" of the new Entry, which is the crytographic hash of the Entry data combined with its entry type string. This address can be used later to retrieve the Entry.","title":"Creating Entries, or \"Committing\""},{"location":"guide/zome/read_and_write/#updating-entries","text":"The act of updating an Entry is technically speaking just the same \"committing an Entry\", plus storing a bit of metadata indicating that this new entry is an \"update\" to the entry at an old address which is also given. This has the effect that when an attempt to retrieve an Entry by its address, it will forward the request along to the latest version in a potential series of updates, and return that instead. There is always the option to return the results for an Entry according to whether it's the \"initial\" version, the \"latest\", or getting the entire history too. So to update an Entry, use the address of that entry, and provide the new Entry to replace it. You can use the address of the Entry at any point in its update history as the address to update (as long as it hasn't been marked deleted), and it will still work, technically updating the very latest version of that Entry instead of whatever you pass in. So in general, the process that Holochain follows while trying to update an Entry during an \"update_entry\" call is this: Retrieves the very latest version of the Entry at the given address Collects data necessary for use in validation (a \"validation package\") Checks whether the new entry is valid by passing it through the validation callback specified in the Zome for its given entry type. If this fails, it will exit here and return either a user defined, or system level error message Creates a new Header for the Entry, and writes it to storage Writes the Entry itself to storage Announces over the network module this new Header and Entry, requesting that peers validate and hold a copy of it. This is known internally as \"publishing\". This step also involves updating the metadata for the Entry at the old address such that default requests for it will forward to the new Entry. Returns the \"address\" of the new Entry, which is the crytographic hash of the Entry data combined with its entry type string. This address can be used later to retrieve the Entry.","title":"Updating Entries"},{"location":"guide/zome/read_and_write/#removing-entries","text":"The goal of removing an Entry is so that it will not automatically show up as a result when attempts to retrieve it are made, it is NOT to delete it entirely. Removing an Entry will not prevent someone who wishes to retrieve it from retrieving it, they will just have to pass a special option to do so. This makes sense due to the \"append-only\" nature of Holochain: the original Entry is never gone. To remove an Entry, Holochain actually commits an Entry of a special type: DeletionEntry . So in general, the process that Holochain follows while trying to remove an Entry during an \"remove_entry\" call is this: Retrieves the very latest version of the Entry at the given address Collects data necessary for use in validation (a \"validation package\") Creates a new DeletionEntry , containing the address of the very latest version of the given entry Checks whether the DeletionEntry is valid by passing it through the validation callback specified in the Zome for its given entry type. If this fails, it will exit here and return either a user defined, or system level error message Creates a new Header for the Entry, and writes it to storage Writes the Entry itself to storage Announces over the network module this new Header and Entry, requesting that peers validate and hold a copy of it. This is known internally as \"publishing\". This step also involves updating the metadata for the Entry at the old address such that default requests for it will return no Entry. Returns a null value, as there is no need to retrieve the new DeletionEntry at any point for any reason.","title":"Removing Entries"},{"location":"guide/zome/read_and_write/#reading-data","text":"Reading data is a lot more straightforward than writing data. Data is either read from your own device, if it lives there, or is fetched from peers over network connections. Throughout the writing data section, it was mentioned that Entries and even Headers have \"addresses\". These addresses are linked to the content itself, following a pattern known as \"content addressable storage\" . This means that to determine the address, the content is passed through a cryptographic hash function, which will deterministically give the same hash whenever the same content is given, and can take an input of any length, and return a result of fixed length. The content is then deposited in a location on a computer, and a network, where it can be retrieved given its address/hash. Only Entries may be retrieved by their address, not Headers.","title":"Reading Data"},{"location":"guide/zome/read_and_write/#get-an-entry","text":"","title":"Get an Entry"},{"location":"guide/zome/read_and_write/#query-local-chain-entries","text":"","title":"Query Local Chain Entries"},{"location":"guide/zome/read_and_write/#building-in-rust-read-write-data","text":"","title":"Building in Rust: Read &amp; Write Data"},{"location":"guide/zome/read_and_write/#hdkcommit_entry","text":"","title":"hdk::commit_entry"},{"location":"guide/zome/read_and_write/#hdkupdate_entry","text":"","title":"hdk::update_entry"},{"location":"guide/zome/read_and_write/#hdkremove_entry","text":"","title":"hdk::remove_entry"},{"location":"guide/zome/read_and_write/#hdkget_entry","text":"","title":"hdk::get_entry"},{"location":"guide/zome/read_and_write/#hdkquery","text":"","title":"hdk::query"},{"location":"guide/zome/rust/","text":"Writing in Rust \u00b6 It has always been in the designs for Holochain to support programming in multiple languages. In the prototype of Holochain, Zomes could be written in variants of Javascript (ES5) and Lisp (Zygomys). In the new version of Holochain the primary \"Ribosome\", where there was a JS one and Lisp one before, interprets WebAssembly code. While we will provide a small introduction to WebAssembly shortly, we should also briefly introduce Rust, since it is the first language that has a first class Holochain app development experience, with its' WebAssembly compilation. This is accomplished via a Holochain Development Kit (HDK) library which has been written for Rust. For the time being, writing Holochain apps requires writing code in Rust. This will not always be the case. Assemblyscript , a language based off Typescript, is the next likely language in which there will be an HDK library. If it happens to interest you, there is an article here about writing an HDK , since that is something we also invite and encourage the community to do. From Wikipedia: \"Rust is a systems programming language with a focus on safety, especially safe concurrency, supporting both functional and imperative paradigms.\" Rust is a strongly typed language, which is desirable for the development of secure P2P applications, and compilation from Rust to WebAssembly is extremely easy. If Rust is new to you, don't worry. With lots of Holochain app development happening in an open source way, and through learning resources like this guidebook, and the \"Rust book\" you will have lots to reference to get started. While there are lots of other materials available for learning Rust, the base materials for the language are always a good resource to go back to: Rust Docs . In many articles in the following chapters, you will find that there is a dedicated section at the bottom of the article called \"Building in Rust\". This typically follows after a general discussion of a concept, and is done this way because implementation details may differ based on the language developers are writing Zomes in.","title":"Writing in Rust"},{"location":"guide/zome/rust/#writing-in-rust","text":"It has always been in the designs for Holochain to support programming in multiple languages. In the prototype of Holochain, Zomes could be written in variants of Javascript (ES5) and Lisp (Zygomys). In the new version of Holochain the primary \"Ribosome\", where there was a JS one and Lisp one before, interprets WebAssembly code. While we will provide a small introduction to WebAssembly shortly, we should also briefly introduce Rust, since it is the first language that has a first class Holochain app development experience, with its' WebAssembly compilation. This is accomplished via a Holochain Development Kit (HDK) library which has been written for Rust. For the time being, writing Holochain apps requires writing code in Rust. This will not always be the case. Assemblyscript , a language based off Typescript, is the next likely language in which there will be an HDK library. If it happens to interest you, there is an article here about writing an HDK , since that is something we also invite and encourage the community to do. From Wikipedia: \"Rust is a systems programming language with a focus on safety, especially safe concurrency, supporting both functional and imperative paradigms.\" Rust is a strongly typed language, which is desirable for the development of secure P2P applications, and compilation from Rust to WebAssembly is extremely easy. If Rust is new to you, don't worry. With lots of Holochain app development happening in an open source way, and through learning resources like this guidebook, and the \"Rust book\" you will have lots to reference to get started. While there are lots of other materials available for learning Rust, the base materials for the language are always a good resource to go back to: Rust Docs . In many articles in the following chapters, you will find that there is a dedicated section at the bottom of the article called \"Building in Rust\". This typically follows after a general discussion of a concept, and is done this way because implementation details may differ based on the language developers are writing Zomes in.","title":"Writing in Rust"},{"location":"guide/zome/system_constants/","text":"API System Constants \u00b6 Note: Full reference is available in language-specific API Reference documentation. (TODO add links) Name Purpose VERSION Version of the Holochain software running the zome HashNotFound Value returned when a hash provided could not be found. Status Enum holding all possible state of an entry. GetEntryMask Mask values used for calling the get_entry Zome API Function. LinkAction Constants used for calling the link_entries Zome API Function. PkgRequest TODO ChainOption TODO BridgeSide TODO SysEntryType Enum holding all possible types of system entries bundle_cancel.Reason Enum used as argument for bundle_canceled callback bundle_cancel.Response Enum used as return value for bundle_canceled callback","title":"API System Constants"},{"location":"guide/zome/system_constants/#api-system-constants","text":"Note: Full reference is available in language-specific API Reference documentation. (TODO add links) Name Purpose VERSION Version of the Holochain software running the zome HashNotFound Value returned when a hash provided could not be found. Status Enum holding all possible state of an entry. GetEntryMask Mask values used for calling the get_entry Zome API Function. LinkAction Constants used for calling the link_entries Zome API Function. PkgRequest TODO ChainOption TODO BridgeSide TODO SysEntryType Enum holding all possible types of system entries bundle_cancel.Reason Enum used as argument for bundle_canceled callback bundle_cancel.Response Enum used as return value for bundle_canceled callback","title":"API System Constants"},{"location":"guide/zome/validate_agent/","text":"Validate Agent \u00b6 In Holochain the validation of the second entry on the chain, the AgentID, plays a important role because it serves as place to define membrane functions. Therefore we have created a special validation function that must always be defined in every zome. Example: validate_agent : | validation_data : EntryValidationData :: < AgentId >| {{ if let EntryValidationData :: Create { entry , ..} = validation_data { let agent = entry as AgentId ; if agent . nick == \"reject_agent::app\" { Err ( \"This agent will always be rejected\" . into ()) } else { Ok (()) } } else { Err ( \"Cannot update or delete an agent at this time\" . into ()) } }}","title":"Validate Agent"},{"location":"guide/zome/validate_agent/#validate-agent","text":"In Holochain the validation of the second entry on the chain, the AgentID, plays a important role because it serves as place to define membrane functions. Therefore we have created a special validation function that must always be defined in every zome. Example: validate_agent : | validation_data : EntryValidationData :: < AgentId >| {{ if let EntryValidationData :: Create { entry , ..} = validation_data { let agent = entry as AgentId ; if agent . nick == \"reject_agent::app\" { Err ( \"This agent will always be rejected\" . into ()) } else { Ok (()) } } else { Err ( \"Cannot update or delete an agent at this time\" . into ()) } }}","title":"Validate Agent"},{"location":"guide/zome/welcome/","text":"Building Holochain Apps: Zome Code \u00b6 Recall that for the DNA of a hApp, there can be many Zomes, and each one will have their own source code. Think of Zomes as the fundamental unit of composability for DNA. As a DNA developer you can think of Zomes as modules. We expect developers to reuse Zomes written by others, and thus Zomes can call one another's functionality, using the call API function. Though currently Rust is the only available language for writing Zomes, note that these Zomes could be written in different languages (any language that compiles to WebAssembly) from one another in the future, and still access one another's functionality. While writing the source code for DNA, it is extremely important to verify, before putting it into people's hands, that the code works as expected. For this reason, there are tools for testing included by default in newly generated projects. While there are technically a variety of ways that testing could be accomplished, and you could build your own, the most accessible of those is included by default, which is a JavaScript/nodejs Holochain Conductor. What this means is that the full scope of writing DNA, as of this writing, is likely for most people to include source code in two languages: - Rust - JavaScript In the near future, this is likely to expand in diversity on both sides, Zome code and testing code. Throughout this chapter, there will be plenty of examples given as to writing Zome code in Rust. This chapter starts with a step by step tutorial, and goes on with a detailed explanation of the affordances of Holochain, and how to use its core functions.","title":"Building Holochain Apps: Zome Code"},{"location":"guide/zome/welcome/#building-holochain-apps-zome-code","text":"Recall that for the DNA of a hApp, there can be many Zomes, and each one will have their own source code. Think of Zomes as the fundamental unit of composability for DNA. As a DNA developer you can think of Zomes as modules. We expect developers to reuse Zomes written by others, and thus Zomes can call one another's functionality, using the call API function. Though currently Rust is the only available language for writing Zomes, note that these Zomes could be written in different languages (any language that compiles to WebAssembly) from one another in the future, and still access one another's functionality. While writing the source code for DNA, it is extremely important to verify, before putting it into people's hands, that the code works as expected. For this reason, there are tools for testing included by default in newly generated projects. While there are technically a variety of ways that testing could be accomplished, and you could build your own, the most accessible of those is included by default, which is a JavaScript/nodejs Holochain Conductor. What this means is that the full scope of writing DNA, as of this writing, is likely for most people to include source code in two languages: - Rust - JavaScript In the near future, this is likely to expand in diversity on both sides, Zome code and testing code. Throughout this chapter, there will be plenty of examples given as to writing Zome code in Rust. This chapter starts with a step by step tutorial, and goes on with a detailed explanation of the affordances of Holochain, and how to use its core functions.","title":"Building Holochain Apps: Zome Code"},{"location":"guide/zome/zome_functions/","text":"Zome Functions \u00b6 It is time to address the core application logic of Zomes. What Zome functions you write depends, of course, on what you are building your application to do. By exposing a number of native capacities of Holochain to Zomes, developers have been given access to a rich suite of tools that offer limitless ways they can be combined. Holochain achieves this by exposing core functions to the WASM code of Zomes. A core feature of an HDK is that it will handle that native interface to Holochain, wrapping the underlying functions in easy to call, well defined and well documented functions that are native to the language that you're writing in! So within Zome code, by calling functions of the HDK, you can do powerful things with Holochain, like: - Read and write data to and from the configured storage mechanism, and from the network - Transport messages directly between nodes of a network - Call functions in other Zomes, and even \"bridged\" DNA instances - Use cryptographic functions like signing and verification to handle data security and integrity - Emit \"signals\" containing data from a Zome, to a UI for example How these different functions work and how to use them will be covered throughout the rest of this chapter in detail. This article will provide a general overview of what Zome functions themselves are and how they work. Recall that Zomes will be written in diverse programming languages, any one that compiles to WebAssembly. Towards the bottom of this article, \"Building in Rust\" gives examples of what writing functions in Rust will be like. It is difficult to show what a function in WASM looks like, since even the \"human-readable\" version of WASM, WAT, is not highly readable. DNA, Zomes, Functions, Traits, and Capabilities \u00b6 When Holochain loads a DNA file, to start an instance from it, it expects the presence of one or more Zomes in the definition. Here is a skeletal (incomplete) DNA JSON file to illustrate this: { \"name\" : \"test\" , \"zomes\" : { \"test_zome\" : { \"name\" : \"test_zome\" , \"traits\" : { \"hc_public\" : { \"functions\" : [], } }, \"fn_declarations\" : [], \"code\" : { \"code\" : \"AAECAw==\" } } } } This theoretical DNA has one Zome, \"test_zome\". However, it has no functions. Note that the nested fn_declarations property is an empty array. There are a few things to learn from this DNA JSON. The first, is that the code, Base64 encoded WASM, is actually embedded in the Zome's definition, nested under code.code . All the functions Holochain expects to be implemented need to be encapsulated within that WASM code. The second is that even outside of the WASM code, Holochain expects a certain level of visibility into the functions contained within, at least the ones meant to be called via Holochain (as oppose to private/internal functions). There are at least two reasons for this: - to be able to reason about data inputs and outputs for those functions - to group those functions semantically for composition These will both be discussed below. Function Declarations \u00b6 All of the Zome's functions are declared in the fn_declarations array. Here is an example of one: \"fn_declarations\" : [ { \"name\" : \"get_task_list\" , \"inputs\" : [{ \"name\" : \"username\" , \"type\" : \"string\" }], \"outputs\" : [{ \"name\" : \"task_list\" , \"type\" : \"json\" }] } ] Each function declaration is an object that includes the name , and the inputs and outputs expected for the function. Since WebAssembly only compiles from code languages with a type system, the generation of these inputs and outputs can expected to be automated. The name is the most important thing here, because when a function call to an instance is being performed, it will have to match a name which Holochain can find in the functions . If the function isn't declared, Holochain will treat it as if it doesn't exist, even if it is an exposed function in the WASM code. Traits \u00b6 Traits provide a way to group functions by name. The primary use of this feature is for creating a composibility space where DNA creators can implement different DNAs to emergent function interfaces and then compose with them in the conductor by matching on the function group names and signatures. Additionally Holochain may reserve a few special trait names that have specific side-effects. The first of such reserved names is hc_public . Functions grouped in this name will have automatically added to a public capability grant that happens at init time, thus making them accessible to any caller. For more details on the Holochain security model please see the Capabilities section. Here is an example of what a trait definition using the public reserved trait name might look like: \"traits\" : { \"hc_public\" : { \"functions\" : [ \"get_task_list\" ] } } Data Interchange - Inputs and Outputs \u00b6 In order to support building zomes in a variety of languages, we decided to use a simple language agnostic function specification format, using JSON. Other formats may be supported in the future. This has two big implications: Holochain Conductor implementations must handle JSON serialization and deserialization on the \"outside\", and HDKs and Zomes must handle JSON serialization and deserialization on the \"inside\". Holochain agrees only to mediate between the two by passing a string (which should represent valid JSON data). How Zome Functions Are Called \u00b6 Function calls are received by Holochain from client requests (which there are a variety of implementations of, discussed later). When function calls are being made, they will need to include a complete enough set of arguments to know the following: - which Zome? - which function? - what values should the function be called with? Before making the function call, Holochain will check the validity of the request, and fail if necessary. If the request is deemed valid, Holochain will mount the WASM code for a Zome using its' WASM interpreter, and then make a function call into it, giving it the arguments given to it in the request. When it receives the response from the WASM, it will then pass that return value as the response to the request. This may sound complex, but that's just what's going on internally, actually using it with an HDK and a Conductor (which is discussed later) is easy. Building in Rust: Zome Functions \u00b6 So far, in entry type definitions and init , the most complex example of define_zome! was still very simple, and didn't include any functions: ... #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Post { content : String , date_created : String , } define_zome ! { entries : [ entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , native_type : Post , validation_package : || { ValidationPackageDefinition :: Entry }, validation : | _post : Post , _validation_data : ValidationData | { Ok (()) } ) ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] } functions is where the function declarations will be made. Adding Traits: \u00b6 Here are some sample traits ... define_zome ! { ... traits : { hc_public [ read_post ] authoring [ create_post , update_post ] } } In this example, hc_public is the reserved trait name which creates a Public Capbility-type grant at init time for access to the read_post function. Additionally it names an authoring trait for the create_post and update_post functions. Adding a Zome Function \u00b6 In order to add a Zome function, there are two primary steps that are involved. 1. declare your function in define_zome! 2. write the Rust code for the handler of that function, calling any HDK functions you need Step 1 The functions section looks a bit like an array of key-value pairs: ... define_zome ! { ... functions : [ send_message : { inputs : | to_agent : Address , message : String | , outputs : | response : ZomeApiResult < String >| , handler : handle_send_message } ] } In this example, send_message is the given name of this function, by which it will be referenced and called elsewhere. There are three properties necessary to provide send_message , and any function declaration: inputs , outputs , and handler . inputs expects a list or argument names, and types, for the send_message function to be called with. outputs expects a single declaration of a return type. The name (which in the example is response ) is arbitrary, call it anything. handler expects the name of a function which will handle this function call, and which matches the function signature of inputs and outputs . In this case, handle_send_message , which has yet to be defined. Step 2 Here is an example of a simplistic function, for illustration purposes. It centers on the use of a function call to an HDK function. fn handle_send_message ( to_agent : Address , message : String ) -> ZomeApiResult < String > { hdk :: send ( to_agent , message , 60000. into ()) } Notice right away how the arguments match perfectly with the inputs: |...| section of the function declaration. Any differences will cause issues. This is also true of the return type of the output. Note the pairing of ZomeApiResult<String> as the return type. The name of the function, handle_send_message is the same as the name given as the handler in the define_zome! function declaration. Within the function, handle_send_message makes use of a Holochain/HDK function that sends messages directly node-to-node . The available functions, their purpose, and how to use them is fully documented elsewhere, in the API reference and the List of API Functions . In the example, handle_send_message simply forwards the result of calling hdk::send as its' own result. Here are the above two steps combined: ... fn handle_send_message ( to_agent : Address , message : String ) -> ZomeApiResult < String > { hdk :: send ( to_agent , message , 60000. into ()) } define_zome ! { ... functions : [ send_message : { inputs : | to_agent : Address , message : String | , outputs : | response : ZomeApiResult < String >| , handler : handle_send_message } ] } To see plenty of examples of adding functions, check out a file used for testing the many capacities of the HDK . Adding Traits: \u00b6 Here are some sample traits ... define_zome ! { ... traits : { hc_public [ read_post ] authoring [ create_post , update_post ] } } In this example, hc_public is the reserved trait name which create a Public Capability-type grant at init time for access to the read_post function. Additionally it names an authoring trait the create_post and update_post functions. Continue reading to learn all about the API Functions and examples of how to use them.","title":"Zome Functions"},{"location":"guide/zome/zome_functions/#zome-functions","text":"It is time to address the core application logic of Zomes. What Zome functions you write depends, of course, on what you are building your application to do. By exposing a number of native capacities of Holochain to Zomes, developers have been given access to a rich suite of tools that offer limitless ways they can be combined. Holochain achieves this by exposing core functions to the WASM code of Zomes. A core feature of an HDK is that it will handle that native interface to Holochain, wrapping the underlying functions in easy to call, well defined and well documented functions that are native to the language that you're writing in! So within Zome code, by calling functions of the HDK, you can do powerful things with Holochain, like: - Read and write data to and from the configured storage mechanism, and from the network - Transport messages directly between nodes of a network - Call functions in other Zomes, and even \"bridged\" DNA instances - Use cryptographic functions like signing and verification to handle data security and integrity - Emit \"signals\" containing data from a Zome, to a UI for example How these different functions work and how to use them will be covered throughout the rest of this chapter in detail. This article will provide a general overview of what Zome functions themselves are and how they work. Recall that Zomes will be written in diverse programming languages, any one that compiles to WebAssembly. Towards the bottom of this article, \"Building in Rust\" gives examples of what writing functions in Rust will be like. It is difficult to show what a function in WASM looks like, since even the \"human-readable\" version of WASM, WAT, is not highly readable.","title":"Zome Functions"},{"location":"guide/zome/zome_functions/#dna-zomes-functions-traits-and-capabilities","text":"When Holochain loads a DNA file, to start an instance from it, it expects the presence of one or more Zomes in the definition. Here is a skeletal (incomplete) DNA JSON file to illustrate this: { \"name\" : \"test\" , \"zomes\" : { \"test_zome\" : { \"name\" : \"test_zome\" , \"traits\" : { \"hc_public\" : { \"functions\" : [], } }, \"fn_declarations\" : [], \"code\" : { \"code\" : \"AAECAw==\" } } } } This theoretical DNA has one Zome, \"test_zome\". However, it has no functions. Note that the nested fn_declarations property is an empty array. There are a few things to learn from this DNA JSON. The first, is that the code, Base64 encoded WASM, is actually embedded in the Zome's definition, nested under code.code . All the functions Holochain expects to be implemented need to be encapsulated within that WASM code. The second is that even outside of the WASM code, Holochain expects a certain level of visibility into the functions contained within, at least the ones meant to be called via Holochain (as oppose to private/internal functions). There are at least two reasons for this: - to be able to reason about data inputs and outputs for those functions - to group those functions semantically for composition These will both be discussed below.","title":"DNA, Zomes, Functions, Traits, and Capabilities"},{"location":"guide/zome/zome_functions/#function-declarations","text":"All of the Zome's functions are declared in the fn_declarations array. Here is an example of one: \"fn_declarations\" : [ { \"name\" : \"get_task_list\" , \"inputs\" : [{ \"name\" : \"username\" , \"type\" : \"string\" }], \"outputs\" : [{ \"name\" : \"task_list\" , \"type\" : \"json\" }] } ] Each function declaration is an object that includes the name , and the inputs and outputs expected for the function. Since WebAssembly only compiles from code languages with a type system, the generation of these inputs and outputs can expected to be automated. The name is the most important thing here, because when a function call to an instance is being performed, it will have to match a name which Holochain can find in the functions . If the function isn't declared, Holochain will treat it as if it doesn't exist, even if it is an exposed function in the WASM code.","title":"Function Declarations"},{"location":"guide/zome/zome_functions/#traits","text":"Traits provide a way to group functions by name. The primary use of this feature is for creating a composibility space where DNA creators can implement different DNAs to emergent function interfaces and then compose with them in the conductor by matching on the function group names and signatures. Additionally Holochain may reserve a few special trait names that have specific side-effects. The first of such reserved names is hc_public . Functions grouped in this name will have automatically added to a public capability grant that happens at init time, thus making them accessible to any caller. For more details on the Holochain security model please see the Capabilities section. Here is an example of what a trait definition using the public reserved trait name might look like: \"traits\" : { \"hc_public\" : { \"functions\" : [ \"get_task_list\" ] } }","title":"Traits"},{"location":"guide/zome/zome_functions/#data-interchange-inputs-and-outputs","text":"In order to support building zomes in a variety of languages, we decided to use a simple language agnostic function specification format, using JSON. Other formats may be supported in the future. This has two big implications: Holochain Conductor implementations must handle JSON serialization and deserialization on the \"outside\", and HDKs and Zomes must handle JSON serialization and deserialization on the \"inside\". Holochain agrees only to mediate between the two by passing a string (which should represent valid JSON data).","title":"Data Interchange - Inputs and Outputs"},{"location":"guide/zome/zome_functions/#how-zome-functions-are-called","text":"Function calls are received by Holochain from client requests (which there are a variety of implementations of, discussed later). When function calls are being made, they will need to include a complete enough set of arguments to know the following: - which Zome? - which function? - what values should the function be called with? Before making the function call, Holochain will check the validity of the request, and fail if necessary. If the request is deemed valid, Holochain will mount the WASM code for a Zome using its' WASM interpreter, and then make a function call into it, giving it the arguments given to it in the request. When it receives the response from the WASM, it will then pass that return value as the response to the request. This may sound complex, but that's just what's going on internally, actually using it with an HDK and a Conductor (which is discussed later) is easy.","title":"How Zome Functions Are Called"},{"location":"guide/zome/zome_functions/#building-in-rust-zome-functions","text":"So far, in entry type definitions and init , the most complex example of define_zome! was still very simple, and didn't include any functions: ... #[derive(Serialize, Deserialize, Debug, DefaultJson)] struct Post { content : String , date_created : String , } define_zome ! { entries : [ entry ! ( name : \"post\" , description : \"A blog post entry which has an author\" , sharing : Sharing :: Public , native_type : Post , validation_package : || { ValidationPackageDefinition :: Entry }, validation : | _post : Post , _validation_data : ValidationData | { Ok (()) } ) ] init : || { Ok (()) } validate_agent : | validation_data : EntryValidationData :: < AgentId >| { Ok (()) } functions : [] } functions is where the function declarations will be made.","title":"Building in Rust: Zome Functions"},{"location":"guide/zome/zome_functions/#adding-traits","text":"Here are some sample traits ... define_zome ! { ... traits : { hc_public [ read_post ] authoring [ create_post , update_post ] } } In this example, hc_public is the reserved trait name which creates a Public Capbility-type grant at init time for access to the read_post function. Additionally it names an authoring trait for the create_post and update_post functions.","title":"Adding Traits:"},{"location":"guide/zome/zome_functions/#adding-a-zome-function","text":"In order to add a Zome function, there are two primary steps that are involved. 1. declare your function in define_zome! 2. write the Rust code for the handler of that function, calling any HDK functions you need Step 1 The functions section looks a bit like an array of key-value pairs: ... define_zome ! { ... functions : [ send_message : { inputs : | to_agent : Address , message : String | , outputs : | response : ZomeApiResult < String >| , handler : handle_send_message } ] } In this example, send_message is the given name of this function, by which it will be referenced and called elsewhere. There are three properties necessary to provide send_message , and any function declaration: inputs , outputs , and handler . inputs expects a list or argument names, and types, for the send_message function to be called with. outputs expects a single declaration of a return type. The name (which in the example is response ) is arbitrary, call it anything. handler expects the name of a function which will handle this function call, and which matches the function signature of inputs and outputs . In this case, handle_send_message , which has yet to be defined. Step 2 Here is an example of a simplistic function, for illustration purposes. It centers on the use of a function call to an HDK function. fn handle_send_message ( to_agent : Address , message : String ) -> ZomeApiResult < String > { hdk :: send ( to_agent , message , 60000. into ()) } Notice right away how the arguments match perfectly with the inputs: |...| section of the function declaration. Any differences will cause issues. This is also true of the return type of the output. Note the pairing of ZomeApiResult<String> as the return type. The name of the function, handle_send_message is the same as the name given as the handler in the define_zome! function declaration. Within the function, handle_send_message makes use of a Holochain/HDK function that sends messages directly node-to-node . The available functions, their purpose, and how to use them is fully documented elsewhere, in the API reference and the List of API Functions . In the example, handle_send_message simply forwards the result of calling hdk::send as its' own result. Here are the above two steps combined: ... fn handle_send_message ( to_agent : Address , message : String ) -> ZomeApiResult < String > { hdk :: send ( to_agent , message , 60000. into ()) } define_zome ! { ... functions : [ send_message : { inputs : | to_agent : Address , message : String | , outputs : | response : ZomeApiResult < String >| , handler : handle_send_message } ] } To see plenty of examples of adding functions, check out a file used for testing the many capacities of the HDK .","title":"Adding a Zome Function"},{"location":"guide/zome/zome_functions/#adding-traits_1","text":"Here are some sample traits ... define_zome ! { ... traits : { hc_public [ read_post ] authoring [ create_post , update_post ] } } In this example, hc_public is the reserved trait name which create a Public Capability-type grant at init time for access to the read_post function. Additionally it names an authoring trait the create_post and update_post functions. Continue reading to learn all about the API Functions and examples of how to use them.","title":"Adding Traits:"}]}